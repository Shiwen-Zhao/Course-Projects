{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy的ndarray和Tensor的区别\n",
    "\n",
    "参考资料：\n",
    "PyTorch中torch.tensor与torch.Tensor的区别详解\n",
    "https://www.jb51.net/article/186764.htm\n",
    "\n",
    "**注：pytorch里的Tensor与tensorflow无任何关系，仅仅只是pytorch里面的一个数据类型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "创建一个Tensor并设置requires_grad=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_zeros: \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]], requires_grad=True)\n",
      "x_ones: \n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "x_randn: \n",
      " tensor([[-0.5896, -1.7527],\n",
      "        [-0.7348, -0.7916]], requires_grad=True)\n",
      "\n",
      "x.grad_fn:  None\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "# 分别创建全0、全1、随机的tensor\n",
    "x_zeros = torch.zeros(2, 2, requires_grad=True)\n",
    "x_ones = torch.ones(2, 2, requires_grad=True)\n",
    "x_randn = torch.randn(2, 2, requires_grad=True) # 缺失情况下默认 requires_grad = False\n",
    "\n",
    "print(\"x_zeros: \\n\", x_zeros)\n",
    "print(\"x_ones: \\n\", x_ones)\n",
    "print(\"x_randn: \\n\", x_randn)\n",
    "print()\n",
    "print(\"x.grad_fn: \",x_ones.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy的ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_zeros_np: \n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "x_ones_np_np: \n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "x_rand_np_np: \n",
      " [[-2.06748811 -0.31502071]\n",
      " [-1.6087552  -0.57207833]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分别创建全0、全1、随机的ndarray数组\n",
    "\n",
    "\n",
    "x_zeros_np = np.zeros((2, 2))\n",
    "x_ones_np = np.ones((2, 2))\n",
    "x_rand_np = np.random.randn(2, 2)\n",
    "\n",
    "print(\"x_zeros_np: \\n\", x_zeros_np)\n",
    "print(\"x_ones_np_np: \\n\", x_ones_np)\n",
    "print(\"x_rand_np_np: \\n\", x_rand_np)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy array 和 Tensor 相互转换\n",
    "\n",
    "参考资料：\n",
    "torch里面的Tensor、as_tensor、tensor以及from_numpy究竟有何区别? \\\n",
    "https://www.136.la/jingpin/show-208975.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Tensor 转 numpy array\n",
    "# RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "# a = x_zeros.numpy()\n",
    "\n",
    "a = x_zeros.detach().numpy()\n",
    "\n",
    "a_tensor = torch.from_numpy(a)\n",
    "a_tensor = torch.tensor(a)\n",
    "a_tensor = torch.as_tensor(a)\n",
    "a_tensor = torch.Tensor(a)\n",
    "\n",
    "print(a_tensor)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "y的ndarray.grad_fn:  <AddBackward0 object at 0x0000022C1C4A6F70>\n"
     ]
    }
   ],
   "source": [
    "y = x_ones + 2\n",
    "print(y)\n",
    "print(\"y的ndarray.grad_fn: \",y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有grad_fn, 而y是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn。\n",
    "\n",
    "像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x_ones.is_leaf, y.is_leaf) # True False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来点复杂度运算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "\n",
    "out_mean = z.mean()\n",
    "z_sum = z.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要求查Tensor.sum()的函数使用方法去是实现以下要求\n",
    "\n",
    "### 对 z.sum(None) 中的None进行填空\n",
    "\n",
    "### (查询函数需要传入的参数)\n",
    "### 参考: https://blog.csdn.net/hahameier/article/details/103742831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor(108., grad_fn=<SumBackward0>)\n",
      "\n",
      "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
      "torch.Size([2])\n",
      "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 第1个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_sum_dim1 = z.sum(dim=1)\n",
    "# 第0个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_sum_dim0 = z.sum(dim=0)\n",
    "\n",
    "print(z)\n",
    "print(out_mean)\n",
    "print(z_sum)\n",
    "\n",
    "print()\n",
    "print(z_sum_dim1)\n",
    "print(z_sum_dim1.shape)\n",
    "print(z_sum_dim0)\n",
    "print(z_sum_dim0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期望输出\n",
    "\n",
    "```python\n",
    "tensor([[27., 27.],\n",
    "        [27., 27.]], grad_fn=<MulBackward0>)\n",
    "    \n",
    "tensor(27., grad_fn=<MeanBackward0>)\n",
    "tensor(108., grad_fn=<SumBackward0>)\n",
    "\n",
    "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
    "torch.Size([2])\n",
    "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
    "torch.Size([2])\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "z_re: \n",
      " tensor([[[27.]],\n",
      "\n",
      "        [[27.]],\n",
      "\n",
      "        [[27.]],\n",
      "\n",
      "        [[27.]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "z_re_sum_dim0: \n",
      " tensor([[108.]], grad_fn=<SumBackward1>)\n",
      "z_re_sum_dim1: \n",
      " tensor([[27.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [27.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z_re = z.reshape(4,1,1)\n",
    "# （1，1）\n",
    "# （4，1）\n",
    "print()\n",
    "print(\"z_re: \\n\",z_re)\n",
    "\n",
    "# 第0个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_re_sum_dim0 = z_re.sum(dim=0)\n",
    "print(\"z_re_sum_dim0: \\n\",z_re_sum_dim0)\n",
    "\n",
    "# 第1个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_re_sum_dim1 = z_re.sum(dim=1)\n",
    "print(\"z_re_sum_dim1: \\n\",z_re_sum_dim1)\n",
    "\n",
    "\n",
    "# 所有的第二个维度的元素相加\n",
    "# 第二个维度的结构: [[27.]]\n",
    "# 第二个维度数组中包含的所有元素: [27.]\n",
    "\n",
    "# 所有的第二个维度的元素相加结果\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "\n",
    "# 最后变成：\n",
    "# [[27.],\n",
    "#  [27.],\n",
    "#  [27.],\n",
    "#  [27.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期望输出\n",
    "\n",
    "```python\n",
    "z_re: \n",
    " tensor([[[27.]],\n",
    "\n",
    "        [[27.]],\n",
    "\n",
    "        [[27.]],\n",
    "\n",
    "        [[27.]]], grad_fn=<ReshapeAliasBackward0>)\n",
    "z_re_sum_dim0: \n",
    " tensor([[108.]], grad_fn=<SumBackward1>)\n",
    "z_re_sum_dim1: \n",
    " tensor([[27.],\n",
    "        [27.],\n",
    "        [27.],\n",
    "        [27.]], grad_fn=<SumBackward1>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过.requires_grad_()来用in-place的方式改变requires_grad属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000022C1CA041F0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "因为out是一个标量，所以调用backward()时不需要指定求导变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward() # 等价于 b.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看out关于a的梯度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2970, -0.4920],\n",
      "        [ 1.2228, 23.4311]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x_ones.sum()\n",
    "out2.backward()\n",
    "print(x_ones)\n",
    "\n",
    "# out3 = x_ones.sum()\n",
    "x_ones.grad.data.zero_()\n",
    "# out3.backward()\n",
    "print(x_ones.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图 Computer Graph(CG) 是用来描述运算的有向无环图\n",
    "\n",
    "计算图（Computer Graph）是用来描述运算的有向无环图\n",
    "CG包括两个主要元素：结点（Node）、边（Edge）\n",
    "\n",
    "Node表示数据\n",
    "Edge表示运算\n",
    "如果想使用非leaf结点的梯度。则需要在反向传播之前调用.retain_grad() (不用记)\n",
    "\n",
    "**CG作用：方便梯度求取及反向传播**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例\n",
    "#### y = (x+w)*w，则当w=1，x=2时，y对w的grad是多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.])\n",
      "<AddBackward0 object at 0x0000022C1C463FD0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# b = torch.tensor([6.], requires_grad=True)\n",
    "w = torch.tensor([5.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "y = torch.mul(a, w)\n",
    "\n",
    "y = x + (w * w)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二层神经网络的从零开始实现\n",
    "\n",
    "下面，我们一起来动手实现二层神经网络。首先导入实现所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from func.dnn_app_utils_v2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获取和读取数据\n",
    "\n",
    "这里继续使用上一次实验使用的数据集,但本次使用构建神经网络的框架pytorch来进行构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "import cv2\n",
    "# Example of a picture\n",
    "index = 7\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\n",
    "\n",
    "# 标准化\n",
    "\n",
    "train_x_orig = train_x_orig/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义模型参数\n",
    "\n",
    "我们提供的数据集中图像形状为 $64 \\times 64 \\times 3$，类别数为2。本节中我们依然使用长度为 $64 \\times 64 \\times 3 = 12288$ 的向量表示每一张图像。因此，输入个数为$12288$，输出类别个数为2。实验中，我们设超参数隐藏单元个数为256。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens, num_hiddens2 = 12288, 2, 256, 128\n",
    "\n",
    "def params_init_func():\n",
    "    W1_np = np.random.normal(0, 0.01, (num_inputs, num_hiddens))\n",
    "    W2_np = np.random.normal(0, 0.01, (num_hiddens, num_outputs))\n",
    "\n",
    "    W1 = torch.tensor(W1_np, dtype=torch.float, requires_grad = True)\n",
    "    b1 = torch.zeros(num_hiddens, dtype=torch.float, requires_grad = True)\n",
    "\n",
    "    W2 = torch.tensor(W2_np, dtype=torch.float, requires_grad = True)\n",
    "    b2 = torch.zeros(num_outputs, dtype=torch.float, requires_grad = True)\n",
    "\n",
    "    params = [W1, b1, W2, b2]\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义激活函数(该步骤非必须)\n",
    "\n",
    "这里我们使用基础的`max`函数来实现ReLU，而非直接调用`relu`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义模型\n",
    "\n",
    "我们通过`view`函数将每张原始图像改成长度为`num_inputs`的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_inputs, num_outputs, num_hiddens,num_hiddens2 = 12288, 2, 256, 128\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, params_init_func, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        params = params_init_func()\n",
    "        \n",
    "        for param in params:\n",
    "            param.requires_grad_(requires_grad=True)\n",
    "        \n",
    "#         # 确保都是fp32\n",
    "#         nn.Parameter(torch.Tensor)\n",
    "        \n",
    "        self.W1 = nn.Parameter(params[0].float())\n",
    "        self.b1 = nn.Parameter(params[1].float())\n",
    "\n",
    "        self.W2 = nn.Parameter(params[2].float())\n",
    "        self.b2 = nn.Parameter(params[3].float())\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view((-1, num_inputs))\n",
    "        \n",
    "        H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "        out = torch.matmul(H, self.W2) + self.b2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义损失函数\n",
    "\n",
    "为了得到更好的数值稳定性，我们直接使用PyTorch提供的包括softmax运算和交叉熵损失计算的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练模型\n",
    "\n",
    "构建模型,定义optimizer,传递模型需要优化的参数给optimizer,然后开始训练,我们在这里设超参数迭代周期数为500，学习率为0.01。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 0.6732991337776184\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 1 train loss: 0.6632466316223145\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 2 train loss: 0.6570687294006348\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 3 train loss: 0.6532591581344604\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 4 train loss: 0.6507996916770935\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 5 train loss: 0.6491736173629761\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 6 train loss: 0.6479936242103577\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 7 train loss: 0.647074282169342\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 0.64630526304245\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 9 train loss: 0.6456373929977417\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 10 train loss: 0.6450324058532715\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 11 train loss: 0.6444475650787354\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 12 train loss: 0.6438693404197693\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 13 train loss: 0.6432901620864868\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 14 train loss: 0.6427145600318909\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 15 train loss: 0.642140805721283\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 16 train loss: 0.641562283039093\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 17 train loss: 0.640984058380127\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 18 train loss: 0.6404036283493042\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 19 train loss: 0.6398146152496338\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 20 train loss: 0.639214813709259\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 21 train loss: 0.6385955810546875\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 22 train loss: 0.6379752159118652\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 23 train loss: 0.6373741626739502\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 24 train loss: 0.6367700099945068\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 25 train loss: 0.636165976524353\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 26 train loss: 0.6355535387992859\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 27 train loss: 0.6349337100982666\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 28 train loss: 0.6343107223510742\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 29 train loss: 0.6336883902549744\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 30 train loss: 0.633065938949585\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 31 train loss: 0.6324411630630493\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 32 train loss: 0.6318124532699585\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 33 train loss: 0.6311755776405334\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 34 train loss: 0.630525529384613\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 35 train loss: 0.6298614144325256\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 36 train loss: 0.6291880011558533\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 37 train loss: 0.6285074949264526\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 38 train loss: 0.6278218030929565\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 39 train loss: 0.627128541469574\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 40 train loss: 0.6264286041259766\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 41 train loss: 0.6257230639457703\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 42 train loss: 0.6250127553939819\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 43 train loss: 0.624291181564331\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 44 train loss: 0.6235699653625488\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 45 train loss: 0.6228341460227966\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 46 train loss: 0.62209552526474\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 47 train loss: 0.6213380098342896\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 48 train loss: 0.6205835938453674\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 49 train loss: 0.6198267340660095\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 50 train loss: 0.619060754776001\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 51 train loss: 0.6182897090911865\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 52 train loss: 0.6175200343132019\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 53 train loss: 0.6167483329772949\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 54 train loss: 0.615969181060791\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 55 train loss: 0.6151800751686096\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 56 train loss: 0.6143808960914612\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 57 train loss: 0.6135715842247009\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 58 train loss: 0.6127519607543945\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 59 train loss: 0.6119216084480286\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 60 train loss: 0.6110811233520508\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 61 train loss: 0.6102374792098999\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 62 train loss: 0.6093958616256714\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 63 train loss: 0.6085481643676758\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 64 train loss: 0.6076930165290833\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 65 train loss: 0.606833279132843\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 66 train loss: 0.6059621572494507\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 67 train loss: 0.6050825715065002\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 68 train loss: 0.6041954755783081\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 69 train loss: 0.603305995464325\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 70 train loss: 0.6024071574211121\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 71 train loss: 0.6015004515647888\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 72 train loss: 0.6005855798721313\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 73 train loss: 0.5996631979942322\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 74 train loss: 0.5987303256988525\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 75 train loss: 0.5977849364280701\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 76 train loss: 0.5968330502510071\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 77 train loss: 0.5958820581436157\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 78 train loss: 0.5949278473854065\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 79 train loss: 0.5939686894416809\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 80 train loss: 0.5930055975914001\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 81 train loss: 0.592036247253418\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 82 train loss: 0.5910669565200806\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 83 train loss: 0.5900890231132507\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 84 train loss: 0.5891025066375732\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 85 train loss: 0.5881169438362122\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 86 train loss: 0.5871159434318542\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 87 train loss: 0.5861176252365112\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 88 train loss: 0.5851222276687622\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 89 train loss: 0.5841118693351746\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 90 train loss: 0.5830905437469482\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 91 train loss: 0.5820664167404175\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 92 train loss: 0.5810263752937317\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 93 train loss: 0.5799900889396667\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 94 train loss: 0.5789483785629272\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 95 train loss: 0.5779078006744385\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 96 train loss: 0.5768498182296753\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 97 train loss: 0.5757949352264404\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 98 train loss: 0.5747241377830505\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 99 train loss: 0.5736557245254517\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 100 train loss: 0.5725685954093933\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 101 train loss: 0.5714907646179199\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 102 train loss: 0.5703979730606079\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 103 train loss: 0.5693022608757019\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 104 train loss: 0.5682114958763123\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 105 train loss: 0.5671137571334839\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 106 train loss: 0.566005527973175\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 107 train loss: 0.5648958683013916\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 108 train loss: 0.5637748837471008\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 109 train loss: 0.562652051448822\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 110 train loss: 0.5615179538726807\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 111 train loss: 0.5603909492492676\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 112 train loss: 0.559256911277771\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 113 train loss: 0.5581197738647461\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 114 train loss: 0.5569750666618347\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 115 train loss: 0.5558257102966309\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 116 train loss: 0.5546792149543762\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 117 train loss: 0.5535206198692322\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 118 train loss: 0.5523527264595032\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 119 train loss: 0.5512047410011292\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 120 train loss: 0.5500251650810242\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 121 train loss: 0.5488508939743042\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 122 train loss: 0.5476741790771484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 123 train loss: 0.5465039610862732\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 124 train loss: 0.5453164577484131\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 125 train loss: 0.5441393256187439\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 126 train loss: 0.5429511070251465\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 127 train loss: 0.5417601466178894\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 128 train loss: 0.5405526161193848\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 129 train loss: 0.5393572449684143\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 130 train loss: 0.5381600856781006\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 131 train loss: 0.5369699001312256\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 132 train loss: 0.5357480645179749\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 133 train loss: 0.5345392227172852\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 134 train loss: 0.5333343148231506\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 135 train loss: 0.5321389436721802\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 136 train loss: 0.5309057831764221\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 137 train loss: 0.5296834111213684\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 138 train loss: 0.5284431576728821\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 139 train loss: 0.5272015929222107\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 140 train loss: 0.5259714722633362\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 141 train loss: 0.5247309803962708\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 142 train loss: 0.5234721899032593\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 143 train loss: 0.5222268104553223\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 144 train loss: 0.5209752917289734\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 145 train loss: 0.5197015404701233\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 146 train loss: 0.5184473395347595\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 147 train loss: 0.5172122716903687\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 148 train loss: 0.5159608721733093\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 149 train loss: 0.5146873593330383\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 150 train loss: 0.5134364366531372\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 151 train loss: 0.5121791958808899\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 152 train loss: 0.5108908414840698\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 153 train loss: 0.5096473693847656\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 154 train loss: 0.5083824992179871\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 155 train loss: 0.507108747959137\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 156 train loss: 0.5058562755584717\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 157 train loss: 0.5045680403709412\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 158 train loss: 0.5032745599746704\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 159 train loss: 0.5020387768745422\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 160 train loss: 0.5007582306861877\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 161 train loss: 0.49943849444389343\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 162 train loss: 0.49817997217178345\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 163 train loss: 0.4969067871570587\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 164 train loss: 0.49562808871269226\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 165 train loss: 0.49427530169487\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 166 train loss: 0.4930257201194763\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 167 train loss: 0.49175477027893066\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 168 train loss: 0.4905102849006653\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 169 train loss: 0.48924002051353455\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 170 train loss: 0.4880744516849518\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 171 train loss: 0.48682618141174316\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 172 train loss: 0.48570823669433594\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 173 train loss: 0.48443979024887085\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 174 train loss: 0.4832410216331482\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 175 train loss: 0.4820323884487152\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 176 train loss: 0.48085781931877136\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 177 train loss: 0.47958213090896606\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 178 train loss: 0.4784925580024719\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 179 train loss: 0.47730758786201477\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 180 train loss: 0.47628769278526306\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 181 train loss: 0.47502750158309937\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 182 train loss: 0.4741560220718384\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 183 train loss: 0.47302865982055664\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 184 train loss: 0.472079336643219\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 185 train loss: 0.4709470868110657\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 186 train loss: 0.4700976014137268\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 187 train loss: 0.4690195620059967\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 188 train loss: 0.4684009552001953\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 189 train loss: 0.4676651358604431\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 190 train loss: 0.4675244688987732\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 191 train loss: 0.4669472873210907\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 192 train loss: 0.4670308530330658\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 193 train loss: 0.46619316935539246\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 194 train loss: 0.4669782221317291\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 195 train loss: 0.4663635790348053\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 196 train loss: 0.46748027205467224\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 197 train loss: 0.4668101966381073\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 198 train loss: 0.46836110949516296\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 199 train loss: 0.4676365554332733\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 200 train loss: 0.46943798661231995\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 201 train loss: 0.4689441919326782\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 202 train loss: 0.4711511731147766\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 203 train loss: 0.4703103005886078\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 204 train loss: 0.4729940593242645\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 205 train loss: 0.4717845618724823\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 206 train loss: 0.47518235445022583\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 207 train loss: 0.4738743305206299\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 208 train loss: 0.47728219628334045\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 209 train loss: 0.47479209303855896\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 210 train loss: 0.4779515266418457\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 211 train loss: 0.4749886095523834\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 212 train loss: 0.47782570123672485\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 213 train loss: 0.4748201370239258\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 214 train loss: 0.4765557050704956\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 215 train loss: 0.47273218631744385\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 216 train loss: 0.47510576248168945\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 217 train loss: 0.4725725054740906\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 218 train loss: 0.4746650159358978\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 219 train loss: 0.4710609018802643\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 220 train loss: 0.47276145219802856\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 221 train loss: 0.4692516028881073\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 222 train loss: 0.47115257382392883\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 223 train loss: 0.4680057168006897\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 224 train loss: 0.47049957513809204\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 225 train loss: 0.468371719121933\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 226 train loss: 0.4709721505641937\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 227 train loss: 0.4672408699989319\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 228 train loss: 0.47037553787231445\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 229 train loss: 0.4675818681716919\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 230 train loss: 0.47044989466667175\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 231 train loss: 0.4666995108127594\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 232 train loss: 0.46983009576797485\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 233 train loss: 0.46553361415863037\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 234 train loss: 0.4678792953491211\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 235 train loss: 0.46385008096694946\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 236 train loss: 0.46584925055503845\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 237 train loss: 0.46057194471359253\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 238 train loss: 0.4639051854610443\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 239 train loss: 0.4608539640903473\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 240 train loss: 0.4643715023994446\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 241 train loss: 0.46095994114875793\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 242 train loss: 0.464233934879303\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 243 train loss: 0.4602649211883545\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 244 train loss: 0.46286872029304504\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 245 train loss: 0.4577406048774719\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 246 train loss: 0.4604435861110687\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 247 train loss: 0.4570239186286926\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 248 train loss: 0.46030259132385254\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 249 train loss: 0.45590856671333313\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 250 train loss: 0.4591192901134491\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 251 train loss: 0.4551953077316284\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 252 train loss: 0.45766088366508484\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 253 train loss: 0.4525924324989319\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 254 train loss: 0.45479100942611694\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 255 train loss: 0.4510684609413147\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 256 train loss: 0.45400041341781616\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 257 train loss: 0.45006850361824036\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 258 train loss: 0.4518270790576935\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 259 train loss: 0.4476647973060608\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 260 train loss: 0.4497915506362915\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 261 train loss: 0.44647952914237976\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 262 train loss: 0.4478464722633362\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 263 train loss: 0.4446978271007538\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 264 train loss: 0.44584932923316956\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 265 train loss: 0.4423695504665375\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 266 train loss: 0.44293251633644104\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 267 train loss: 0.44017577171325684\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 268 train loss: 0.44090789556503296\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 269 train loss: 0.4388481080532074\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 270 train loss: 0.43984493613243103\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 271 train loss: 0.4378243684768677\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 272 train loss: 0.4389403760433197\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 273 train loss: 0.4373040497303009\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 274 train loss: 0.43877479434013367\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 275 train loss: 0.43757131695747375\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 276 train loss: 0.43861156702041626\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 277 train loss: 0.43524083495140076\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 278 train loss: 0.43622374534606934\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 279 train loss: 0.43410947918891907\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 280 train loss: 0.43608659505844116\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 281 train loss: 0.4321470856666565\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 282 train loss: 0.43397653102874756\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 283 train loss: 0.4321584403514862\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 284 train loss: 0.43432193994522095\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 285 train loss: 0.4307248294353485\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 286 train loss: 0.43195194005966187\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 287 train loss: 0.4296742379665375\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 288 train loss: 0.43131574988365173\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 289 train loss: 0.42726293206214905\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 290 train loss: 0.42929428815841675\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 291 train loss: 0.42756083607673645\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 292 train loss: 0.4291495382785797\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 293 train loss: 0.4247332811355591\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 294 train loss: 0.4257875978946686\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 295 train loss: 0.42241814732551575\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 296 train loss: 0.4241788983345032\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 297 train loss: 0.4208468496799469\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 298 train loss: 0.4223315715789795\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 299 train loss: 0.42003029584884644\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 300 train loss: 0.4217735528945923\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 301 train loss: 0.41786226630210876\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 302 train loss: 0.4198983311653137\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 303 train loss: 0.41793742775917053\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 304 train loss: 0.42004427313804626\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 305 train loss: 0.41628071665763855\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 306 train loss: 0.41825833916664124\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 307 train loss: 0.41651004552841187\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 308 train loss: 0.41779154539108276\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 309 train loss: 0.41433191299438477\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 310 train loss: 0.41575077176094055\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 311 train loss: 0.41037389636039734\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 312 train loss: 0.41377854347229004\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 313 train loss: 0.4119071662425995\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 314 train loss: 0.41436314582824707\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 315 train loss: 0.411070317029953\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 316 train loss: 0.4137405753135681\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 317 train loss: 0.40985560417175293\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 318 train loss: 0.41100701689720154\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 319 train loss: 0.40380141139030457\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 320 train loss: 0.4061693251132965\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 321 train loss: 0.401512086391449\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 322 train loss: 0.4034554064273834\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 323 train loss: 0.4002508521080017\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 324 train loss: 0.4025769829750061\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 325 train loss: 0.3997122049331665\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 326 train loss: 0.4020960330963135\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 327 train loss: 0.399901419878006\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 328 train loss: 0.40243273973464966\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 329 train loss: 0.3994085490703583\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 330 train loss: 0.401533842086792\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 331 train loss: 0.39628133177757263\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 332 train loss: 0.3982373774051666\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 333 train loss: 0.39349737763404846\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 334 train loss: 0.39439430832862854\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 335 train loss: 0.3893093764781952\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 336 train loss: 0.3917354643344879\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 337 train loss: 0.3888418972492218\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 338 train loss: 0.39118704199790955\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 339 train loss: 0.38917431235313416\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 340 train loss: 0.3930690586566925\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 341 train loss: 0.39290452003479004\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 342 train loss: 0.3933493494987488\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 343 train loss: 0.3857770264148712\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 344 train loss: 0.387590229511261\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 345 train loss: 0.3836422860622406\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 346 train loss: 0.38564589619636536\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 347 train loss: 0.3820851147174835\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 348 train loss: 0.38491740822792053\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 349 train loss: 0.38275328278541565\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 350 train loss: 0.38457396626472473\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 351 train loss: 0.3807815611362457\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 352 train loss: 0.38299646973609924\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 353 train loss: 0.38111555576324463\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 354 train loss: 0.38309550285339355\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 355 train loss: 0.3768477439880371\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 356 train loss: 0.3801553547382355\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 357 train loss: 0.38027477264404297\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 358 train loss: 0.38231250643730164\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 359 train loss: 0.3748862147331238\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 360 train loss: 0.3782707154750824\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 361 train loss: 0.3774408996105194\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 362 train loss: 0.37904250621795654\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 363 train loss: 0.3718055784702301\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 364 train loss: 0.3753596544265747\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 365 train loss: 0.3743689954280853\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 366 train loss: 0.3757344186306\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 367 train loss: 0.370918869972229\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 368 train loss: 0.3708842098712921\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 369 train loss: 0.3668258786201477\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 370 train loss: 0.3687966465950012\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 371 train loss: 0.36313238739967346\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 372 train loss: 0.3662670850753784\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 373 train loss: 0.3638502061367035\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 374 train loss: 0.3674367368221283\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 375 train loss: 0.3657079041004181\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 376 train loss: 0.36810335516929626\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 377 train loss: 0.3667724132537842\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 378 train loss: 0.3683187961578369\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 379 train loss: 0.362947940826416\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 380 train loss: 0.3642401099205017\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 381 train loss: 0.3596443235874176\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 382 train loss: 0.3612774610519409\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 383 train loss: 0.3571397066116333\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 384 train loss: 0.3599899411201477\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 385 train loss: 0.35805070400238037\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 386 train loss: 0.35971853137016296\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 387 train loss: 0.35618361830711365\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 388 train loss: 0.3579224944114685\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 389 train loss: 0.3538985252380371\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 390 train loss: 0.355640709400177\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 391 train loss: 0.3524802029132843\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 392 train loss: 0.35437822341918945\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 393 train loss: 0.35028398036956787\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 394 train loss: 0.35117945075035095\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 395 train loss: 0.3469160497188568\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 396 train loss: 0.3500959873199463\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 397 train loss: 0.34791049361228943\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 398 train loss: 0.3498523235321045\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 399 train loss: 0.3478291928768158\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 400 train loss: 0.3505021333694458\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 401 train loss: 0.3488449454307556\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 402 train loss: 0.34943637251853943\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 403 train loss: 0.3432529866695404\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 404 train loss: 0.34568846225738525\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 405 train loss: 0.3426207900047302\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 406 train loss: 0.3447725176811218\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 407 train loss: 0.3409627079963684\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 408 train loss: 0.3410540819168091\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 409 train loss: 0.33619576692581177\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 410 train loss: 0.34040123224258423\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 411 train loss: 0.33935827016830444\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 412 train loss: 0.3412686884403229\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 413 train loss: 0.33809664845466614\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 414 train loss: 0.3402005434036255\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 415 train loss: 0.3355015516281128\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 416 train loss: 0.33698397874832153\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 417 train loss: 0.3351852595806122\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 418 train loss: 0.3373115658760071\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 419 train loss: 0.333121657371521\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 420 train loss: 0.33363258838653564\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 421 train loss: 0.32944971323013306\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 422 train loss: 0.3303200304508209\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 423 train loss: 0.32569658756256104\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 424 train loss: 0.32694557309150696\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 425 train loss: 0.3238738179206848\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 426 train loss: 0.3267885744571686\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 427 train loss: 0.32492396235466003\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 428 train loss: 0.3272908627986908\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 429 train loss: 0.3275097906589508\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 430 train loss: 0.32970717549324036\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 431 train loss: 0.3254883587360382\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 432 train loss: 0.3245481252670288\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 433 train loss: 0.3205653429031372\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 434 train loss: 0.32124313712120056\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 435 train loss: 0.31678250432014465\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 436 train loss: 0.317730188369751\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 437 train loss: 0.3142625689506531\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 438 train loss: 0.3169364333152771\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 439 train loss: 0.31801265478134155\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 440 train loss: 0.31953367590904236\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 441 train loss: 0.3180040717124939\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 442 train loss: 0.31742507219314575\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 443 train loss: 0.31438159942626953\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 444 train loss: 0.3154163658618927\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 445 train loss: 0.3115213215351105\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 446 train loss: 0.312784343957901\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 447 train loss: 0.3088515102863312\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 448 train loss: 0.30911627411842346\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 449 train loss: 0.30602434277534485\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 450 train loss: 0.30862492322921753\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 451 train loss: 0.3100112974643707\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 452 train loss: 0.3110303282737732\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 453 train loss: 0.309123158454895\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 454 train loss: 0.3088547885417938\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 455 train loss: 0.30520015954971313\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 456 train loss: 0.3051528036594391\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 457 train loss: 0.30299630761146545\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 458 train loss: 0.3028225302696228\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 459 train loss: 0.29894450306892395\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 460 train loss: 0.3008459806442261\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 461 train loss: 0.2972266674041748\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 462 train loss: 0.29942214488983154\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 463 train loss: 0.30078890919685364\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 464 train loss: 0.3030266761779785\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 465 train loss: 0.3019033968448639\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 466 train loss: 0.3013850450515747\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 467 train loss: 0.29670652747154236\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 468 train loss: 0.29702815413475037\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 469 train loss: 0.29259923100471497\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 470 train loss: 0.29211801290512085\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 471 train loss: 0.28821417689323425\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 472 train loss: 0.2909417450428009\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 473 train loss: 0.289917528629303\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 474 train loss: 0.290828138589859\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 475 train loss: 0.29204005002975464\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 476 train loss: 0.2938191592693329\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 477 train loss: 0.2925407290458679\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 478 train loss: 0.29230359196662903\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 479 train loss: 0.2884402573108673\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 480 train loss: 0.29078611731529236\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 481 train loss: 0.28822582960128784\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 482 train loss: 0.28734830021858215\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 483 train loss: 0.2848367989063263\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 484 train loss: 0.28503671288490295\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 485 train loss: 0.28119608759880066\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 486 train loss: 0.2807157337665558\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 487 train loss: 0.27711936831474304\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 488 train loss: 0.27864158153533936\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 489 train loss: 0.27689623832702637\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 490 train loss: 0.2773851156234741\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 491 train loss: 0.2755851745605469\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 492 train loss: 0.27818959951400757\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 493 train loss: 0.2803803086280823\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 494 train loss: 0.2807239592075348\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 495 train loss: 0.27850401401519775\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 496 train loss: 0.27958905696868896\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 497 train loss: 0.277282178401947\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 498 train loss: 0.2771685719490051\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 499 train loss: 0.2722434103488922\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "# [[1, 0, 1, 0, .....]]\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 初始化模型\n",
    "net = Net(params_init_func, num_inputs)\n",
    "net.train()\n",
    "\n",
    "# 定义optimizer\n",
    "lr = 0.01\n",
    "optim = SGD(net.parameters(),lr=lr)\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 500\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "# num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "# [1, 0]\n",
    "# [0, 1]\n",
    "\n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "# print(x)\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    j = 0\n",
    "    \n",
    "    # 前向传播\n",
    "    out = net(x)\n",
    "    \n",
    "    # 计算loss\n",
    "    ls = loss(out, y)\n",
    "    print(f\"epoch: {i} train loss:\", ls.item())\n",
    "    \n",
    "    # 反向传播\n",
    "    ls.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optim.step()\n",
    "    \n",
    "    # 清空梯度\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    \n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "#         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "#         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "        print(\"Acc：\", np.mean(result),'\\n')\n",
    "    \n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result))\n",
    "\n",
    "# plot the cost\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      "result: [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True False  True  True\n",
      "  True False  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True False\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True False  True  True  True  True  True\n",
      "  True  True False False False  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True False False  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True]\n",
      "\n",
      "Acc： 0.8708133971291866\n"
     ]
    }
   ],
   "source": [
    "# print(np.argmax(net(x).detach().numpy(),axis=1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(np.argmax(net(x).detach().numpy(), axis=1))\n",
    "\n",
    "    result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "    print(\"result:\",result)\n",
    "    print(\"\\nAcc：\", np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集说明\n",
    "本项目提供了一个数据集(data.h5,用load_data()读取),其中包含：一组标记为cat(1)或non-cat(0)的图像训练集(209张图片,209个标签),一组标记为cat(1)或non-cat(0)的测试图像测试集(50张图片,50个标签). 每个图像形状都是（64, 64, 3），其中3代表3个通道（RGB）\n",
    "\n",
    "## 任务要求\n",
    "\n",
    "### 必选项：\n",
    "\n",
    "**任务0.** 完整阅读一遍本notebook, 填写里面所需的代码\n",
    "\n",
    "**任务1.** 尝试用本实验项目提供的测试集数据(变量名为test_x_orig, test_y)对上面训练好的模型进行测试\n",
    "\n",
    "**任务2.** 按照表格要求调整各项参数，然后进行实验，填写表格\n",
    "\n",
    "**任务3.** 尝试更换优化器，将随机梯度下降(SGD)换成Adam,然后将lr设置成0.0001, 使用训练集对模型进行训练和测试\n",
    "\n",
    "**任务4.** 尝试在神经网络的初始化(init)阶段用torch.nn.linear()定义self.linear，并且用self.linear替换所有的torch.matmul(X, self.W1) + self.b1 \\\n",
    "(使用SGD, lr = 0.001, 使用本项目提供的原版np.random.normal(),不需要变更里面的参数)\n",
    "\n",
    "**任务5.** 搭建一个新模型, 要求如下:\\\n",
    "5-1. 参考上面示例模型搭建步骤，尝试搭建一个4层的模型(4个linear层，其中包括输出层) \\\n",
    "5-2. 每一层的神经元个数为128/256/512/2 (最后为2,因为是二分类) \\\n",
    "5.3. 每一层linear的bias设置为True\n",
    "5-4. 前三层linear,每层输出接一个relu,即一层神经网络为: (linear->relu), 然后整个网络: (linear->relu) * 3 \\-> linear \\\n",
    "5-5. 在模型初始化时，使用xavier参数初始化方法对权重矩阵$W_i$进行初始化，偏置项$b_i$初始化置0 **(下标i，表示第i层)** \\\n",
    "5-6. 优化器使用SGD, lr = 0.001 \\\n",
    "5-7. 设置训练迭代epoch为200 \\\n",
    "5-8. 上述实验结束后,使用Adam作为优化器,lr不变,再进行一次实验,观察结果\n",
    "\n",
    "---\n",
    "\n",
    "### 加分项：\n",
    "\n",
    "**任务7/8/9为一个系列**\n",
    "\n",
    "**任务6.** 在图像数据输入神经网络训练之前，使用torchvision库对图像数据进行增强\n",
    "\n",
    "**任务7.** 将训练集中的209个数据分批(batch)输入训练,要求: 一个batch的大小为10(batch_size = 10), 并且回答为何需要将数据分批训练\n",
    "\n",
    "**任务8.** 使用torch内置的工具,构建一个数据读取器(dataloader)来读取数据\n",
    "\n",
    "**任务9.** 使用**任务8**中的DataLoader替换任务7中自定义的batch数据提取器 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务1:\n",
    "尝试用本实验项目提供的测试集数据对上面训练好的模型进行测试 \\\n",
    "(测试集的图像数据及标签的变量名分别为test_x_orig和test_y)\n",
    "\n",
    "**(将代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始使用测试集测试\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
      " 1 0 0 1 1 1 0 0 0 1 1 1 0]\n",
      "Acc： 0.58 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "test_x_orig = torch.tensor(test_x_orig / 255).float()\n",
    "y_test_true =torch.tensor(np.eye(2)[test_y.reshape(-1)]).float()\n",
    "with torch.no_grad():\n",
    "    #print(np.argmax(net(x).detach().numpy(), axis=1))\n",
    "\n",
    "    #result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    #print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "   # print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "    #print(\"\\nAcc：\", np.mean(result))\n",
    "    print(\"开始使用测试集测试\")\n",
    "    x_test_pred = net(test_x_orig)\n",
    "    result = (np.argmax(x_test_pred.detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "    \n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(x_test_pred.detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y_test_true.numpy(),axis=1))\n",
    "    print(\"Acc：\", np.mean(result),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务2: \n",
    "按照表格要求调整各项参数，然后用训练集进行实验，填写表格 \\\n",
    "**(实验完成后需要提交表格)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务3: \n",
    "尝试更换优化器，将SGD换成Adam,然后使用将lr设置成0.0001,然后使用训练集对模型进行训练和测试 \\\n",
    "**(将优化器定义在下方,并在下方再次编写模型训练的代码,然后训练,保留训练过程中输出的结果)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# 获取数据\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "# 构建Adam优化器\n",
    "lr = 0.0001\n",
    "optim = Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss = 0.7029824256896973\n",
      "epoch 1, train loss = 0.7029824256896973\n",
      "epoch 2, train loss = 0.7029824256896973\n",
      "epoch 3, train loss = 0.7029824256896973\n",
      "epoch 4, train loss = 0.7029824256896973\n",
      "epoch 5, train loss = 0.7029824256896973\n",
      "epoch 6, train loss = 0.7029824256896973\n",
      "epoch 7, train loss = 0.7029824256896973\n",
      "epoch 8, train loss = 0.7029824256896973\n",
      "epoch 9, train loss = 0.7029824256896973\n",
      "epoch 10, train loss = 0.7029824256896973\n",
      "epoch 11, train loss = 0.7029824256896973\n",
      "epoch 12, train loss = 0.7029824256896973\n",
      "epoch 13, train loss = 0.7029824256896973\n",
      "epoch 14, train loss = 0.7029824256896973\n",
      "epoch 15, train loss = 0.7029824256896973\n",
      "epoch 16, train loss = 0.7029824256896973\n",
      "epoch 17, train loss = 0.7029824256896973\n",
      "epoch 18, train loss = 0.7029824256896973\n",
      "epoch 19, train loss = 0.7029824256896973\n",
      "epoch 20, train loss = 0.7029824256896973\n",
      "epoch 21, train loss = 0.7029824256896973\n",
      "epoch 22, train loss = 0.7029824256896973\n",
      "epoch 23, train loss = 0.7029824256896973\n",
      "epoch 24, train loss = 0.7029824256896973\n",
      "epoch 25, train loss = 0.7029824256896973\n",
      "epoch 26, train loss = 0.7029824256896973\n",
      "epoch 27, train loss = 0.7029824256896973\n",
      "epoch 28, train loss = 0.7029824256896973\n",
      "epoch 29, train loss = 0.7029824256896973\n",
      "epoch 30, train loss = 0.7029824256896973\n",
      "epoch 31, train loss = 0.7029824256896973\n",
      "epoch 32, train loss = 0.7029824256896973\n",
      "epoch 33, train loss = 0.7029824256896973\n",
      "epoch 34, train loss = 0.7029824256896973\n",
      "epoch 35, train loss = 0.7029824256896973\n",
      "epoch 36, train loss = 0.7029824256896973\n",
      "epoch 37, train loss = 0.7029824256896973\n",
      "epoch 38, train loss = 0.7029824256896973\n",
      "epoch 39, train loss = 0.7029824256896973\n",
      "epoch 40, train loss = 0.7029824256896973\n",
      "epoch 41, train loss = 0.7029824256896973\n",
      "epoch 42, train loss = 0.7029824256896973\n",
      "epoch 43, train loss = 0.7029824256896973\n",
      "epoch 44, train loss = 0.7029824256896973\n",
      "epoch 45, train loss = 0.7029824256896973\n",
      "epoch 46, train loss = 0.7029824256896973\n",
      "epoch 47, train loss = 0.7029824256896973\n",
      "epoch 48, train loss = 0.7029824256896973\n",
      "epoch 49, train loss = 0.7029824256896973\n",
      "epoch 50, train loss = 0.7029824256896973\n",
      "epoch 51, train loss = 0.7029824256896973\n",
      "epoch 52, train loss = 0.7029824256896973\n",
      "epoch 53, train loss = 0.7029824256896973\n",
      "epoch 54, train loss = 0.7029824256896973\n",
      "epoch 55, train loss = 0.7029824256896973\n",
      "epoch 56, train loss = 0.7029824256896973\n",
      "epoch 57, train loss = 0.7029824256896973\n",
      "epoch 58, train loss = 0.7029824256896973\n",
      "epoch 59, train loss = 0.7029824256896973\n",
      "epoch 60, train loss = 0.7029824256896973\n",
      "epoch 61, train loss = 0.7029824256896973\n",
      "epoch 62, train loss = 0.7029824256896973\n",
      "epoch 63, train loss = 0.7029824256896973\n",
      "epoch 64, train loss = 0.7029824256896973\n",
      "epoch 65, train loss = 0.7029824256896973\n",
      "epoch 66, train loss = 0.7029824256896973\n",
      "epoch 67, train loss = 0.7029824256896973\n",
      "epoch 68, train loss = 0.7029824256896973\n",
      "epoch 69, train loss = 0.7029824256896973\n",
      "epoch 70, train loss = 0.7029824256896973\n",
      "epoch 71, train loss = 0.7029824256896973\n",
      "epoch 72, train loss = 0.7029824256896973\n",
      "epoch 73, train loss = 0.7029824256896973\n",
      "epoch 74, train loss = 0.7029824256896973\n",
      "epoch 75, train loss = 0.7029824256896973\n",
      "epoch 76, train loss = 0.7029824256896973\n",
      "epoch 77, train loss = 0.7029824256896973\n",
      "epoch 78, train loss = 0.7029824256896973\n",
      "epoch 79, train loss = 0.7029824256896973\n",
      "epoch 80, train loss = 0.7029824256896973\n",
      "epoch 81, train loss = 0.7029824256896973\n",
      "epoch 82, train loss = 0.7029824256896973\n",
      "epoch 83, train loss = 0.7029824256896973\n",
      "epoch 84, train loss = 0.7029824256896973\n",
      "epoch 85, train loss = 0.7029824256896973\n",
      "epoch 86, train loss = 0.7029824256896973\n",
      "epoch 87, train loss = 0.7029824256896973\n",
      "epoch 88, train loss = 0.7029824256896973\n",
      "epoch 89, train loss = 0.7029824256896973\n",
      "epoch 90, train loss = 0.7029824256896973\n",
      "epoch 91, train loss = 0.7029824256896973\n",
      "epoch 92, train loss = 0.7029824256896973\n",
      "epoch 93, train loss = 0.7029824256896973\n",
      "epoch 94, train loss = 0.7029824256896973\n",
      "epoch 95, train loss = 0.7029824256896973\n",
      "epoch 96, train loss = 0.7029824256896973\n",
      "epoch 97, train loss = 0.7029824256896973\n",
      "epoch 98, train loss = 0.7029824256896973\n",
      "epoch 99, train loss = 0.7029824256896973\n",
      "epoch 100, train loss = 0.7029824256896973\n",
      "epoch 101, train loss = 0.7029824256896973\n",
      "epoch 102, train loss = 0.7029824256896973\n",
      "epoch 103, train loss = 0.7029824256896973\n",
      "epoch 104, train loss = 0.7029824256896973\n",
      "epoch 105, train loss = 0.7029824256896973\n",
      "epoch 106, train loss = 0.7029824256896973\n",
      "epoch 107, train loss = 0.7029824256896973\n",
      "epoch 108, train loss = 0.7029824256896973\n",
      "epoch 109, train loss = 0.7029824256896973\n",
      "epoch 110, train loss = 0.7029824256896973\n",
      "epoch 111, train loss = 0.7029824256896973\n",
      "epoch 112, train loss = 0.7029824256896973\n",
      "epoch 113, train loss = 0.7029824256896973\n",
      "epoch 114, train loss = 0.7029824256896973\n",
      "epoch 115, train loss = 0.7029824256896973\n",
      "epoch 116, train loss = 0.7029824256896973\n",
      "epoch 117, train loss = 0.7029824256896973\n",
      "epoch 118, train loss = 0.7029824256896973\n",
      "epoch 119, train loss = 0.7029824256896973\n",
      "epoch 120, train loss = 0.7029824256896973\n",
      "epoch 121, train loss = 0.7029824256896973\n",
      "epoch 122, train loss = 0.7029824256896973\n",
      "epoch 123, train loss = 0.7029824256896973\n",
      "epoch 124, train loss = 0.7029824256896973\n",
      "epoch 125, train loss = 0.7029824256896973\n",
      "epoch 126, train loss = 0.7029824256896973\n",
      "epoch 127, train loss = 0.7029824256896973\n",
      "epoch 128, train loss = 0.7029824256896973\n",
      "epoch 129, train loss = 0.7029824256896973\n",
      "epoch 130, train loss = 0.7029824256896973\n",
      "epoch 131, train loss = 0.7029824256896973\n",
      "epoch 132, train loss = 0.7029824256896973\n",
      "epoch 133, train loss = 0.7029824256896973\n",
      "epoch 134, train loss = 0.7029824256896973\n",
      "epoch 135, train loss = 0.7029824256896973\n",
      "epoch 136, train loss = 0.7029824256896973\n",
      "epoch 137, train loss = 0.7029824256896973\n",
      "epoch 138, train loss = 0.7029824256896973\n",
      "epoch 139, train loss = 0.7029824256896973\n",
      "epoch 140, train loss = 0.7029824256896973\n",
      "epoch 141, train loss = 0.7029824256896973\n",
      "epoch 142, train loss = 0.7029824256896973\n",
      "epoch 143, train loss = 0.7029824256896973\n",
      "epoch 144, train loss = 0.7029824256896973\n",
      "epoch 145, train loss = 0.7029824256896973\n",
      "epoch 146, train loss = 0.7029824256896973\n",
      "epoch 147, train loss = 0.7029824256896973\n",
      "epoch 148, train loss = 0.7029824256896973\n",
      "epoch 149, train loss = 0.7029824256896973\n",
      "epoch 150, train loss = 0.7029824256896973\n",
      "epoch 151, train loss = 0.7029824256896973\n",
      "epoch 152, train loss = 0.7029824256896973\n",
      "epoch 153, train loss = 0.7029824256896973\n",
      "epoch 154, train loss = 0.7029824256896973\n",
      "epoch 155, train loss = 0.7029824256896973\n",
      "epoch 156, train loss = 0.7029824256896973\n",
      "epoch 157, train loss = 0.7029824256896973\n",
      "epoch 158, train loss = 0.7029824256896973\n",
      "epoch 159, train loss = 0.7029824256896973\n",
      "epoch 160, train loss = 0.7029824256896973\n",
      "epoch 161, train loss = 0.7029824256896973\n",
      "epoch 162, train loss = 0.7029824256896973\n",
      "epoch 163, train loss = 0.7029824256896973\n",
      "epoch 164, train loss = 0.7029824256896973\n",
      "epoch 165, train loss = 0.7029824256896973\n",
      "epoch 166, train loss = 0.7029824256896973\n",
      "epoch 167, train loss = 0.7029824256896973\n",
      "epoch 168, train loss = 0.7029824256896973\n",
      "epoch 169, train loss = 0.7029824256896973\n",
      "epoch 170, train loss = 0.7029824256896973\n",
      "epoch 171, train loss = 0.7029824256896973\n",
      "epoch 172, train loss = 0.7029824256896973\n",
      "epoch 173, train loss = 0.7029824256896973\n",
      "epoch 174, train loss = 0.7029824256896973\n",
      "epoch 175, train loss = 0.7029824256896973\n",
      "epoch 176, train loss = 0.7029824256896973\n",
      "epoch 177, train loss = 0.7029824256896973\n",
      "epoch 178, train loss = 0.7029824256896973\n",
      "epoch 179, train loss = 0.7029824256896973\n",
      "epoch 180, train loss = 0.7029824256896973\n",
      "epoch 181, train loss = 0.7029824256896973\n",
      "epoch 182, train loss = 0.7029824256896973\n",
      "epoch 183, train loss = 0.7029824256896973\n",
      "epoch 184, train loss = 0.7029824256896973\n",
      "epoch 185, train loss = 0.7029824256896973\n",
      "epoch 186, train loss = 0.7029824256896973\n",
      "epoch 187, train loss = 0.7029824256896973\n",
      "epoch 188, train loss = 0.7029824256896973\n",
      "epoch 189, train loss = 0.7029824256896973\n",
      "epoch 190, train loss = 0.7029824256896973\n",
      "epoch 191, train loss = 0.7029824256896973\n",
      "epoch 192, train loss = 0.7029824256896973\n",
      "epoch 193, train loss = 0.7029824256896973\n",
      "epoch 194, train loss = 0.7029824256896973\n",
      "epoch 195, train loss = 0.7029824256896973\n",
      "epoch 196, train loss = 0.7029824256896973\n",
      "epoch 197, train loss = 0.7029824256896973\n",
      "epoch 198, train loss = 0.7029824256896973\n",
      "epoch 199, train loss = 0.7029824256896973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 200, train loss = 0.7029824256896973\n",
      "epoch 201, train loss = 0.7029824256896973\n",
      "epoch 202, train loss = 0.7029824256896973\n",
      "epoch 203, train loss = 0.7029824256896973\n",
      "epoch 204, train loss = 0.7029824256896973\n",
      "epoch 205, train loss = 0.7029824256896973\n",
      "epoch 206, train loss = 0.7029824256896973\n",
      "epoch 207, train loss = 0.7029824256896973\n",
      "epoch 208, train loss = 0.7029824256896973\n",
      "epoch 209, train loss = 0.7029824256896973\n",
      "epoch 210, train loss = 0.7029824256896973\n",
      "epoch 211, train loss = 0.7029824256896973\n",
      "epoch 212, train loss = 0.7029824256896973\n",
      "epoch 213, train loss = 0.7029824256896973\n",
      "epoch 214, train loss = 0.7029824256896973\n",
      "epoch 215, train loss = 0.7029824256896973\n",
      "epoch 216, train loss = 0.7029824256896973\n",
      "epoch 217, train loss = 0.7029824256896973\n",
      "epoch 218, train loss = 0.7029824256896973\n",
      "epoch 219, train loss = 0.7029824256896973\n",
      "epoch 220, train loss = 0.7029824256896973\n",
      "epoch 221, train loss = 0.7029824256896973\n",
      "epoch 222, train loss = 0.7029824256896973\n",
      "epoch 223, train loss = 0.7029824256896973\n",
      "epoch 224, train loss = 0.7029824256896973\n",
      "epoch 225, train loss = 0.7029824256896973\n",
      "epoch 226, train loss = 0.7029824256896973\n",
      "epoch 227, train loss = 0.7029824256896973\n",
      "epoch 228, train loss = 0.7029824256896973\n",
      "epoch 229, train loss = 0.7029824256896973\n",
      "epoch 230, train loss = 0.7029824256896973\n",
      "epoch 231, train loss = 0.7029824256896973\n",
      "epoch 232, train loss = 0.7029824256896973\n",
      "epoch 233, train loss = 0.7029824256896973\n",
      "epoch 234, train loss = 0.7029824256896973\n",
      "epoch 235, train loss = 0.7029824256896973\n",
      "epoch 236, train loss = 0.7029824256896973\n",
      "epoch 237, train loss = 0.7029824256896973\n",
      "epoch 238, train loss = 0.7029824256896973\n",
      "epoch 239, train loss = 0.7029824256896973\n",
      "epoch 240, train loss = 0.7029824256896973\n",
      "epoch 241, train loss = 0.7029824256896973\n",
      "epoch 242, train loss = 0.7029824256896973\n",
      "epoch 243, train loss = 0.7029824256896973\n",
      "epoch 244, train loss = 0.7029824256896973\n",
      "epoch 245, train loss = 0.7029824256896973\n",
      "epoch 246, train loss = 0.7029824256896973\n",
      "epoch 247, train loss = 0.7029824256896973\n",
      "epoch 248, train loss = 0.7029824256896973\n",
      "epoch 249, train loss = 0.7029824256896973\n",
      "epoch 250, train loss = 0.7029824256896973\n",
      "epoch 251, train loss = 0.7029824256896973\n",
      "epoch 252, train loss = 0.7029824256896973\n",
      "epoch 253, train loss = 0.7029824256896973\n",
      "epoch 254, train loss = 0.7029824256896973\n",
      "epoch 255, train loss = 0.7029824256896973\n",
      "epoch 256, train loss = 0.7029824256896973\n",
      "epoch 257, train loss = 0.7029824256896973\n",
      "epoch 258, train loss = 0.7029824256896973\n",
      "epoch 259, train loss = 0.7029824256896973\n",
      "epoch 260, train loss = 0.7029824256896973\n",
      "epoch 261, train loss = 0.7029824256896973\n",
      "epoch 262, train loss = 0.7029824256896973\n",
      "epoch 263, train loss = 0.7029824256896973\n",
      "epoch 264, train loss = 0.7029824256896973\n",
      "epoch 265, train loss = 0.7029824256896973\n",
      "epoch 266, train loss = 0.7029824256896973\n",
      "epoch 267, train loss = 0.7029824256896973\n",
      "epoch 268, train loss = 0.7029824256896973\n",
      "epoch 269, train loss = 0.7029824256896973\n",
      "epoch 270, train loss = 0.7029824256896973\n",
      "epoch 271, train loss = 0.7029824256896973\n",
      "epoch 272, train loss = 0.7029824256896973\n",
      "epoch 273, train loss = 0.7029824256896973\n",
      "epoch 274, train loss = 0.7029824256896973\n",
      "epoch 275, train loss = 0.7029824256896973\n",
      "epoch 276, train loss = 0.7029824256896973\n",
      "epoch 277, train loss = 0.7029824256896973\n",
      "epoch 278, train loss = 0.7029824256896973\n",
      "epoch 279, train loss = 0.7029824256896973\n",
      "epoch 280, train loss = 0.7029824256896973\n",
      "epoch 281, train loss = 0.7029824256896973\n",
      "epoch 282, train loss = 0.7029824256896973\n",
      "epoch 283, train loss = 0.7029824256896973\n",
      "epoch 284, train loss = 0.7029824256896973\n",
      "epoch 285, train loss = 0.7029824256896973\n",
      "epoch 286, train loss = 0.7029824256896973\n",
      "epoch 287, train loss = 0.7029824256896973\n",
      "epoch 288, train loss = 0.7029824256896973\n",
      "epoch 289, train loss = 0.7029824256896973\n",
      "epoch 290, train loss = 0.7029824256896973\n",
      "epoch 291, train loss = 0.7029824256896973\n",
      "epoch 292, train loss = 0.7029824256896973\n",
      "epoch 293, train loss = 0.7029824256896973\n",
      "epoch 294, train loss = 0.7029824256896973\n",
      "epoch 295, train loss = 0.7029824256896973\n",
      "epoch 296, train loss = 0.7029824256896973\n",
      "epoch 297, train loss = 0.7029824256896973\n",
      "epoch 298, train loss = 0.7029824256896973\n",
      "epoch 299, train loss = 0.7029824256896973\n",
      "epoch 300, train loss = 0.7029824256896973\n",
      "epoch 301, train loss = 0.7029824256896973\n",
      "epoch 302, train loss = 0.7029824256896973\n",
      "epoch 303, train loss = 0.7029824256896973\n",
      "epoch 304, train loss = 0.7029824256896973\n",
      "epoch 305, train loss = 0.7029824256896973\n",
      "epoch 306, train loss = 0.7029824256896973\n",
      "epoch 307, train loss = 0.7029824256896973\n",
      "epoch 308, train loss = 0.7029824256896973\n",
      "epoch 309, train loss = 0.7029824256896973\n",
      "epoch 310, train loss = 0.7029824256896973\n",
      "epoch 311, train loss = 0.7029824256896973\n",
      "epoch 312, train loss = 0.7029824256896973\n",
      "epoch 313, train loss = 0.7029824256896973\n",
      "epoch 314, train loss = 0.7029824256896973\n",
      "epoch 315, train loss = 0.7029824256896973\n",
      "epoch 316, train loss = 0.7029824256896973\n",
      "epoch 317, train loss = 0.7029824256896973\n",
      "epoch 318, train loss = 0.7029824256896973\n",
      "epoch 319, train loss = 0.7029824256896973\n",
      "epoch 320, train loss = 0.7029824256896973\n",
      "epoch 321, train loss = 0.7029824256896973\n",
      "epoch 322, train loss = 0.7029824256896973\n",
      "epoch 323, train loss = 0.7029824256896973\n",
      "epoch 324, train loss = 0.7029824256896973\n",
      "epoch 325, train loss = 0.7029824256896973\n",
      "epoch 326, train loss = 0.7029824256896973\n",
      "epoch 327, train loss = 0.7029824256896973\n",
      "epoch 328, train loss = 0.7029824256896973\n",
      "epoch 329, train loss = 0.7029824256896973\n",
      "epoch 330, train loss = 0.7029824256896973\n",
      "epoch 331, train loss = 0.7029824256896973\n",
      "epoch 332, train loss = 0.7029824256896973\n",
      "epoch 333, train loss = 0.7029824256896973\n",
      "epoch 334, train loss = 0.7029824256896973\n",
      "epoch 335, train loss = 0.7029824256896973\n",
      "epoch 336, train loss = 0.7029824256896973\n",
      "epoch 337, train loss = 0.7029824256896973\n",
      "epoch 338, train loss = 0.7029824256896973\n",
      "epoch 339, train loss = 0.7029824256896973\n",
      "epoch 340, train loss = 0.7029824256896973\n",
      "epoch 341, train loss = 0.7029824256896973\n",
      "epoch 342, train loss = 0.7029824256896973\n",
      "epoch 343, train loss = 0.7029824256896973\n",
      "epoch 344, train loss = 0.7029824256896973\n",
      "epoch 345, train loss = 0.7029824256896973\n",
      "epoch 346, train loss = 0.7029824256896973\n",
      "epoch 347, train loss = 0.7029824256896973\n",
      "epoch 348, train loss = 0.7029824256896973\n",
      "epoch 349, train loss = 0.7029824256896973\n",
      "epoch 350, train loss = 0.7029824256896973\n",
      "epoch 351, train loss = 0.7029824256896973\n",
      "epoch 352, train loss = 0.7029824256896973\n",
      "epoch 353, train loss = 0.7029824256896973\n",
      "epoch 354, train loss = 0.7029824256896973\n",
      "epoch 355, train loss = 0.7029824256896973\n",
      "epoch 356, train loss = 0.7029824256896973\n",
      "epoch 357, train loss = 0.7029824256896973\n",
      "epoch 358, train loss = 0.7029824256896973\n",
      "epoch 359, train loss = 0.7029824256896973\n",
      "epoch 360, train loss = 0.7029824256896973\n",
      "epoch 361, train loss = 0.7029824256896973\n",
      "epoch 362, train loss = 0.7029824256896973\n",
      "epoch 363, train loss = 0.7029824256896973\n",
      "epoch 364, train loss = 0.7029824256896973\n",
      "epoch 365, train loss = 0.7029824256896973\n",
      "epoch 366, train loss = 0.7029824256896973\n",
      "epoch 367, train loss = 0.7029824256896973\n",
      "epoch 368, train loss = 0.7029824256896973\n",
      "epoch 369, train loss = 0.7029824256896973\n",
      "epoch 370, train loss = 0.7029824256896973\n",
      "epoch 371, train loss = 0.7029824256896973\n",
      "epoch 372, train loss = 0.7029824256896973\n",
      "epoch 373, train loss = 0.7029824256896973\n",
      "epoch 374, train loss = 0.7029824256896973\n",
      "epoch 375, train loss = 0.7029824256896973\n",
      "epoch 376, train loss = 0.7029824256896973\n",
      "epoch 377, train loss = 0.7029824256896973\n",
      "epoch 378, train loss = 0.7029824256896973\n",
      "epoch 379, train loss = 0.7029824256896973\n",
      "epoch 380, train loss = 0.7029824256896973\n",
      "epoch 381, train loss = 0.7029824256896973\n",
      "epoch 382, train loss = 0.7029824256896973\n",
      "epoch 383, train loss = 0.7029824256896973\n",
      "epoch 384, train loss = 0.7029824256896973\n",
      "epoch 385, train loss = 0.7029824256896973\n",
      "epoch 386, train loss = 0.7029824256896973\n",
      "epoch 387, train loss = 0.7029824256896973\n",
      "epoch 388, train loss = 0.7029824256896973\n",
      "epoch 389, train loss = 0.7029824256896973\n",
      "epoch 390, train loss = 0.7029824256896973\n",
      "epoch 391, train loss = 0.7029824256896973\n",
      "epoch 392, train loss = 0.7029824256896973\n",
      "epoch 393, train loss = 0.7029824256896973\n",
      "epoch 394, train loss = 0.7029824256896973\n",
      "epoch 395, train loss = 0.7029824256896973\n",
      "epoch 396, train loss = 0.7029824256896973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 397, train loss = 0.7029824256896973\n",
      "epoch 398, train loss = 0.7029824256896973\n",
      "epoch 399, train loss = 0.7029824256896973\n",
      "epoch 400, train loss = 0.7029824256896973\n",
      "epoch 401, train loss = 0.7029824256896973\n",
      "epoch 402, train loss = 0.7029824256896973\n",
      "epoch 403, train loss = 0.7029824256896973\n",
      "epoch 404, train loss = 0.7029824256896973\n",
      "epoch 405, train loss = 0.7029824256896973\n",
      "epoch 406, train loss = 0.7029824256896973\n",
      "epoch 407, train loss = 0.7029824256896973\n",
      "epoch 408, train loss = 0.7029824256896973\n",
      "epoch 409, train loss = 0.7029824256896973\n",
      "epoch 410, train loss = 0.7029824256896973\n",
      "epoch 411, train loss = 0.7029824256896973\n",
      "epoch 412, train loss = 0.7029824256896973\n",
      "epoch 413, train loss = 0.7029824256896973\n",
      "epoch 414, train loss = 0.7029824256896973\n",
      "epoch 415, train loss = 0.7029824256896973\n",
      "epoch 416, train loss = 0.7029824256896973\n",
      "epoch 417, train loss = 0.7029824256896973\n",
      "epoch 418, train loss = 0.7029824256896973\n",
      "epoch 419, train loss = 0.7029824256896973\n",
      "epoch 420, train loss = 0.7029824256896973\n",
      "epoch 421, train loss = 0.7029824256896973\n",
      "epoch 422, train loss = 0.7029824256896973\n",
      "epoch 423, train loss = 0.7029824256896973\n",
      "epoch 424, train loss = 0.7029824256896973\n",
      "epoch 425, train loss = 0.7029824256896973\n",
      "epoch 426, train loss = 0.7029824256896973\n",
      "epoch 427, train loss = 0.7029824256896973\n",
      "epoch 428, train loss = 0.7029824256896973\n",
      "epoch 429, train loss = 0.7029824256896973\n",
      "epoch 430, train loss = 0.7029824256896973\n",
      "epoch 431, train loss = 0.7029824256896973\n",
      "epoch 432, train loss = 0.7029824256896973\n",
      "epoch 433, train loss = 0.7029824256896973\n",
      "epoch 434, train loss = 0.7029824256896973\n",
      "epoch 435, train loss = 0.7029824256896973\n",
      "epoch 436, train loss = 0.7029824256896973\n",
      "epoch 437, train loss = 0.7029824256896973\n",
      "epoch 438, train loss = 0.7029824256896973\n",
      "epoch 439, train loss = 0.7029824256896973\n",
      "epoch 440, train loss = 0.7029824256896973\n",
      "epoch 441, train loss = 0.7029824256896973\n",
      "epoch 442, train loss = 0.7029824256896973\n",
      "epoch 443, train loss = 0.7029824256896973\n",
      "epoch 444, train loss = 0.7029824256896973\n",
      "epoch 445, train loss = 0.7029824256896973\n",
      "epoch 446, train loss = 0.7029824256896973\n",
      "epoch 447, train loss = 0.7029824256896973\n",
      "epoch 448, train loss = 0.7029824256896973\n",
      "epoch 449, train loss = 0.7029824256896973\n",
      "epoch 450, train loss = 0.7029824256896973\n",
      "epoch 451, train loss = 0.7029824256896973\n",
      "epoch 452, train loss = 0.7029824256896973\n",
      "epoch 453, train loss = 0.7029824256896973\n",
      "epoch 454, train loss = 0.7029824256896973\n",
      "epoch 455, train loss = 0.7029824256896973\n",
      "epoch 456, train loss = 0.7029824256896973\n",
      "epoch 457, train loss = 0.7029824256896973\n",
      "epoch 458, train loss = 0.7029824256896973\n",
      "epoch 459, train loss = 0.7029824256896973\n",
      "epoch 460, train loss = 0.7029824256896973\n",
      "epoch 461, train loss = 0.7029824256896973\n",
      "epoch 462, train loss = 0.7029824256896973\n",
      "epoch 463, train loss = 0.7029824256896973\n",
      "epoch 464, train loss = 0.7029824256896973\n",
      "epoch 465, train loss = 0.7029824256896973\n",
      "epoch 466, train loss = 0.7029824256896973\n",
      "epoch 467, train loss = 0.7029824256896973\n",
      "epoch 468, train loss = 0.7029824256896973\n",
      "epoch 469, train loss = 0.7029824256896973\n",
      "epoch 470, train loss = 0.7029824256896973\n",
      "epoch 471, train loss = 0.7029824256896973\n",
      "epoch 472, train loss = 0.7029824256896973\n",
      "epoch 473, train loss = 0.7029824256896973\n",
      "epoch 474, train loss = 0.7029824256896973\n",
      "epoch 475, train loss = 0.7029824256896973\n",
      "epoch 476, train loss = 0.7029824256896973\n",
      "epoch 477, train loss = 0.7029824256896973\n",
      "epoch 478, train loss = 0.7029824256896973\n",
      "epoch 479, train loss = 0.7029824256896973\n",
      "epoch 480, train loss = 0.7029824256896973\n",
      "epoch 481, train loss = 0.7029824256896973\n",
      "epoch 482, train loss = 0.7029824256896973\n",
      "epoch 483, train loss = 0.7029824256896973\n",
      "epoch 484, train loss = 0.7029824256896973\n",
      "epoch 485, train loss = 0.7029824256896973\n",
      "epoch 486, train loss = 0.7029824256896973\n",
      "epoch 487, train loss = 0.7029824256896973\n",
      "epoch 488, train loss = 0.7029824256896973\n",
      "epoch 489, train loss = 0.7029824256896973\n",
      "epoch 490, train loss = 0.7029824256896973\n",
      "epoch 491, train loss = 0.7029824256896973\n",
      "epoch 492, train loss = 0.7029824256896973\n",
      "epoch 493, train loss = 0.7029824256896973\n",
      "epoch 494, train loss = 0.7029824256896973\n",
      "epoch 495, train loss = 0.7029824256896973\n",
      "epoch 496, train loss = 0.7029824256896973\n",
      "epoch 497, train loss = 0.7029824256896973\n",
      "epoch 498, train loss = 0.7029824256896973\n",
      "epoch 499, train loss = 0.7029824256896973\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "\n",
    "# 创建模型对象\n",
    "net = Net(params_init_func, num_inputs)\n",
    "net.train()\n",
    "\n",
    "# 创建损失函数对象\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# epoch数量\n",
    "num_epoch = 500\n",
    "\n",
    "\n",
    "\n",
    "# 开始进行训练\n",
    "for i in range(num_epoch):\n",
    "        \n",
    "        # 前向传播\n",
    "        pred = net(x)\n",
    "        \n",
    "        # 计算loss\n",
    "        ls = loss(pred,y)\n",
    "        \n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "        \n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "        \n",
    "        # 清空梯度\n",
    "        \n",
    "        # 打印\n",
    "        print(f'epoch {i}, train loss = {ls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      "result: [False False  True False  True False False  True False  True False False\n",
      " False  True  True False False False False  True False False False False\n",
      "  True False False  True False  True False False False False False False\n",
      " False False  True False False  True  True False False  True False  True\n",
      " False False  True False False False  True False  True  True False  True\n",
      "  True  True False False False False False False  True False False False\n",
      " False  True  True False False False False False False False False  True\n",
      "  True False False False  True False False False  True  True  True False\n",
      " False  True False False False False False False  True False  True  True\n",
      "  True  True  True  True False False False False  True  True False False\n",
      " False  True False False  True False  True False  True  True False False\n",
      " False  True  True  True  True  True False  True False False  True False\n",
      "  True  True  True False  True  True False False False  True False False\n",
      "  True  True False False False False  True False  True False  True False\n",
      " False  True  True  True False False  True  True False  True False  True\n",
      " False False False False False  True False False  True False False False\n",
      "  True False False False False  True False False  True False False False\n",
      " False  True False False False]\n",
      "\n",
      "Acc： 0.3684210526315789\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(np.argmax(net(x).detach().numpy(), axis=1))\n",
    "\n",
    "    result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "    print(\"result:\",result)\n",
    "    print(\"\\nAcc：\", np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务4:  \n",
    "尝试在神经网络的初始化(init)阶段定义self.linear，并用self.linear替换所有的torch.matmul(X, self.W1) + self.b1 \\\n",
    "(使用SGD, lr = 0.001, 使用本项目提供的原版np.random.normal(),不需要变更里面的参数)\n",
    "\n",
    "#### 1. torch.nn.Linear创建出的对象是一个线性变换层函数： \n",
    "$$Y = WX+b$$\n",
    "\n",
    "```python\n",
    "对象创建方法： linear = torch.nn.Linear(in_features, out_features, bias=True)\n",
    "对象调用方法： 输出tensor = linear(输入tensor)\n",
    "```\n",
    "\n",
    "#### 参数bias=False时，变成：\n",
    "$$Y = WX$$\n",
    "\n",
    "#### 2. 实现Y = WX+b, 本项目初始提供的linear层构建/计算方案：\n",
    "\n",
    "init阶段：\n",
    "```python\n",
    "self.W1 = nn.Parameter(torch.tensor((num_inputs, num_hiddens), dtype=torch.float, requires_grad = True))\n",
    "self.b1 = nn.Parameter(torch.zeros(num_hiddens, dtype=torch.float, requires_grad = True))\n",
    "```     \n",
    "forward阶段：\n",
    "```python\n",
    "Y = torch.matmul(X, self.W1) + self.b1\n",
    "```\n",
    "\n",
    "#### 3. 更加方便的linear层构建/使用方式：\n",
    "\n",
    "init阶段：\n",
    "```python\n",
    "self.linear = torch.nn.Linear(num_inputs, num_hiddens, bias=True)\n",
    "```\n",
    "forward阶段：\n",
    "```python\n",
    "Y = self.linear(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**(将重新构建的模型代码写在下方,并且用训练集进行训练和测试,保留训练过程中输出的结果)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=12288, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建模型\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "# 128/256/512/2\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params_init_func, num_inputs):\n",
    "    def __init__(self, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 替换这个部分\n",
    "#         params = params_init_func()\n",
    "        \n",
    "#         for param in params:\n",
    "#             param.requires_grad_(requires_grad=True)\n",
    "        \n",
    "#         self.W1 = nn.Parameter(params[0].float())\n",
    "#         self.b1 = nn.Parameter(params[1].float())\n",
    "\n",
    "#         self.W2 = nn.Parameter(params[2].float())\n",
    "#         self.b2 = nn.Parameter(params[3].float())\n",
    "\n",
    "        # 代码填空\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "#        self.linear1 = torch.nn.Linear((num_inputs, 128))\n",
    "        \n",
    "\n",
    " #       self.num_inputs = num_inputs\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(num_inputs, num_hiddens, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(num_hiddens, num_outputs, bias=True)\n",
    "        relu=nn.ReLU()\n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "        # 权重初始化\n",
    "        self.linear1.weights = nn.Parameter(torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)),requires_grad = True).float())\n",
    "        self.linear2.weights = nn.Parameter(torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), requires_grad = True).float())\n",
    "        self.linear1.bias = nn.Parameter(torch.tensor(np.zeros((num_hiddens)),requires_grad = True).float())\n",
    "        self.linear2.bias = nn.Parameter(torch.tensor(np.zeros((num_outputs)), requires_grad = True).float())\n",
    "    def forward(self, X):\n",
    "        X = X.view((-1, num_inputs))\n",
    "        \n",
    "#         H = self.linear1(X)\n",
    "#       等同于\n",
    "#       H = torch.matmul(X, self.W1) + self.b1\n",
    "        \n",
    "        \n",
    "        # 替换这个部分\n",
    "#         H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "#         out = torch.matmul(H, self.W2) + self.b2\n",
    "\n",
    "        # 代码填空\n",
    "        out = self.linear2(relu(self.linear1(X)))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "net = Net(num_inputs)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 0.7024520039558411\n",
      "epoch: 1 train loss: 0.7024520039558411\n",
      "epoch: 2 train loss: 0.7024520039558411\n",
      "epoch: 0 train loss: 5.482044696807861\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 1 train loss: 1087.9329833984375\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 2 train loss: 286.6563415527344\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 3 train loss: 0.6269807815551758\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 4 train loss: 0.5850155353546143\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 5 train loss: 0.5820357203483582\n",
      "开始测试\n",
      "Acc： 0.6507177033492823 \n",
      "\n",
      "epoch: 6 train loss: 0.6361196637153625\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 7 train loss: 0.6523911952972412\n",
      "开始测试\n",
      "Acc： 0.6411483253588517 \n",
      "\n",
      "epoch: 8 train loss: 0.6585225462913513\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 9 train loss: 0.5790011286735535\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 10 train loss: 0.5649150609970093\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 11 train loss: 0.5776049494743347\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 12 train loss: 0.6351306438446045\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 13 train loss: 0.6154441237449646\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 14 train loss: 0.5970500111579895\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 15 train loss: 0.5845760107040405\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 16 train loss: 0.5834153294563293\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 17 train loss: 0.6368964314460754\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 18 train loss: 0.5954285860061646\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 19 train loss: 0.5992231369018555\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 20 train loss: 0.623397171497345\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 21 train loss: 0.5795109868049622\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 22 train loss: 0.5476704239845276\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 23 train loss: 0.5244938731193542\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 24 train loss: 0.5773820877075195\n",
      "开始测试\n",
      "Acc： 0.36363636363636365 \n",
      "\n",
      "epoch: 25 train loss: 1.629988670349121\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 26 train loss: 2.896172523498535\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 27 train loss: 0.6602135896682739\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 28 train loss: 0.6551743745803833\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 29 train loss: 0.6437592506408691\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 30 train loss: 0.6102085709571838\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 31 train loss: 0.6834965944290161\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 32 train loss: 0.646223783493042\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 33 train loss: 0.6120458841323853\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 34 train loss: 0.642638087272644\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 35 train loss: 0.5923665761947632\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 36 train loss: 0.6549130082130432\n",
      "开始测试\n",
      "Acc： 0.6363636363636364 \n",
      "\n",
      "epoch: 37 train loss: 0.6394121646881104\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 38 train loss: 0.6743040084838867\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 39 train loss: 0.624210000038147\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 40 train loss: 0.6607028841972351\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 41 train loss: 0.6574647426605225\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 42 train loss: 0.6558631062507629\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 43 train loss: 0.650747537612915\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 44 train loss: 0.645236074924469\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 45 train loss: 0.6397004127502441\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 46 train loss: 0.619035005569458\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 47 train loss: 0.6235288977622986\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 48 train loss: 0.6710118651390076\n",
      "开始测试\n",
      "Acc： 0.6172248803827751 \n",
      "\n",
      "epoch: 49 train loss: 0.676713228225708\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 50 train loss: 0.6605618000030518\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 51 train loss: 0.6564945578575134\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 52 train loss: 0.6591222882270813\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 53 train loss: 0.6570584177970886\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 54 train loss: 0.6497092843055725\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 55 train loss: 0.6473754644393921\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 56 train loss: 0.6456242799758911\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 57 train loss: 0.6449591517448425\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 58 train loss: 0.6407627463340759\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 59 train loss: 0.6348413825035095\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 60 train loss: 0.6236445307731628\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 61 train loss: 0.6066502332687378\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 62 train loss: 0.5955142378807068\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 63 train loss: 0.5959280729293823\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 64 train loss: 0.6388468742370605\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 65 train loss: 0.6280006766319275\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 66 train loss: 0.5646122694015503\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 67 train loss: 3.091893196105957\n",
      "开始测试\n",
      "Acc： 0.4258373205741627 \n",
      "\n",
      "epoch: 68 train loss: 1.0405819416046143\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 69 train loss: 0.6726599335670471\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 70 train loss: 0.6624107360839844\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 71 train loss: 0.657415509223938\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 72 train loss: 0.6552509665489197\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 73 train loss: 0.6532350182533264\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 74 train loss: 1.0072792768478394\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 75 train loss: 0.6792719960212708\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 76 train loss: 0.6791077852249146\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 77 train loss: 0.6790281534194946\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 78 train loss: 0.6787627935409546\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 79 train loss: 0.6785100698471069\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 80 train loss: 0.678365170955658\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 81 train loss: 0.6783207654953003\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 82 train loss: 0.6780816912651062\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 83 train loss: 0.6778622269630432\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 84 train loss: 0.6776976585388184\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 85 train loss: 0.6774935126304626\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 86 train loss: 0.6775168180465698\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 87 train loss: 0.6773348450660706\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 88 train loss: 0.6771204471588135\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 89 train loss: 0.6769171953201294\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 90 train loss: 0.676786482334137\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 91 train loss: 0.6767194271087646\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 92 train loss: 0.6765198111534119\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 93 train loss: 0.6763298511505127\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 94 train loss: 0.676238477230072\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 95 train loss: 0.6761623024940491\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 96 train loss: 0.6759687066078186\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 97 train loss: 0.6758019328117371\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 98 train loss: 0.6756579279899597\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 99 train loss: 0.6754761338233948\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 100 train loss: 0.6753271818161011\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 101 train loss: 0.6752902865409851\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 102 train loss: 0.6749685406684875\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 103 train loss: 0.6746979355812073\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 104 train loss: 0.6745926141738892\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 105 train loss: 0.6742749810218811\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 106 train loss: 0.6740414500236511\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 107 train loss: 0.6737886071205139\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 108 train loss: 0.6735891103744507\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 109 train loss: 0.673668622970581\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 110 train loss: 0.6736249923706055\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 111 train loss: 0.6732168197631836\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 112 train loss: 0.6728375554084778\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 113 train loss: 0.6728622913360596\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 114 train loss: 0.6727343797683716\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 115 train loss: 0.672344982624054\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 116 train loss: 0.6719788312911987\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 117 train loss: 0.6719062924385071\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 118 train loss: 0.6717687249183655\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 119 train loss: 0.6713926792144775\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 120 train loss: 0.6710953712463379\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 121 train loss: 0.670907199382782\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 122 train loss: 0.6706328988075256\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 123 train loss: 0.6705741286277771\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 124 train loss: 0.6703038811683655\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 125 train loss: 0.6699422001838684\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 126 train loss: 0.6695179343223572\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 127 train loss: 0.669059693813324\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 128 train loss: 0.6685853600502014\n",
      "开始测试\n",
      "Acc： 0.6507177033492823 \n",
      "\n",
      "epoch: 129 train loss: 0.6705038547515869\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 130 train loss: 0.6691327095031738\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 131 train loss: 0.6685330271720886\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 132 train loss: 0.667951226234436\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 133 train loss: 0.6674229502677917\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 134 train loss: 0.6668791770935059\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 135 train loss: 0.6662347316741943\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 136 train loss: 0.6642992496490479\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 137 train loss: 0.6607305407524109\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 138 train loss: 0.6498867273330688\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 139 train loss: 0.626384437084198\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 140 train loss: 0.6196937561035156\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 141 train loss: 0.6229076981544495\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 142 train loss: 0.6587908267974854\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 143 train loss: 0.6354420781135559\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 144 train loss: 0.6570289134979248\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 145 train loss: 0.662857711315155\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 146 train loss: 0.6653682589530945\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 147 train loss: 0.6639155149459839\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 148 train loss: 0.6613426208496094\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 149 train loss: 0.6608158946037292\n",
      "开始测试\n",
      "Acc： 0.6507177033492823 \n",
      "\n",
      "epoch: 150 train loss: 0.662886917591095\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 151 train loss: 0.6650539040565491\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 152 train loss: 0.6645190715789795\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 153 train loss: 0.6642680168151855\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 154 train loss: 0.6635943055152893\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 155 train loss: 0.6631704568862915\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 156 train loss: 0.6627776026725769\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 157 train loss: 0.6610153317451477\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 158 train loss: 0.6587092876434326\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 159 train loss: 0.655015230178833\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 160 train loss: 0.6541436910629272\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 161 train loss: 0.6562263369560242\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 162 train loss: 0.6530779004096985\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 163 train loss: 0.6503071188926697\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 164 train loss: 0.6463093161582947\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 165 train loss: 0.6423372626304626\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 166 train loss: 0.6394186019897461\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 167 train loss: 0.6312013268470764\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 168 train loss: 0.62579745054245\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 169 train loss: 0.6231200098991394\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 170 train loss: 0.7493902444839478\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 171 train loss: 0.6610745787620544\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 172 train loss: 0.6627941131591797\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 173 train loss: 0.6596877574920654\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 174 train loss: 0.6591610908508301\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 175 train loss: 0.6613640785217285\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 176 train loss: 0.6661965250968933\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 177 train loss: 0.6658551692962646\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 178 train loss: 0.6645787358283997\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 179 train loss: 0.6637870669364929\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 180 train loss: 0.6630476117134094\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 181 train loss: 0.6599680781364441\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 182 train loss: 0.6574668288230896\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 183 train loss: 0.6597706079483032\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 184 train loss: 0.6564176082611084\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 185 train loss: 0.6567105650901794\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 186 train loss: 0.6578513979911804\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 187 train loss: 0.6586762070655823\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 188 train loss: 0.6654207706451416\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 189 train loss: 0.6652451753616333\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 190 train loss: 0.6650923490524292\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 191 train loss: 0.6649547815322876\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 192 train loss: 0.6647833585739136\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 193 train loss: 0.6620664000511169\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 194 train loss: 0.6594268083572388\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 195 train loss: 0.6530352234840393\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 196 train loss: 0.626824676990509\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 197 train loss: 0.6312486529350281\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 198 train loss: 0.7661281824111938\n",
      "开始测试\n",
      "Acc： 0.430622009569378 \n",
      "\n",
      "epoch: 199 train loss: 1.0992679595947266\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 200 train loss: 0.6737615466117859\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 201 train loss: 0.673695981502533\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 202 train loss: 0.673630952835083\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 203 train loss: 0.6735661625862122\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 204 train loss: 0.673502504825592\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 205 train loss: 0.6734382510185242\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 206 train loss: 0.6733749508857727\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 207 train loss: 0.673311173915863\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 208 train loss: 0.6732460856437683\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 209 train loss: 0.6699352860450745\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 210 train loss: 0.6690055727958679\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 211 train loss: 0.6730592250823975\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 212 train loss: 0.6729739308357239\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 213 train loss: 0.672412097454071\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 214 train loss: 0.6719416379928589\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 215 train loss: 0.6714653372764587\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 216 train loss: 0.6708545088768005\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 217 train loss: 0.6709376573562622\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 218 train loss: 0.6701658964157104\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 219 train loss: 0.6709883809089661\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 220 train loss: 0.6696659922599792\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 221 train loss: 0.6685349941253662\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 222 train loss: 0.6677532196044922\n",
      "开始测试\n",
      "Acc： 0.6363636363636364 \n",
      "\n",
      "epoch: 223 train loss: 0.6701192259788513\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 224 train loss: 0.6710132956504822\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 225 train loss: 0.6699020266532898\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 226 train loss: 0.6689368486404419\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 227 train loss: 0.6664876937866211\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 228 train loss: 0.6645874381065369\n",
      "开始测试\n",
      "Acc： 0.6363636363636364 \n",
      "\n",
      "epoch: 229 train loss: 0.6671397089958191\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 230 train loss: 0.6712574362754822\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 231 train loss: 0.6698353886604309\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 232 train loss: 0.6687869429588318\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 233 train loss: 0.6679899096488953\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 234 train loss: 0.6671411395072937\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 235 train loss: 0.6648411154747009\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 236 train loss: 0.6618284583091736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 237 train loss: 0.6617291569709778\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 238 train loss: 0.6687213778495789\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 239 train loss: 0.667698323726654\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 240 train loss: 0.6667903661727905\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 241 train loss: 0.6649784445762634\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 242 train loss: 0.663267970085144\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 243 train loss: 0.6602240800857544\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 244 train loss: 0.6542856097221375\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 245 train loss: 0.6565294861793518\n",
      "开始测试\n",
      "Acc： 0.6124401913875598 \n",
      "\n",
      "epoch: 246 train loss: 0.6758390069007874\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 247 train loss: 0.6703128814697266\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 248 train loss: 0.6702192425727844\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 249 train loss: 0.6701250076293945\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 250 train loss: 0.6700305342674255\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 251 train loss: 0.6699355244636536\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 252 train loss: 0.669840395450592\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 253 train loss: 0.6697452068328857\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 254 train loss: 0.669649600982666\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 255 train loss: 0.6695542335510254\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 256 train loss: 0.6694586277008057\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 257 train loss: 0.6693633794784546\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 258 train loss: 0.6692680716514587\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 259 train loss: 0.6691731810569763\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 260 train loss: 0.6690784692764282\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 261 train loss: 0.6689842343330383\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 262 train loss: 0.6688902378082275\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 263 train loss: 0.6687970757484436\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 264 train loss: 0.6687043309211731\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 265 train loss: 0.6686066389083862\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 266 train loss: 0.6692883968353271\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 267 train loss: 0.6684585809707642\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 268 train loss: 0.667558491230011\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 269 train loss: 0.6669391393661499\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 270 train loss: 0.6663956642150879\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 271 train loss: 0.66490238904953\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 272 train loss: 0.6640128493309021\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 273 train loss: 0.663425624370575\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 274 train loss: 0.6629924178123474\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 275 train loss: 0.6623240113258362\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 276 train loss: 0.6614179015159607\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 277 train loss: 0.659205973148346\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 278 train loss: 0.6573843955993652\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 279 train loss: 0.6557992696762085\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 280 train loss: 0.6393938064575195\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 281 train loss: 0.721901535987854\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 282 train loss: 0.6654208302497864\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 283 train loss: 0.6640849709510803\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 284 train loss: 0.6627694964408875\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 285 train loss: 0.6616568565368652\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 286 train loss: 0.6600900292396545\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 287 train loss: 0.6586352586746216\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 288 train loss: 0.6569286584854126\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 289 train loss: 0.6535666584968567\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 290 train loss: 0.648637592792511\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 291 train loss: 0.644328773021698\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 292 train loss: 0.6408015489578247\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 293 train loss: 0.6356704831123352\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 294 train loss: 0.628839910030365\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 295 train loss: 0.620351254940033\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 296 train loss: 0.6136314272880554\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 297 train loss: 0.6079519391059875\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 298 train loss: 0.6007223129272461\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 299 train loss: 0.5956467986106873\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 300 train loss: 0.5863156318664551\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 301 train loss: 0.5888676047325134\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 302 train loss: 0.6143525242805481\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 303 train loss: 0.6536449790000916\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 304 train loss: 0.6429336071014404\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 305 train loss: 0.6546623110771179\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 306 train loss: 0.6448952555656433\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 307 train loss: 0.6411468386650085\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 308 train loss: 0.6549351215362549\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 309 train loss: 0.6485833525657654\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 310 train loss: 0.6312275528907776\n",
      "开始测试\n",
      "Acc： 0.5980861244019139 \n",
      "\n",
      "epoch: 311 train loss: 0.6718327403068542\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 312 train loss: 0.641030490398407\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 313 train loss: 0.6160622835159302\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 314 train loss: 0.6018829941749573\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 315 train loss: 0.5828006267547607\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 316 train loss: 0.573228657245636\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 317 train loss: 1.0862210988998413\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 318 train loss: 0.6746549010276794\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 319 train loss: 0.6745604276657104\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 320 train loss: 0.674464762210846\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 321 train loss: 0.6743680238723755\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 322 train loss: 0.6742703318595886\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 323 train loss: 0.6741713881492615\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 324 train loss: 0.6740713715553284\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 325 train loss: 0.6739702224731445\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 326 train loss: 0.6738605499267578\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 327 train loss: 0.6713910698890686\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 328 train loss: 0.6708550453186035\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 329 train loss: 0.6704997420310974\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 330 train loss: 0.6702712774276733\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 331 train loss: 0.6701741814613342\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 332 train loss: 0.6700032353401184\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 333 train loss: 0.6698377132415771\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 334 train loss: 0.6696881055831909\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 335 train loss: 0.6695476174354553\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 336 train loss: 0.669413149356842\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 337 train loss: 0.6692826747894287\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 338 train loss: 0.6691550612449646\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 339 train loss: 0.6690106987953186\n",
      "开始测试\n",
      "Acc： 0.6507177033492823 \n",
      "\n",
      "epoch: 340 train loss: 0.6826953291893005\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 341 train loss: 0.6728507876396179\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 342 train loss: 0.6726887822151184\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 343 train loss: 0.6725332736968994\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 344 train loss: 0.6723828911781311\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 345 train loss: 0.6722375154495239\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 346 train loss: 0.6720958948135376\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 347 train loss: 0.6719582080841064\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 348 train loss: 0.6718236804008484\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 349 train loss: 0.6716943979263306\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 350 train loss: 0.67156583070755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 351 train loss: 0.671439528465271\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 352 train loss: 0.6713159084320068\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 353 train loss: 0.6711947321891785\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 354 train loss: 0.671075701713562\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 355 train loss: 0.6709065437316895\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 356 train loss: 0.6704076528549194\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 357 train loss: 0.6697854995727539\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 358 train loss: 0.6684895753860474\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 359 train loss: 0.6666242480278015\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 360 train loss: 0.6623097062110901\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 361 train loss: 0.6303872466087341\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 362 train loss: 0.6863371729850769\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 363 train loss: 0.6741586923599243\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 364 train loss: 0.6741158962249756\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 365 train loss: 0.6740734577178955\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 366 train loss: 0.6740313172340393\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 367 train loss: 0.6739895939826965\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 368 train loss: 0.6739481687545776\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 369 train loss: 0.6739071011543274\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 370 train loss: 0.6738663911819458\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 371 train loss: 0.6738258600234985\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 372 train loss: 0.6737856268882751\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 373 train loss: 0.6737455725669861\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 374 train loss: 0.6737067699432373\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 375 train loss: 0.6736679673194885\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 376 train loss: 0.6736286878585815\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 377 train loss: 0.6735894083976746\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 378 train loss: 0.6735504269599915\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 379 train loss: 0.6735113859176636\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 380 train loss: 0.6734727025032043\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 381 train loss: 0.6734343767166138\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 382 train loss: 0.6733959913253784\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 383 train loss: 0.6733577847480774\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 384 train loss: 0.6733198165893555\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 385 train loss: 0.6732818484306335\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 386 train loss: 0.6732441186904907\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 387 train loss: 0.6732064485549927\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 388 train loss: 0.6731690168380737\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 389 train loss: 0.6731314659118652\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 390 train loss: 0.6730942726135254\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 391 train loss: 0.6730570793151855\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 392 train loss: 0.6730200052261353\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 393 train loss: 0.6729829907417297\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 394 train loss: 0.6729461550712585\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 395 train loss: 0.6729094386100769\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 396 train loss: 0.6728726625442505\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 397 train loss: 0.6728360652923584\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 398 train loss: 0.6727995872497559\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 399 train loss: 0.6727631688117981\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 400 train loss: 0.6727268695831299\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 401 train loss: 0.6726905703544617\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 402 train loss: 0.6726542711257935\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 403 train loss: 0.6726183891296387\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 404 train loss: 0.6725823283195496\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 405 train loss: 0.67254638671875\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 406 train loss: 0.67251056432724\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 407 train loss: 0.6724746823310852\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 408 train loss: 0.6724389791488647\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 409 train loss: 0.6724032759666443\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 410 train loss: 0.6723678112030029\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 411 train loss: 0.672332227230072\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 412 train loss: 0.6722967624664307\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 413 train loss: 0.6722614169120789\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 414 train loss: 0.6722260117530823\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 415 train loss: 0.67219078540802\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 416 train loss: 0.6721556186676025\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 417 train loss: 0.6721204519271851\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 418 train loss: 0.6720854043960571\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 419 train loss: 0.6720503568649292\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 420 train loss: 0.672015368938446\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 421 train loss: 0.6719805002212524\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 422 train loss: 0.6719455718994141\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 423 train loss: 0.6719107627868652\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 424 train loss: 0.6718760132789612\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 425 train loss: 0.6718413233757019\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 426 train loss: 0.6718066930770874\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 427 train loss: 0.6717720031738281\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 428 train loss: 0.6717376112937927\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 429 train loss: 0.671703040599823\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 430 train loss: 0.671668529510498\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 431 train loss: 0.6716341972351074\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 432 train loss: 0.6715999245643616\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 433 train loss: 0.6715655326843262\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 434 train loss: 0.6715313196182251\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 435 train loss: 0.671497106552124\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 436 train loss: 0.6714630126953125\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 437 train loss: 0.671428918838501\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 438 train loss: 0.6713947653770447\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 439 train loss: 0.6713607907295227\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 440 train loss: 0.6713269352912903\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 441 train loss: 0.6712929606437683\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 442 train loss: 0.6712590456008911\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 443 train loss: 0.6712252497673035\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 444 train loss: 0.6711915135383606\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 445 train loss: 0.671157717704773\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 446 train loss: 0.6711240410804749\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 447 train loss: 0.6710903644561768\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 448 train loss: 0.6710567474365234\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 449 train loss: 0.6710231900215149\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 450 train loss: 0.6709895730018616\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 451 train loss: 0.6709561944007874\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 452 train loss: 0.6709226965904236\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 453 train loss: 0.6708893179893494\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 454 train loss: 0.6708559989929199\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 455 train loss: 0.6708227396011353\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 456 train loss: 0.6707893013954163\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 457 train loss: 0.6707560420036316\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 458 train loss: 0.6707228422164917\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 459 train loss: 0.6706897616386414\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 460 train loss: 0.6706566214561462\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 461 train loss: 0.6706236600875854\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 462 train loss: 0.6705904006958008\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 463 train loss: 0.6705574989318848\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 464 train loss: 0.670524537563324\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 465 train loss: 0.670491635799408\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 466 train loss: 0.6704587936401367\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 467 train loss: 0.6704260110855103\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 468 train loss: 0.6703930497169495\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 469 train loss: 0.670360267162323\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 470 train loss: 0.6703276634216309\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 471 train loss: 0.6702949404716492\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 472 train loss: 0.670262336730957\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 473 train loss: 0.6702296733856201\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 474 train loss: 0.670197069644928\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 475 train loss: 0.6701644659042358\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 476 train loss: 0.670132040977478\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 477 train loss: 0.6700994968414307\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 478 train loss: 0.6700670719146729\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 479 train loss: 0.6700347661972046\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 480 train loss: 0.6700023412704468\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 481 train loss: 0.6699701547622681\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 482 train loss: 0.6699376702308655\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 483 train loss: 0.669905424118042\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 484 train loss: 0.6698732376098633\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 485 train loss: 0.6698410511016846\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 486 train loss: 0.6698089838027954\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 487 train loss: 0.6697767972946167\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 488 train loss: 0.6697449088096619\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 489 train loss: 0.6697128415107727\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 490 train loss: 0.669680655002594\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 491 train loss: 0.6696488261222839\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 492 train loss: 0.6696169376373291\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 493 train loss: 0.6695849895477295\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 494 train loss: 0.6695531606674194\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 495 train loss: 0.6695213317871094\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 496 train loss: 0.6694895029067993\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 497 train loss: 0.669457733631134\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 498 train loss: 0.6694261431694031\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 499 train loss: 0.6693943738937378\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "#     # 前向传播\n",
    "    \n",
    "#     # 计算loss\n",
    "    \n",
    "#     # 反向传播\n",
    "    \n",
    "#     # 更新参数\n",
    "    \n",
    "#     # 清空梯度\n",
    "\n",
    "# 前向传播\n",
    "    out = net(x)\n",
    "    \n",
    "    # 计算loss\n",
    "    ls = loss(out, y)\n",
    "    print(f\"epoch: {i} train loss:\", ls.item())\n",
    "    \n",
    "    # 反向传播\n",
    "    ls.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optim.step()\n",
    "    \n",
    "    # 清空梯度\n",
    "    optim.zero_grad()\n",
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = SGD(net.parameters(),lr=lr)\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 500\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "    \n",
    "train_epochs(net, num_epochs, loss, optim, x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      "result: [ True  True False  True  True  True  True False  True  True  True False\n",
      "  True False False  True  True  True  True False  True  True  True  True\n",
      " False False  True False  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False  True  True  True  True False\n",
      "  True  True False  True  True  True False  True False False  True False\n",
      " False False  True  True  True  True  True  True False  True  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False  True  True  True False  True  True  True False False False  True\n",
      "  True False  True  True  True  True False  True False  True False False\n",
      " False False False False  True  True  True  True  True False  True  True\n",
      "  True False  True  True False  True False  True False False  True  True\n",
      "  True False False False False False  True  True  True  True False  True\n",
      " False False False  True False False  True  True  True False  True  True\n",
      " False  True  True  True  True  True False  True False  True False  True\n",
      "  True False False False  True  True False False  True False  True False\n",
      "  True  True  True  True  True False  True  True False  True  True  True\n",
      " False  True  True  True  True False  True  True False  True  True  True\n",
      "  True  True  True  True  True]\n",
      "\n",
      "Acc： 0.6602870813397129\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "with torch.no_grad():\n",
    "    print(np.argmax(net(x).detach().numpy(), axis=1))\n",
    "\n",
    "    result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "    print(\"result:\",result)\n",
    "    print(\"\\nAcc：\", np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务5: \n",
    "搭建一个新模型, 要求如下:\\\n",
    "5-1. 参考上面示例模型搭建步骤，尝试搭建一个4层的模型(4个linear层，其中包括输出层) \\\n",
    "5-2. 每一层的神经元个数为128/256/512/2 (最后为2,因为是二分类) \\\n",
    "5.3. 每一层linear的bias设置为True\n",
    "5-4. 前三层linear,每层输出接一个relu,即一层神经网络为: (linear->relu), 然后整个网络: (linear->relu) * 3 \\-> linear \\\n",
    "5-5. 在模型初始化时，使用xavier参数初始化方法对权重矩阵$W_i$进行初始化，偏置项$b_i$初始化置0 **(下标i，表示第i层)** \\\n",
    "5-6. 优化器使用SGD, lr = 0.001 \\\n",
    "5-7. 设置训练迭代epoch为200 \\\n",
    "5-8. 上述实验结束后,使用Adam作为优化器,lr不变,再进行一次实验,观察结果\n",
    "\n",
    "#### 与xavier方法相关的参考资料（若仅仅需要使用xavier，一般只用看前两个）：\n",
    "（1）神经网络各种初始化方法（normal，uniform，xavier）的numpy实现以及表现对比\n",
    "https://blog.csdn.net/kane7csdn/article/details/108896031\n",
    "\n",
    "（2）使用torch随机初始化参数 （包括xavier）\n",
    "https://blog.csdn.net/weixin_36893273/article/details/123641399\n",
    "\n",
    "（3）初始化及分布 （对各种初始化参数方法的原理介绍）\n",
    "https://blog.csdn.net/LWD19981223/article/details/124348675\n",
    "\n",
    "---\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (ModuleLis): ModuleList(\n",
       "    (0): Linear(in_features=12288, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 搭建模型和各种工具\n",
    "\n",
    "\n",
    "# 参数初始化\n",
    "# # W1\n",
    "# input_dim= 12288 # = 64 x 64 x 3\n",
    "# output_dim= 128\n",
    "\n",
    "# # W2\n",
    "# input_dim= 128 #\n",
    "# output_dim= 256\n",
    "\n",
    "# # W3\n",
    "# input_dim= 256 #\n",
    "# output_dim= 512\n",
    "\n",
    "# # W4\n",
    "# input_dim= 512 #\n",
    "# output_dim= 2\n",
    "\n",
    "# # in: 12288 out: 128\n",
    "# # 128/256/512/2 \n",
    "    \n",
    "# std = np.sqrt(2. / (input_dim + output_dim))\n",
    "# self.W1 = np.random.normal(loc=0., scale=std, size=[out_dim, in_dim])\n",
    "# self.W2 = np.random.normal(loc=0., scale=std, size=[out_dim, in_dim])\n",
    "\n",
    "# 128/256/512/2\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = 4\n",
    "        self.neuron_num_list = [12288, 128, 256, 512, 2]\n",
    "        \n",
    "        self.ModuleLis = nn.ModuleList()\n",
    "        for i in range(self.layers):\n",
    "            self.ModuleLis.append(torch.nn.Linear(self.neuron_num_list[i], self.neuron_num_list[i+1], bias=True))\n",
    "            \n",
    "            # 权重初始化\n",
    "#             std = np.sqrt(2. / (in_dim + out_dim))\n",
    "#             self.W[layer] = np.random.normal(loc=0., scale=std, size=[out_dim, in_dim])\n",
    "            std = np.sqrt(2. / (self.neuron_num_list[i] + self.neuron_num_list[i+1]))\n",
    "            self.ModuleLis[i].weights = nn.Parameter(torch.tensor(np.random.normal(loc=0., scale=std, size=(self.neuron_num_list[i], self.neuron_num_list[i+1])),requires_grad = True).float())\n",
    "            self.ModuleLis[i].bias = nn.Parameter(torch.tensor(np.zeros((self.neuron_num_list[i+1])), requires_grad = True).float())\n",
    "        \n",
    "        relu=nn.ReLU()\n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = X.view((-1, num_inputs))\n",
    "        for i in range(self.layers - 1):\n",
    "            X = relu(self.ModuleLis[i](X))\n",
    "        return self.ModuleLis[self.layers-1](X)\n",
    "    \n",
    "net = Net(num_inputs)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train_epochs(model, num_epochs, loss_fn, optimizer, train_x, train_y):\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # 前向传播\n",
    "        out = net(x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, y)\n",
    "        print(f\"epoch: {i} train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "        # 测试部分\n",
    "        with torch.no_grad():\n",
    "            print(\"开始测试\")\n",
    "    #         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "            result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    #         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    #         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "            print(\"Acc：\", np.mean(result),'\\n')\n",
    "\n",
    "        # record the cost every 10 training epoch\n",
    "        if i % 10 == 0:\n",
    "            costs.append(ls.item())\n",
    "            accs.append(np.mean(result))\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.plot(np.squeeze(accs))\n",
    "    plt.ylabel('cost/acc')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(lr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 3.9644572734832764\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 1 train loss: 333.8972473144531\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 2 train loss: 142.06419372558594\n",
      "开始测试\n",
      "Acc： 0.5358851674641149 \n",
      "\n",
      "epoch: 3 train loss: 1.6161357164382935\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 4 train loss: 206.95712280273438\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 5 train loss: 41.095703125\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 6 train loss: 107.06780242919922\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 7 train loss: 130.06399536132812\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 102.62747955322266\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 9 train loss: 52.568397521972656\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 10 train loss: 7.797521591186523\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 11 train loss: 4.97502326965332\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 12 train loss: 29.018709182739258\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 13 train loss: 33.54844284057617\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 14 train loss: 21.755115509033203\n",
      "开始测试\n",
      "Acc： 0.5645933014354066 \n",
      "\n",
      "epoch: 15 train loss: 1.4673926830291748\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 16 train loss: 36.63143539428711\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 17 train loss: 26.84635353088379\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 18 train loss: 7.335597515106201\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 19 train loss: 12.810259819030762\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 20 train loss: 7.827208042144775\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 21 train loss: 5.621317386627197\n",
      "开始测试\n",
      "Acc： 0.45454545454545453 \n",
      "\n",
      "epoch: 22 train loss: 0.971153736114502\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 23 train loss: 2.1994094848632812\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 24 train loss: 2.754728317260742\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 25 train loss: 3.4039812088012695\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 26 train loss: 3.2133703231811523\n",
      "开始测试\n",
      "Acc： 0.36363636363636365 \n",
      "\n",
      "epoch: 27 train loss: 2.5095551013946533\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 28 train loss: 1.775466799736023\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 29 train loss: 1.047589898109436\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 30 train loss: 3.6548123359680176\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 31 train loss: 0.6364365220069885\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 32 train loss: 1.9440548419952393\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 33 train loss: 1.1843878030776978\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 34 train loss: 1.9623477458953857\n",
      "开始测试\n",
      "Acc： 0.5645933014354066 \n",
      "\n",
      "epoch: 35 train loss: 0.7415950298309326\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 36 train loss: 1.6983364820480347\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 37 train loss: 1.7551372051239014\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 38 train loss: 0.7764485478401184\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 39 train loss: 1.7592054605484009\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 40 train loss: 0.9977899789810181\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 41 train loss: 1.3657475709915161\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 42 train loss: 1.5252439975738525\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 43 train loss: 0.7142139673233032\n",
      "开始测试\n",
      "Acc： 0.3588516746411483 \n",
      "\n",
      "epoch: 44 train loss: 1.5997189283370972\n",
      "开始测试\n",
      "Acc： 0.430622009569378 \n",
      "\n",
      "epoch: 45 train loss: 0.7317665219306946\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 46 train loss: 1.3776296377182007\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 47 train loss: 1.205631971359253\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 48 train loss: 0.6001135110855103\n",
      "开始测试\n",
      "Acc： 0.49760765550239233 \n",
      "\n",
      "epoch: 49 train loss: 0.8890812397003174\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 50 train loss: 0.6300159692764282\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 51 train loss: 0.732246994972229\n",
      "开始测试\n",
      "Acc： 0.5167464114832536 \n",
      "\n",
      "epoch: 52 train loss: 1.0541657209396362\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 53 train loss: 0.9241887331008911\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 54 train loss: 0.6016683578491211\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 55 train loss: 0.7520429491996765\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 56 train loss: 0.5600442290306091\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 57 train loss: 0.6843193173408508\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 58 train loss: 0.5806145071983337\n",
      "开始测试\n",
      "Acc： 0.6172248803827751 \n",
      "\n",
      "epoch: 59 train loss: 0.6854050755500793\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 60 train loss: 0.5316774845123291\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 61 train loss: 0.5786117315292358\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 62 train loss: 0.5305063724517822\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 63 train loss: 0.5380959510803223\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 64 train loss: 0.511755645275116\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 65 train loss: 0.5148410797119141\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 66 train loss: 0.4948955476284027\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 67 train loss: 0.48793113231658936\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 68 train loss: 0.4380534887313843\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 69 train loss: 0.46525412797927856\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 70 train loss: 0.4238135814666748\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 71 train loss: 0.41301682591438293\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 72 train loss: 0.41001325845718384\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 73 train loss: 0.37604355812072754\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 74 train loss: 0.39110100269317627\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 75 train loss: 0.3734982907772064\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 76 train loss: 0.34417352080345154\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 77 train loss: 0.3574795126914978\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 78 train loss: 0.34882161021232605\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 79 train loss: 0.3142581582069397\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 80 train loss: 0.3011504113674164\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 81 train loss: 0.3126707077026367\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 82 train loss: 0.32548263669013977\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 83 train loss: 0.338480144739151\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 84 train loss: 0.32338064908981323\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 85 train loss: 0.33155739307403564\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 86 train loss: 0.2652561664581299\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 87 train loss: 0.23294399678707123\n",
      "开始测试\n",
      "Acc： 0.9234449760765551 \n",
      "\n",
      "epoch: 88 train loss: 0.23176512122154236\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 89 train loss: 0.2439395934343338\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 90 train loss: 0.25720757246017456\n",
      "开始测试\n",
      "Acc： 0.9234449760765551 \n",
      "\n",
      "epoch: 91 train loss: 0.22717802226543427\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 92 train loss: 0.2017512321472168\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 93 train loss: 0.1811552494764328\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 94 train loss: 0.1798807680606842\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 95 train loss: 0.19786758720874786\n",
      "开始测试\n",
      "Acc： 0.9234449760765551 \n",
      "\n",
      "epoch: 96 train loss: 0.23198503255844116\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 97 train loss: 0.3668261170387268\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 98 train loss: 0.4327702522277832\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 99 train loss: 0.4942234456539154\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 100 train loss: 0.17970316112041473\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 101 train loss: 0.26639336347579956\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 102 train loss: 0.3975270688533783\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 103 train loss: 0.17455118894577026\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 104 train loss: 0.42025986313819885\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 105 train loss: 0.35116061568260193\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 106 train loss: 0.30163252353668213\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 107 train loss: 0.40552183985710144\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 108 train loss: 0.16240324079990387\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 109 train loss: 0.27455538511276245\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 110 train loss: 0.2818996012210846\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 111 train loss: 0.1528262048959732\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 112 train loss: 0.14838775992393494\n",
      "开始测试\n",
      "Acc： 0.9282296650717703 \n",
      "\n",
      "epoch: 113 train loss: 0.19425685703754425\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 114 train loss: 0.14715056121349335\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 115 train loss: 0.13280326128005981\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 116 train loss: 0.12253472954034805\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.9665071770334929 \n",
      "\n",
      "epoch: 117 train loss: 0.12475760281085968\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 118 train loss: 0.13699287176132202\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 119 train loss: 0.11538112908601761\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 120 train loss: 0.12432663887739182\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 121 train loss: 0.08371350169181824\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 122 train loss: 0.09576193243265152\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 123 train loss: 0.09832271188497543\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 124 train loss: 0.08886177837848663\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 125 train loss: 0.0828295648097992\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 126 train loss: 0.06319853663444519\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 127 train loss: 0.06864751875400543\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 128 train loss: 0.06421500444412231\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 129 train loss: 0.062396928668022156\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 130 train loss: 0.05461146682500839\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 131 train loss: 0.04689701646566391\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 132 train loss: 0.05025196820497513\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 133 train loss: 0.045055240392684937\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 134 train loss: 0.04345135763287544\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 135 train loss: 0.03377879783511162\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 136 train loss: 0.032550178468227386\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 137 train loss: 0.02982683666050434\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 138 train loss: 0.030329590663313866\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 139 train loss: 0.0296893622726202\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 140 train loss: 0.025115080177783966\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 141 train loss: 0.024915846064686775\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 142 train loss: 0.021176816895604134\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 143 train loss: 0.021418064832687378\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 144 train loss: 0.018802400678396225\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 145 train loss: 0.018269067630171776\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 146 train loss: 0.016023172065615654\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 147 train loss: 0.01594833843410015\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 148 train loss: 0.01438237726688385\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 149 train loss: 0.01460037287324667\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 150 train loss: 0.013258270919322968\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 151 train loss: 0.013117271475493908\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 152 train loss: 0.012117493897676468\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 153 train loss: 0.011506140232086182\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 154 train loss: 0.010810500010848045\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 155 train loss: 0.010085553862154484\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 156 train loss: 0.009870113804936409\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 157 train loss: 0.009157412685453892\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 158 train loss: 0.00885713566094637\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 159 train loss: 0.008041766472160816\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 160 train loss: 0.0077248429879546165\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 161 train loss: 0.007211773190647364\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 162 train loss: 0.0068894848227500916\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 163 train loss: 0.006689270026981831\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 164 train loss: 0.0062936139293015\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 165 train loss: 0.00600416399538517\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 166 train loss: 0.005625113844871521\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 167 train loss: 0.00535947922617197\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 168 train loss: 0.005130894482135773\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 169 train loss: 0.004839400295168161\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 170 train loss: 0.004637700039893389\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 171 train loss: 0.004400692880153656\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 172 train loss: 0.004207459744066\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 173 train loss: 0.004046387504786253\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 174 train loss: 0.0038308098446577787\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 175 train loss: 0.0036591028328984976\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 176 train loss: 0.003491918323561549\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 177 train loss: 0.0033323385287076235\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 178 train loss: 0.0032307603396475315\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 179 train loss: 0.0031116653699427843\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 180 train loss: 0.002985816914588213\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 181 train loss: 0.0028737529646605253\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 182 train loss: 0.0027611865662038326\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 183 train loss: 0.002662004204466939\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 184 train loss: 0.0025785265024751425\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 185 train loss: 0.002485122764483094\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 186 train loss: 0.0024013810325413942\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 187 train loss: 0.0023357365280389786\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 188 train loss: 0.002265967894345522\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 189 train loss: 0.0021983501501381397\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 190 train loss: 0.0021333431359380484\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 191 train loss: 0.0020656471606343985\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 192 train loss: 0.002008061157539487\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 193 train loss: 0.0019601592794060707\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 194 train loss: 0.0019102550577372313\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 195 train loss: 0.0018595380242913961\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 196 train loss: 0.0018133945995941758\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 197 train loss: 0.001769712194800377\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 198 train loss: 0.0017256991704925895\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 199 train loss: 0.0016828522784635425\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 测试\n",
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = Adam(net.parameters(),lr=lr)\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 200\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "train_epochs(net, num_epochs, loss, optim, x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务6: \n",
    "torchvision是pytorch的一个图形库, 主要用来处理图像数据,对图像数据进行数据增强\n",
    "\n",
    "要求:\n",
    "1. 在图像数据输入神经网络训练之前，使用python torchvision库对图像数据进行增强 \n",
    "2. 然后将增强后的图像数据输入到**本项目提供的2层神经网络中**进行训练和测试 \n",
    "\n",
    "参考资料: \\\n",
    "torch学习 (三十三)：Torch之图像增广 \\\n",
    "https://blog.csdn.net/weixin_44575152/article/details/118056405\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 图像增强\n",
    "import torchvision\n",
    "\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "\n",
    "# def test5(img):\n",
    "#     shape_aug = torchvision.transforms.RandomResizedCrop(200, scale=(0.1, 1), ratio=(0.5, 2))\n",
    "#     color_aug = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "#     aug = torchvision.transforms.Compose(\n",
    "#         [torchvision.transforms.RandomHorizontalFlip(),\n",
    "#         shape_aug,\n",
    "#         color_aug,]\n",
    "#     )\n",
    "    \n",
    "#     apply(img, aug)\n",
    "\n",
    "# test5(train_x_orig[7])\n",
    "\n",
    "# 中间可能会碰到的问题:\n",
    "# np数组格式\n",
    "# (64, 64, 3)\n",
    "\n",
    "# 用torchvision增强需要转换成以下格式吗?\n",
    "# 大家要查资料,做实验去尝试\n",
    "# (1, 64, 64, 3)\n",
    "# (1, 3, 64, 64)\n",
    "# (3, 64, 64)\n",
    "def test5(img):\n",
    "    shape_aug = torchvision.transforms.RandomResizedCrop(64, scale=(0.1, 1), ratio=(0.5, 2))\n",
    "    color_aug = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "    aug = torchvision.transforms.Compose([torchvision.transforms.RandomHorizontalFlip(),shape_aug,color_aug,] )\n",
    "    img=aug(img)\n",
    "    img=img.permute(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "a=torch.tensor(train_x_orig[7])\n",
    "a=a.permute(2, 1, 0)\n",
    "train_x_orig[7]=test5(a)\n",
    "\n",
    "# Example of a picture\n",
    "index = 7\n",
    "\n",
    "print(train_x_orig[index].shape)\n",
    "plt.imshow(train_x_orig[index])\n",
    "\n",
    "# plt.imshow()\n",
    "\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train_epochs(model, num_epochs, loss_fn, optimizer, train_x, train_y):\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # 前向传播\n",
    "        out = net(x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, y)\n",
    "        print(f\"epoch: {i} train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "        # 测试部分\n",
    "        with torch.no_grad():\n",
    "            print(\"开始测试\")\n",
    "    #         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "            result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    #         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    #         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "            print(\"Acc：\", np.mean(result),'\\n')\n",
    "\n",
    "        # record the cost every 10 training epoch\n",
    "        if i % 10 == 0:\n",
    "            costs.append(ls.item())\n",
    "            accs.append(np.mean(result))\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.plot(np.squeeze(accs))\n",
    "    plt.ylabel('cost/acc')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(lr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 1.7640049457550049\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 1 train loss: 380.19354248046875\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 2 train loss: 34.97193145751953\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 3 train loss: 88.07543182373047\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 4 train loss: 80.40899658203125\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 5 train loss: 18.757368087768555\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 6 train loss: 153.93846130371094\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 7 train loss: 94.98471069335938\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 14.749442100524902\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 9 train loss: 36.994728088378906\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 10 train loss: 34.098636627197266\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 11 train loss: 14.954536437988281\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 12 train loss: 25.29116439819336\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 13 train loss: 17.475008010864258\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 14 train loss: 13.063596725463867\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 15 train loss: 20.366474151611328\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 16 train loss: 16.596986770629883\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 17 train loss: 5.603479385375977\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 18 train loss: 16.17488670349121\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 19 train loss: 16.43189239501953\n",
      "开始测试\n",
      "Acc： 0.583732057416268 \n",
      "\n",
      "epoch: 20 train loss: 1.6999328136444092\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 21 train loss: 11.940516471862793\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 22 train loss: 13.07962703704834\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 23 train loss: 10.459010124206543\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 24 train loss: 5.9375457763671875\n",
      "开始测试\n",
      "Acc： 0.36363636363636365 \n",
      "\n",
      "epoch: 25 train loss: 1.3092819452285767\n",
      "开始测试\n",
      "Acc： 0.4258373205741627 \n",
      "\n",
      "epoch: 26 train loss: 1.338305950164795\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 27 train loss: 2.7671432495117188\n",
      "开始测试\n",
      "Acc： 0.3827751196172249 \n",
      "\n",
      "epoch: 28 train loss: 1.5467697381973267\n",
      "开始测试\n",
      "Acc： 0.583732057416268 \n",
      "\n",
      "epoch: 29 train loss: 1.161415457725525\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 30 train loss: 1.3667654991149902\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 31 train loss: 4.454992771148682\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 32 train loss: 2.411240816116333\n",
      "开始测试\n",
      "Acc： 0.3875598086124402 \n",
      "\n",
      "epoch: 33 train loss: 6.124516487121582\n",
      "开始测试\n",
      "Acc： 0.6507177033492823 \n",
      "\n",
      "epoch: 34 train loss: 1.36923086643219\n",
      "开始测试\n",
      "Acc： 0.6507177033492823 \n",
      "\n",
      "epoch: 35 train loss: 1.1731036901474\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 36 train loss: 4.413715362548828\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 37 train loss: 0.6622495055198669\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 38 train loss: 2.3138418197631836\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 39 train loss: 1.3298287391662598\n",
      "开始测试\n",
      "Acc： 0.47368421052631576 \n",
      "\n",
      "epoch: 40 train loss: 1.531720519065857\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 41 train loss: 0.5674217939376831\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 42 train loss: 1.1486361026763916\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 43 train loss: 0.6638614535331726\n",
      "开始测试\n",
      "Acc： 0.507177033492823 \n",
      "\n",
      "epoch: 44 train loss: 1.2626941204071045\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 45 train loss: 0.8008846044540405\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 46 train loss: 0.8910384178161621\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 47 train loss: 0.5674957036972046\n",
      "开始测试\n",
      "Acc： 0.5550239234449761 \n",
      "\n",
      "epoch: 48 train loss: 0.7939221858978271\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 49 train loss: 0.5403438210487366\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 50 train loss: 0.709867000579834\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 51 train loss: 0.5148236155509949\n",
      "开始测试\n",
      "Acc： 0.645933014354067 \n",
      "\n",
      "epoch: 52 train loss: 0.6459461450576782\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 53 train loss: 0.4880269169807434\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 54 train loss: 0.5273366570472717\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 55 train loss: 0.48948684334754944\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 56 train loss: 0.5043398141860962\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 57 train loss: 0.45222488045692444\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 58 train loss: 0.45926401019096375\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 59 train loss: 0.4373551607131958\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 60 train loss: 0.4271010160446167\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 61 train loss: 0.3922620415687561\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 62 train loss: 0.39301350712776184\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 63 train loss: 0.366106241941452\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 64 train loss: 0.3468753397464752\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 65 train loss: 0.34258517622947693\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 66 train loss: 0.30858123302459717\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 67 train loss: 0.30030789971351624\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 68 train loss: 0.28864794969558716\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 69 train loss: 0.2663115859031677\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 70 train loss: 0.24572256207466125\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 71 train loss: 0.24865108728408813\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 72 train loss: 0.22724664211273193\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 73 train loss: 0.21383069455623627\n",
      "开始测试\n",
      "Acc： 0.9186602870813397 \n",
      "\n",
      "epoch: 74 train loss: 0.19857749342918396\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 75 train loss: 0.18336357176303864\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 76 train loss: 0.17726878821849823\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 77 train loss: 0.1572265774011612\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 78 train loss: 0.1462523192167282\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 79 train loss: 0.13207264244556427\n",
      "开始测试\n",
      "Acc： 0.9665071770334929 \n",
      "\n",
      "epoch: 80 train loss: 0.13191983103752136\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 81 train loss: 0.12011536210775375\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 82 train loss: 0.11118889600038528\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 83 train loss: 0.09569379687309265\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 84 train loss: 0.08941170573234558\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 85 train loss: 0.08259659260511398\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 86 train loss: 0.07808079570531845\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 87 train loss: 0.06485915184020996\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 88 train loss: 0.06423395872116089\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 89 train loss: 0.05666017159819603\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 90 train loss: 0.055045243352651596\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 91 train loss: 0.047106578946113586\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 92 train loss: 0.03931500017642975\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 93 train loss: 0.04062114283442497\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 94 train loss: 0.04362944886088371\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 95 train loss: 0.03683485463261604\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 96 train loss: 0.027320869266986847\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 97 train loss: 0.029144927859306335\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 98 train loss: 0.025908788666129112\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 99 train loss: 0.018438270315527916\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 100 train loss: 0.01764914020895958\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 101 train loss: 0.016175661236047745\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 102 train loss: 0.013605584390461445\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 103 train loss: 0.011759444139897823\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 104 train loss: 0.011188386939466\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 105 train loss: 0.010469499044120312\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 106 train loss: 0.009050073102116585\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 107 train loss: 0.007645995356142521\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 108 train loss: 0.007055236492305994\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 109 train loss: 0.006581692025065422\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 110 train loss: 0.0058182282373309135\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 111 train loss: 0.005161584354937077\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 112 train loss: 0.0046311113983392715\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 113 train loss: 0.00418039271607995\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 114 train loss: 0.003839922370389104\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 115 train loss: 0.003526344895362854\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 116 train loss: 0.0032260888256132603\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 117 train loss: 0.0029432603623718023\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 118 train loss: 0.0027068627532571554\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 119 train loss: 0.0025098782498389482\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 120 train loss: 0.0023326186928898096\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 121 train loss: 0.0021515509579330683\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 122 train loss: 0.0019716930110007524\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 1.0 \n",
      "\n",
      "epoch: 123 train loss: 0.0017988086910918355\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 124 train loss: 0.0016566590638831258\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 125 train loss: 0.0015417532995343208\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 126 train loss: 0.0014468551380559802\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 127 train loss: 0.0013603228144347668\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 128 train loss: 0.0012790350010618567\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 129 train loss: 0.0012038301210850477\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 130 train loss: 0.0011341245844960213\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 131 train loss: 0.0010720579884946346\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 132 train loss: 0.0010177709627896547\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 133 train loss: 0.0009626612882129848\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 134 train loss: 0.0009098291047848761\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 135 train loss: 0.0008670007227919996\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 136 train loss: 0.0008305064402520657\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 137 train loss: 0.0007998543442226946\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 138 train loss: 0.0007706647156737745\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 139 train loss: 0.00074132124427706\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 140 train loss: 0.0007118810899555683\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 141 train loss: 0.0006830447819083929\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 142 train loss: 0.0006559978355653584\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 143 train loss: 0.0006317501538433135\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 144 train loss: 0.0006097885780036449\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 145 train loss: 0.0005910326726734638\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 146 train loss: 0.0005724093643948436\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 147 train loss: 0.0005536619573831558\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 148 train loss: 0.0005347543046809733\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 149 train loss: 0.0005162180168554187\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 150 train loss: 0.0004993175389245152\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 151 train loss: 0.00048321831854991615\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 152 train loss: 0.00046811081119813025\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 153 train loss: 0.000454459514003247\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 154 train loss: 0.0004420582263264805\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 155 train loss: 0.0004303444002289325\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 156 train loss: 0.0004191579937469214\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 157 train loss: 0.0004083519452251494\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 158 train loss: 0.0003962767368648201\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 159 train loss: 0.0003847714397124946\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 160 train loss: 0.0003740381798706949\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 161 train loss: 0.00036378653021529317\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 162 train loss: 0.00035369209945201874\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 163 train loss: 0.0003431010991334915\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 164 train loss: 0.0003335429646540433\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 165 train loss: 0.0003241643134970218\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 166 train loss: 0.0003152400313410908\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 167 train loss: 0.00030702707590535283\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 168 train loss: 0.0002980950812343508\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 169 train loss: 0.0002895933866966516\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 170 train loss: 0.00028155939071439207\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 171 train loss: 0.00027411829796619713\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 172 train loss: 0.0002670881222002208\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 173 train loss: 0.0002599236322566867\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 174 train loss: 0.00025332419318147004\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 175 train loss: 0.000246854149736464\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 176 train loss: 0.00024137928267009556\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 177 train loss: 0.00023596726532559842\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 178 train loss: 0.00023013172904029489\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 179 train loss: 0.00022420604364015162\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 180 train loss: 0.00021904388268012553\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 181 train loss: 0.00021409204055089504\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 182 train loss: 0.0002095298405038193\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 183 train loss: 0.00020459842926356941\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 184 train loss: 0.00019981912919320166\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 185 train loss: 0.00019541107758414\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 186 train loss: 0.0001914681779453531\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 187 train loss: 0.00018742858082987368\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 188 train loss: 0.00018272947636432946\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 189 train loss: 0.00017852750897873193\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 190 train loss: 0.00017488708544988185\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 191 train loss: 0.0001711460208753124\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 192 train loss: 0.00016697894898243248\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 193 train loss: 0.00016323142335750163\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 194 train loss: 0.00016024272190406919\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 195 train loss: 0.00015725736739113927\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 196 train loss: 0.0001536366471555084\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 197 train loss: 0.00015007505135145038\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 198 train loss: 0.0001477126934332773\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 199 train loss: 0.00014487568114418536\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 测试\n",
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = Adam(net.parameters(),lr=lr)\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 200\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "train_epochs(net, num_epochs, loss, optim, x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务7: \n",
    "我们在之前的实验中,均是一次性将209个图片输入到将训练集中的209个数据分批(batch)输入到**本项目提供的2层神经网络中**训练 \n",
    "\n",
    "要求: 一个batch的大小为10(batch_size = 10), 并且查资料/思考之后回答: 为何需要将数据分批训练 \n",
    "\n",
    "tips:\n",
    "1. 需要关注如何切分ndarray \n",
    "2. batch_size为10,将会分为21批(batch)的数据,最后一个batch的size如何通过程序计算(要求不能用if语句**直接指定**最后一个batchsize为9) \\\n",
    "(用取余操作来算出最后一个batch的size) \n",
    "3. 需要在迭代epoch的for循环中,再增加一层for循环来遍历各个batch的数据\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "#print(train_x_orig.shape)\n",
    "# 209\n",
    "\n",
    "# x.shape -> (209, 64, 64, 3) tuple\n",
    "# batch_size = 10\n",
    "# num_of_batch = int(x.shape[0]/10) # 20\n",
    "\n",
    "# if x.shape[0] % 10 != 0: num_of_batch = num_of_batch + 1\n",
    "\n",
    "# for i in range(num_epochs):\n",
    "#     j = 0\n",
    "    \n",
    "#     for j in range(num_of_batch):\n",
    "#         x_batch_j = x[?,:] # 得到 x_batch_j -> (10, 64, 64, 3)? 或者该epoch的最后一组数据,它的shape (9, 64, 64, 3)\n",
    "#     # 前向传播\n",
    "#         out = net(x_batch_j)\n",
    "    \n",
    "#         ...\n",
    "\n",
    "\n",
    "# 如何拿出每一批10个数据去做训练?\n",
    "# (10, 64, 64, 3)\n",
    "# 定义模型/loss/optimizer,设定epochs/batchsize.....\n",
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = Adam(net.parameters(),lr=lr)\n",
    "# 定义epoch数量\n",
    "num_epochs = 200\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "\n",
    "# 数据\n",
    "y = torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "# batch_size设置\n",
    "batch_size = 10\n",
    "\n",
    "# 分开每batch数据\n",
    "batch_number = int(y.shape[0]/10)\n",
    "\n",
    "# 假如不能整除,则last_batch_data_number表示: 该epoch中,最后一个batch的数据的数量\n",
    "last_batch_data_number = y.shape[0] % 10\n",
    "\n",
    "if last_batch_data_number != 0: batch_number = batch_number + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 0, batch:10, train loss: 0.8113876581192017\n",
      "epoch: 0, batch:20, train loss: 0.4941374957561493\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 1, batch:0, train loss: 0.5505307912826538\n",
      "epoch: 1, batch:10, train loss: 0.8114047050476074\n",
      "epoch: 1, batch:20, train loss: 0.494122713804245\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 2, batch:0, train loss: 0.550521731376648\n",
      "epoch: 2, batch:10, train loss: 0.8114201426506042\n",
      "epoch: 2, batch:20, train loss: 0.49410921335220337\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 3, batch:0, train loss: 0.5505134463310242\n",
      "epoch: 3, batch:10, train loss: 0.8114343881607056\n",
      "epoch: 3, batch:20, train loss: 0.494096964597702\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 4, batch:0, train loss: 0.550506055355072\n",
      "epoch: 4, batch:10, train loss: 0.8114471435546875\n",
      "epoch: 4, batch:20, train loss: 0.4940859377384186\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 5, batch:0, train loss: 0.5504991412162781\n",
      "epoch: 5, batch:10, train loss: 0.811458945274353\n",
      "epoch: 5, batch:20, train loss: 0.4940757751464844\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 6, batch:0, train loss: 0.550493061542511\n",
      "epoch: 6, batch:10, train loss: 0.8114694356918335\n",
      "epoch: 6, batch:20, train loss: 0.4940665662288666\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 7, batch:0, train loss: 0.5504873991012573\n",
      "epoch: 7, batch:10, train loss: 0.8114792108535767\n",
      "epoch: 7, batch:20, train loss: 0.49405819177627563\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 8, batch:0, train loss: 0.5504823327064514\n",
      "epoch: 8, batch:10, train loss: 0.8114879727363586\n",
      "epoch: 8, batch:20, train loss: 0.49405062198638916\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 9, batch:0, train loss: 0.5504776239395142\n",
      "epoch: 9, batch:10, train loss: 0.8114959597587585\n",
      "epoch: 9, batch:20, train loss: 0.4940436780452728\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 10, batch:0, train loss: 0.5504733920097351\n",
      "epoch: 10, batch:10, train loss: 0.8115032911300659\n",
      "epoch: 10, batch:20, train loss: 0.4940374195575714\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 11, batch:0, train loss: 0.5504695177078247\n",
      "epoch: 11, batch:10, train loss: 0.811509907245636\n",
      "epoch: 11, batch:20, train loss: 0.494031697511673\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 12, batch:0, train loss: 0.550466001033783\n",
      "epoch: 12, batch:10, train loss: 0.8115159273147583\n",
      "epoch: 12, batch:20, train loss: 0.4940265417098999\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 13, batch:0, train loss: 0.5504629015922546\n",
      "epoch: 13, batch:10, train loss: 0.8115213513374329\n",
      "epoch: 13, batch:20, train loss: 0.49402177333831787\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 14, batch:0, train loss: 0.5504599809646606\n",
      "epoch: 14, batch:10, train loss: 0.8115262985229492\n",
      "epoch: 14, batch:20, train loss: 0.49401748180389404\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 15, batch:0, train loss: 0.5504573583602905\n",
      "epoch: 15, batch:10, train loss: 0.8115308880805969\n",
      "epoch: 15, batch:20, train loss: 0.49401357769966125\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 16, batch:0, train loss: 0.5504549145698547\n",
      "epoch: 16, batch:10, train loss: 0.8115350008010864\n",
      "epoch: 16, batch:20, train loss: 0.4940100312232971\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 17, batch:0, train loss: 0.5504528284072876\n",
      "epoch: 17, batch:10, train loss: 0.8115386962890625\n",
      "epoch: 17, batch:20, train loss: 0.49400678277015686\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 18, batch:0, train loss: 0.5504506826400757\n",
      "epoch: 18, batch:10, train loss: 0.8115420341491699\n",
      "epoch: 18, batch:20, train loss: 0.49400389194488525\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 19, batch:0, train loss: 0.550449013710022\n",
      "epoch: 19, batch:10, train loss: 0.811545193195343\n",
      "epoch: 19, batch:20, train loss: 0.494001179933548\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 20, batch:0, train loss: 0.5504473447799683\n",
      "epoch: 20, batch:10, train loss: 0.8115480542182922\n",
      "epoch: 20, batch:20, train loss: 0.49399879574775696\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 21, batch:0, train loss: 0.5504458546638489\n",
      "epoch: 21, batch:10, train loss: 0.811550498008728\n",
      "epoch: 21, batch:20, train loss: 0.4939965605735779\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 22, batch:0, train loss: 0.550444483757019\n",
      "epoch: 22, batch:10, train loss: 0.811552882194519\n",
      "epoch: 22, batch:20, train loss: 0.4939945638179779\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 23, batch:0, train loss: 0.5504432916641235\n",
      "epoch: 23, batch:10, train loss: 0.8115550875663757\n",
      "epoch: 23, batch:20, train loss: 0.4939926564693451\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 24, batch:0, train loss: 0.550442099571228\n",
      "epoch: 24, batch:10, train loss: 0.8115569353103638\n",
      "epoch: 24, batch:20, train loss: 0.49399101734161377\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 25, batch:0, train loss: 0.5504411458969116\n",
      "epoch: 25, batch:10, train loss: 0.8115586042404175\n",
      "epoch: 25, batch:20, train loss: 0.4939895272254944\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 26, batch:0, train loss: 0.55044025182724\n",
      "epoch: 26, batch:10, train loss: 0.8115602731704712\n",
      "epoch: 26, batch:20, train loss: 0.4939880967140198\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 27, batch:0, train loss: 0.5504393577575684\n",
      "epoch: 27, batch:10, train loss: 0.8115617632865906\n",
      "epoch: 27, batch:20, train loss: 0.49398696422576904\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 28, batch:0, train loss: 0.5504385232925415\n",
      "epoch: 28, batch:10, train loss: 0.8115631341934204\n",
      "epoch: 28, batch:20, train loss: 0.49398571252822876\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 29, batch:0, train loss: 0.5504379272460938\n",
      "epoch: 29, batch:10, train loss: 0.8115642666816711\n",
      "epoch: 29, batch:20, train loss: 0.49398475885391235\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 30, batch:0, train loss: 0.550437331199646\n",
      "epoch: 30, batch:10, train loss: 0.8115653991699219\n",
      "epoch: 30, batch:20, train loss: 0.49398374557495117\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 31, batch:0, train loss: 0.5504366755485535\n",
      "epoch: 31, batch:10, train loss: 0.8115663528442383\n",
      "epoch: 31, batch:20, train loss: 0.4939829409122467\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 32, batch:0, train loss: 0.5504361391067505\n",
      "epoch: 32, batch:10, train loss: 0.8115671873092651\n",
      "epoch: 32, batch:20, train loss: 0.4939820468425751\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 33, batch:0, train loss: 0.5504356622695923\n",
      "epoch: 33, batch:10, train loss: 0.8115680813789368\n",
      "epoch: 33, batch:20, train loss: 0.49398136138916016\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 34, batch:0, train loss: 0.5504352450370789\n",
      "epoch: 34, batch:10, train loss: 0.8115688562393188\n",
      "epoch: 34, batch:20, train loss: 0.49398073554039\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 35, batch:0, train loss: 0.5504348278045654\n",
      "epoch: 35, batch:10, train loss: 0.8115695118904114\n",
      "epoch: 35, batch:20, train loss: 0.49398019909858704\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 36, batch:0, train loss: 0.5504344701766968\n",
      "epoch: 36, batch:10, train loss: 0.8115701675415039\n",
      "epoch: 36, batch:20, train loss: 0.4939796030521393\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 37, batch:0, train loss: 0.5504341125488281\n",
      "epoch: 37, batch:10, train loss: 0.8115707635879517\n",
      "epoch: 37, batch:20, train loss: 0.4939791262149811\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 38, batch:0, train loss: 0.5504339337348938\n",
      "epoch: 38, batch:10, train loss: 0.8115712404251099\n",
      "epoch: 38, batch:20, train loss: 0.4939786493778229\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 39, batch:0, train loss: 0.5504335761070251\n",
      "epoch: 39, batch:10, train loss: 0.8115715980529785\n",
      "epoch: 39, batch:20, train loss: 0.4939782917499542\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 40, batch:0, train loss: 0.550433337688446\n",
      "epoch: 40, batch:10, train loss: 0.8115721940994263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, batch:20, train loss: 0.4939779043197632\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 41, batch:0, train loss: 0.5504331588745117\n",
      "epoch: 41, batch:10, train loss: 0.8115724325180054\n",
      "epoch: 41, batch:20, train loss: 0.4939776062965393\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 42, batch:0, train loss: 0.5504329800605774\n",
      "epoch: 42, batch:10, train loss: 0.8115728497505188\n",
      "epoch: 42, batch:20, train loss: 0.49397727847099304\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 43, batch:0, train loss: 0.5504327416419983\n",
      "epoch: 43, batch:10, train loss: 0.8115731477737427\n",
      "epoch: 43, batch:20, train loss: 0.4939769506454468\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 44, batch:0, train loss: 0.550432562828064\n",
      "epoch: 44, batch:10, train loss: 0.8115733861923218\n",
      "epoch: 44, batch:20, train loss: 0.49397674202919006\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 45, batch:0, train loss: 0.5504325032234192\n",
      "epoch: 45, batch:10, train loss: 0.8115736842155457\n",
      "epoch: 45, batch:20, train loss: 0.49397653341293335\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 46, batch:0, train loss: 0.5504323244094849\n",
      "epoch: 46, batch:10, train loss: 0.81157386302948\n",
      "epoch: 46, batch:20, train loss: 0.4939763844013214\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 47, batch:0, train loss: 0.5504322052001953\n",
      "epoch: 47, batch:10, train loss: 0.8115741014480591\n",
      "epoch: 47, batch:20, train loss: 0.49397626519203186\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 48, batch:0, train loss: 0.550432026386261\n",
      "epoch: 48, batch:10, train loss: 0.8115742802619934\n",
      "epoch: 48, batch:20, train loss: 0.49397599697113037\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 49, batch:0, train loss: 0.5504319071769714\n",
      "epoch: 49, batch:10, train loss: 0.8115744590759277\n",
      "epoch: 49, batch:20, train loss: 0.49397578835487366\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 50, batch:0, train loss: 0.5504319071769714\n",
      "epoch: 50, batch:10, train loss: 0.8115747570991516\n",
      "epoch: 50, batch:20, train loss: 0.4939756989479065\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 51, batch:0, train loss: 0.5504317283630371\n",
      "epoch: 51, batch:10, train loss: 0.8115748167037964\n",
      "epoch: 51, batch:20, train loss: 0.49397552013397217\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 52, batch:0, train loss: 0.5504316687583923\n",
      "epoch: 52, batch:10, train loss: 0.8115750551223755\n",
      "epoch: 52, batch:20, train loss: 0.49397537112236023\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 53, batch:0, train loss: 0.5504316091537476\n",
      "epoch: 53, batch:10, train loss: 0.8115751147270203\n",
      "epoch: 53, batch:20, train loss: 0.49397528171539307\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 54, batch:0, train loss: 0.5504315495491028\n",
      "epoch: 54, batch:10, train loss: 0.8115751147270203\n",
      "epoch: 54, batch:20, train loss: 0.4939751625061035\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 55, batch:0, train loss: 0.5504314303398132\n",
      "epoch: 55, batch:10, train loss: 0.8115752339363098\n",
      "epoch: 55, batch:20, train loss: 0.49397510290145874\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 56, batch:0, train loss: 0.5504313707351685\n",
      "epoch: 56, batch:10, train loss: 0.8115754127502441\n",
      "epoch: 56, batch:20, train loss: 0.49397504329681396\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 57, batch:0, train loss: 0.5504313707351685\n",
      "epoch: 57, batch:10, train loss: 0.8115755319595337\n",
      "epoch: 57, batch:20, train loss: 0.4939749538898468\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 58, batch:0, train loss: 0.5504313707351685\n",
      "epoch: 58, batch:10, train loss: 0.8115755319595337\n",
      "epoch: 58, batch:20, train loss: 0.4939749538898468\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 59, batch:0, train loss: 0.5504312515258789\n",
      "epoch: 59, batch:10, train loss: 0.8115755915641785\n",
      "epoch: 59, batch:20, train loss: 0.49397483468055725\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 60, batch:0, train loss: 0.5504312515258789\n",
      "epoch: 60, batch:10, train loss: 0.811575710773468\n",
      "epoch: 60, batch:20, train loss: 0.493974894285202\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 61, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 61, batch:10, train loss: 0.811575710773468\n",
      "epoch: 61, batch:20, train loss: 0.49397480487823486\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 62, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 62, batch:10, train loss: 0.8115757703781128\n",
      "epoch: 62, batch:20, train loss: 0.4939747452735901\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 63, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 63, batch:10, train loss: 0.811575710773468\n",
      "epoch: 63, batch:20, train loss: 0.49397480487823486\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 64, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 64, batch:10, train loss: 0.8115757703781128\n",
      "epoch: 64, batch:20, train loss: 0.49397480487823486\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 65, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 65, batch:10, train loss: 0.8115758895874023\n",
      "epoch: 65, batch:20, train loss: 0.4939746856689453\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 66, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 66, batch:10, train loss: 0.8115760087966919\n",
      "epoch: 66, batch:20, train loss: 0.4939746856689453\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 67, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 67, batch:10, train loss: 0.8115758895874023\n",
      "epoch: 67, batch:20, train loss: 0.4939746856689453\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 68, batch:0, train loss: 0.5504311323165894\n",
      "epoch: 68, batch:10, train loss: 0.8115758895874023\n",
      "epoch: 68, batch:20, train loss: 0.4939746856689453\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 69, batch:0, train loss: 0.5504310131072998\n",
      "epoch: 69, batch:10, train loss: 0.8115758895874023\n",
      "epoch: 69, batch:20, train loss: 0.49397462606430054\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 70, batch:0, train loss: 0.5504310727119446\n",
      "epoch: 70, batch:10, train loss: 0.8115760684013367\n",
      "epoch: 70, batch:20, train loss: 0.4939745366573334\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 71, batch:0, train loss: 0.5504310727119446\n",
      "epoch: 71, batch:10, train loss: 0.8115760684013367\n",
      "epoch: 71, batch:20, train loss: 0.4939745366573334\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 72, batch:0, train loss: 0.5504310727119446\n",
      "epoch: 72, batch:10, train loss: 0.8115760087966919\n",
      "epoch: 72, batch:20, train loss: 0.49397456645965576\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 73, batch:0, train loss: 0.550430953502655\n",
      "epoch: 73, batch:10, train loss: 0.8115760684013367\n",
      "epoch: 73, batch:20, train loss: 0.4939745366573334\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 74, batch:0, train loss: 0.5504310131072998\n",
      "epoch: 74, batch:10, train loss: 0.8115761876106262\n",
      "epoch: 74, batch:20, train loss: 0.4939744174480438\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 75, batch:0, train loss: 0.5504310131072998\n",
      "epoch: 75, batch:10, train loss: 0.8115761876106262\n",
      "epoch: 75, batch:20, train loss: 0.4939744770526886\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 76, batch:0, train loss: 0.550430953502655\n",
      "epoch: 76, batch:10, train loss: 0.8115760684013367\n",
      "epoch: 76, batch:20, train loss: 0.4939744770526886\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 77, batch:0, train loss: 0.550430953502655\n",
      "epoch: 77, batch:10, train loss: 0.8115761876106262\n",
      "epoch: 77, batch:20, train loss: 0.49397435784339905\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 78, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 78, batch:10, train loss: 0.811576247215271\n",
      "epoch: 78, batch:20, train loss: 0.49397432804107666\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 79, batch:0, train loss: 0.550430953502655\n",
      "epoch: 79, batch:10, train loss: 0.811576247215271\n",
      "epoch: 79, batch:20, train loss: 0.49397435784339905\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 80, batch:0, train loss: 0.550430953502655\n",
      "epoch: 80, batch:10, train loss: 0.811576247215271\n",
      "epoch: 80, batch:20, train loss: 0.49397435784339905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 81, batch:0, train loss: 0.550430953502655\n",
      "epoch: 81, batch:10, train loss: 0.811576247215271\n",
      "epoch: 81, batch:20, train loss: 0.49397435784339905\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 82, batch:0, train loss: 0.550430953502655\n",
      "epoch: 82, batch:10, train loss: 0.811576247215271\n",
      "epoch: 82, batch:20, train loss: 0.49397435784339905\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 83, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 83, batch:10, train loss: 0.811576247215271\n",
      "epoch: 83, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 84, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 84, batch:10, train loss: 0.811576247215271\n",
      "epoch: 84, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 85, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 85, batch:10, train loss: 0.811576247215271\n",
      "epoch: 85, batch:20, train loss: 0.4939742088317871\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 86, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 86, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 86, batch:20, train loss: 0.4939742088317871\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 87, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 87, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 87, batch:20, train loss: 0.4939742088317871\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 88, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 88, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 88, batch:20, train loss: 0.4939742088317871\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 89, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 89, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 89, batch:20, train loss: 0.4939742088317871\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 90, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 90, batch:10, train loss: 0.8115764856338501\n",
      "epoch: 90, batch:20, train loss: 0.4939742088317871\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 91, batch:0, train loss: 0.5504308342933655\n",
      "epoch: 91, batch:10, train loss: 0.8115764856338501\n",
      "epoch: 91, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 92, batch:0, train loss: 0.5504308342933655\n",
      "epoch: 92, batch:10, train loss: 0.8115764856338501\n",
      "epoch: 92, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 93, batch:0, train loss: 0.5504308342933655\n",
      "epoch: 93, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 93, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 94, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 94, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 94, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 95, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 95, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 95, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 96, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 96, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 96, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 97, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 97, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 97, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 98, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 98, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 98, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 99, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 99, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 99, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 100, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 100, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 100, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 101, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 101, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 101, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 102, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 102, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 102, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 103, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 103, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 103, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 104, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 104, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 104, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 105, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 105, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 105, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 106, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 106, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 106, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 107, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 107, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 107, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 108, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 108, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 108, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 109, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 109, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 109, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 110, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 110, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 110, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 111, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 111, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 111, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 112, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 112, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 112, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 113, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 113, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 113, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 114, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 114, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 114, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 115, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 115, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 115, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 116, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 116, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 116, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 117, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 117, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 117, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 118, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 118, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 118, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 119, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 119, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 119, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 120, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 120, batch:10, train loss: 0.8115763664245605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 121, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 121, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 121, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 122, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 122, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 122, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 123, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 123, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 123, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 124, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 124, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 124, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 125, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 125, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 125, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 126, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 126, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 126, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 127, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 127, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 127, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 128, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 128, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 128, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 129, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 129, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 129, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 130, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 130, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 130, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 131, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 131, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 131, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 132, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 132, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 132, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 133, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 133, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 133, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 134, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 134, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 134, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 135, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 135, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 135, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 136, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 136, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 136, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 137, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 137, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 137, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 138, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 138, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 138, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 139, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 139, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 139, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 140, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 140, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 140, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 141, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 141, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 141, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 142, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 142, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 142, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 143, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 143, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 143, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 144, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 144, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 144, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 145, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 145, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 145, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 146, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 146, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 146, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 147, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 147, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 147, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 148, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 148, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 148, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 149, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 149, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 149, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 150, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 150, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 150, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 151, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 151, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 151, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 152, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 152, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 152, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 153, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 153, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 153, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 154, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 154, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 154, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 155, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 155, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 155, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 156, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 156, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 156, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 157, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 157, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 157, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 158, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 158, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 158, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 159, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 159, batch:10, train loss: 0.8115763664245605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 159, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 160, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 160, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 160, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 161, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 161, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 161, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 162, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 162, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 162, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 163, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 163, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 163, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 164, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 164, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 164, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 165, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 165, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 165, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 166, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 166, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 166, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 167, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 167, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 167, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 168, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 168, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 168, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 169, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 169, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 169, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 170, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 170, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 170, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 171, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 171, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 171, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 172, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 172, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 172, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 173, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 173, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 173, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 174, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 174, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 174, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 175, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 175, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 175, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 176, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 176, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 176, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 177, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 177, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 177, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 178, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 178, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 178, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 179, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 179, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 179, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 180, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 180, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 180, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 181, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 181, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 181, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 182, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 182, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 182, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 183, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 183, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 183, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 184, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 184, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 184, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 185, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 185, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 185, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 186, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 186, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 186, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 187, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 187, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 187, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 188, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 188, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 188, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 189, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 189, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 189, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 190, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 190, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 190, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 191, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 191, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 191, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 192, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 192, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 192, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 193, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 193, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 193, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 194, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 194, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 194, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 195, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 195, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 195, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 196, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 196, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 196, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 197, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 197, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 197, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 198, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 198, batch:10, train loss: 0.8115763664245605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 198, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 199, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 199, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 199, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 200, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 200, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 200, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 201, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 201, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 201, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 202, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 202, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 202, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 203, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 203, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 203, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 204, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 204, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 204, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 205, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 205, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 205, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 206, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 206, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 206, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 207, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 207, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 207, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 208, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 208, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 208, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 209, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 209, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 209, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 210, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 210, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 210, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 211, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 211, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 211, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 212, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 212, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 212, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 213, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 213, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 213, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 214, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 214, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 214, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 215, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 215, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 215, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 216, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 216, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 216, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 217, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 217, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 217, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 218, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 218, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 218, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 219, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 219, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 219, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 220, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 220, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 220, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 221, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 221, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 221, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 222, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 222, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 222, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 223, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 223, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 223, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 224, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 224, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 224, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 225, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 225, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 225, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 226, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 226, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 226, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 227, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 227, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 227, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 228, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 228, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 228, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 229, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 229, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 229, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 230, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 230, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 230, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 231, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 231, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 231, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 232, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 232, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 232, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 233, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 233, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 233, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 234, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 234, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 234, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 235, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 235, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 235, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 236, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 236, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 236, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 237, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 237, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 237, batch:20, train loss: 0.4939742684364319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 238, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 238, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 238, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 239, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 239, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 239, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 240, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 240, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 240, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 241, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 241, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 241, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 242, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 242, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 242, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 243, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 243, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 243, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 244, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 244, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 244, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 245, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 245, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 245, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 246, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 246, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 246, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 247, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 247, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 247, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 248, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 248, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 248, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 249, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 249, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 249, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 250, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 250, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 250, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 251, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 251, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 251, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 252, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 252, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 252, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 253, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 253, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 253, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 254, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 254, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 254, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 255, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 255, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 255, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 256, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 256, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 256, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 257, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 257, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 257, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 258, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 258, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 258, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 259, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 259, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 259, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 260, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 260, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 260, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 261, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 261, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 261, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 262, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 262, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 262, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 263, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 263, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 263, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 264, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 264, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 264, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 265, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 265, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 265, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 266, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 266, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 266, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 267, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 267, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 267, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 268, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 268, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 268, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 269, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 269, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 269, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 270, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 270, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 270, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 271, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 271, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 271, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 272, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 272, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 272, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 273, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 273, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 273, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 274, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 274, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 274, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 275, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 275, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 275, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 276, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 276, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 276, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 277, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 277, batch:10, train loss: 0.8115763664245605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 277, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 278, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 278, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 278, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 279, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 279, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 279, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 280, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 280, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 280, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 281, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 281, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 281, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 282, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 282, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 282, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 283, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 283, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 283, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 284, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 284, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 284, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 285, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 285, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 285, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 286, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 286, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 286, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 287, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 287, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 287, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 288, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 288, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 288, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 289, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 289, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 289, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 290, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 290, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 290, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 291, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 291, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 291, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 292, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 292, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 292, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 293, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 293, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 293, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 294, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 294, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 294, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 295, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 295, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 295, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 296, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 296, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 296, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 297, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 297, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 297, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 298, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 298, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 298, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 299, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 299, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 299, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 300, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 300, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 300, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 301, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 301, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 301, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 302, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 302, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 302, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 303, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 303, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 303, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 304, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 304, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 304, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 305, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 305, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 305, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 306, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 306, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 306, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 307, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 307, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 307, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 308, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 308, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 308, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 309, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 309, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 309, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 310, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 310, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 310, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 311, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 311, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 311, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 312, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 312, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 312, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 313, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 313, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 313, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 314, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 314, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 314, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 315, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 315, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 315, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 316, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 316, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 316, batch:20, train loss: 0.4939742684364319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 317, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 317, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 317, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 318, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 318, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 318, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 319, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 319, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 319, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 320, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 320, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 320, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 321, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 321, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 321, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 322, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 322, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 322, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 323, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 323, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 323, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 324, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 324, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 324, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 325, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 325, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 325, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 326, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 326, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 326, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 327, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 327, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 327, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 328, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 328, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 328, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 329, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 329, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 329, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 330, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 330, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 330, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 331, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 331, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 331, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 332, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 332, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 332, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 333, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 333, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 333, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 334, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 334, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 334, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 335, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 335, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 335, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 336, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 336, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 336, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 337, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 337, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 337, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 338, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 338, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 338, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 339, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 339, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 339, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 340, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 340, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 340, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 341, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 341, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 341, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 342, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 342, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 342, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 343, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 343, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 343, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 344, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 344, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 344, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 345, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 345, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 345, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 346, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 346, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 346, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 347, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 347, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 347, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 348, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 348, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 348, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 349, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 349, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 349, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 350, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 350, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 350, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 351, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 351, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 351, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 352, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 352, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 352, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 353, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 353, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 353, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 354, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 354, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 354, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 355, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 355, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 355, batch:20, train loss: 0.4939742684364319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 356, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 356, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 356, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 357, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 357, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 357, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 358, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 358, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 358, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 359, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 359, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 359, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 360, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 360, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 360, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 361, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 361, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 361, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 362, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 362, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 362, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 363, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 363, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 363, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 364, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 364, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 364, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 365, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 365, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 365, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 366, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 366, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 366, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 367, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 367, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 367, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 368, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 368, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 368, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 369, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 369, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 369, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 370, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 370, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 370, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 371, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 371, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 371, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 372, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 372, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 372, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 373, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 373, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 373, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 374, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 374, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 374, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 375, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 375, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 375, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 376, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 376, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 376, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 377, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 377, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 377, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 378, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 378, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 378, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 379, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 379, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 379, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 380, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 380, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 380, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 381, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 381, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 381, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 382, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 382, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 382, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 383, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 383, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 383, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 384, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 384, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 384, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 385, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 385, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 385, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 386, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 386, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 386, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 387, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 387, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 387, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 388, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 388, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 388, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 389, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 389, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 389, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 390, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 390, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 390, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 391, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 391, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 391, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 392, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 392, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 392, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 393, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 393, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 393, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 394, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 394, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 394, batch:20, train loss: 0.4939742684364319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 395, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 395, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 395, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 396, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 396, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 396, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 397, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 397, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 397, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 398, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 398, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 398, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 399, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 399, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 399, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 400, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 400, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 400, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 401, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 401, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 401, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 402, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 402, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 402, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 403, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 403, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 403, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 404, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 404, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 404, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 405, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 405, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 405, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 406, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 406, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 406, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 407, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 407, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 407, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 408, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 408, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 408, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 409, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 409, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 409, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 410, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 410, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 410, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 411, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 411, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 411, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 412, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 412, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 412, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 413, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 413, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 413, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 414, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 414, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 414, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 415, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 415, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 415, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 416, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 416, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 416, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 417, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 417, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 417, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 418, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 418, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 418, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 419, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 419, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 419, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 420, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 420, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 420, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 421, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 421, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 421, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 422, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 422, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 422, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 423, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 423, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 423, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 424, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 424, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 424, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 425, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 425, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 425, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 426, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 426, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 426, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 427, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 427, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 427, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 428, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 428, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 428, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 429, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 429, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 429, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 430, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 430, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 430, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 431, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 431, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 431, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 432, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 432, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 432, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 433, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 433, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 433, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 434, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 434, batch:10, train loss: 0.8115763664245605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 434, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 435, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 435, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 435, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 436, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 436, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 436, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 437, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 437, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 437, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 438, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 438, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 438, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 439, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 439, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 439, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 440, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 440, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 440, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 441, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 441, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 441, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 442, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 442, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 442, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 443, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 443, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 443, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 444, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 444, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 444, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 445, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 445, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 445, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 446, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 446, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 446, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 447, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 447, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 447, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 448, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 448, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 448, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 449, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 449, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 449, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 450, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 450, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 450, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 451, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 451, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 451, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 452, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 452, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 452, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 453, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 453, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 453, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 454, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 454, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 454, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 455, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 455, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 455, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 456, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 456, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 456, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 457, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 457, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 457, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 458, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 458, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 458, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 459, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 459, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 459, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 460, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 460, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 460, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 461, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 461, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 461, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 462, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 462, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 462, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 463, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 463, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 463, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 464, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 464, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 464, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 465, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 465, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 465, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 466, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 466, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 466, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 467, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 467, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 467, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 468, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 468, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 468, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 469, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 469, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 469, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 470, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 470, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 470, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 471, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 471, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 471, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 472, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 472, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 472, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 473, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 473, batch:10, train loss: 0.8115763664245605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 473, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 474, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 474, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 474, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 475, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 475, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 475, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 476, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 476, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 476, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 477, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 477, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 477, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 478, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 478, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 478, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 479, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 479, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 479, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 480, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 480, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 480, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 481, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 481, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 481, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 482, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 482, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 482, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 483, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 483, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 483, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 484, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 484, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 484, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 485, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 485, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 485, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 486, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 486, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 486, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 487, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 487, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 487, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 488, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 488, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 488, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 489, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 489, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 489, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 490, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 490, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 490, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 491, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 491, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 491, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 492, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 492, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 492, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 493, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 493, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 493, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 494, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 494, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 494, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 495, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 495, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 495, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 496, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 496, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 496, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 497, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 497, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 497, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 498, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 498, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 498, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 499, batch:0, train loss: 0.5504308938980103\n",
      "epoch: 499, batch:10, train loss: 0.8115763664245605\n",
      "epoch: 499, batch:20, train loss: 0.4939742684364319\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in range(num_epochs):\n",
    "\n",
    "    for i in range(batch_number):\n",
    "        \n",
    "        # 如果不是该epoch里面最后一组数据\n",
    "        if i != (batch_number - 1):\n",
    "#             print(y[i * batch_size: (i+1) * batch_size, :].shape)\n",
    "            batch_y = y[i * batch_size: (i+1) * batch_size, :]\n",
    "            batch_x = x[i * batch_size: (i+1) * batch_size, :]\n",
    "        else:\n",
    "#             print(y[i * batch_size: , :].shape)\n",
    "            batch_y = y[i * batch_size: , :]\n",
    "            batch_x = x[i * batch_size: , :]\n",
    "        \n",
    "        # 前向传播\n",
    "        out = net(batch_x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, batch_y)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch: {j}, batch:{i}, train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "    \n",
    "\n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result_train = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "        result_test = (np.argmax(net(test_x_orig).detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "        \n",
    "\n",
    "        print(\"train Acc：\", np.mean(result_train),'\\ntest Acc：', np.mean(result_test), '\\n')\n",
    "    \n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result_train))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q: 为何要将数据分批训练:\n",
    "#A: 1.提高了内存利用率以及大矩阵乘法的并行化效率。\n",
    "#   2.减少了跑完一次epoch(全数据集）所需要的迭代次数，加快了对于相同数据量的处理速度。\n",
    "#   3.随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。\n",
    "#   4.以Rprop的方式迭代，会由于各个Batch之间的采样差异性，各次梯度修正值相互抵消，无法修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务8: \n",
    "\n",
    "使用pytorch内置的工具,构建一个数据读取器(dataloader)来读取数据 \\\n",
    "**(代码写在下方)**\n",
    "\n",
    "参考资料:\n",
    "1. 两文读懂PyTorch中Dataset与DataLoader（一）打造自己的数据集\n",
    "https://zhuanlan.zhihu.com/p/105507334\n",
    "2. 两文读懂PyTorch中Dataset与DataLoader（二）理解DataLoader源码\n",
    "https://zhuanlan.zhihu.com/p/105578087\n",
    "\n",
    "tips: \n",
    "ndarray的数据长度用.shape来提取 \\\n",
    "\n",
    "\n",
    "首先需要根据参考资料1,自定义Dataset类型 \\\n",
    "再根据参考资料2,将自定义Dataset类型创建出的对象,传到torch.utils.data.DataLoader \\\n",
    "\n",
    "参考资料2里的sampler,不需要使用.该任务仅仅需要做DataLoader \\\n",
    "\n",
    "参考资料1里的数据是从文件夹里面读取图片,而我们提供的数据,是已经转化为ndarray, \\\n",
    "因此在做自定义dataset不需要再像参考资料那样从文件夹中读取图片和标签放到ndarray \\\n",
    "主要需要考虑如何从ndarray数组里面取出单个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 209)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "\n",
    "print(train_y.shape)\n",
    "\n",
    "# 自定义Dataset\n",
    "class Cat_dataset(data.Dataset):\n",
    "    def __init__(self, img_ndarray, label_ndarray):\n",
    "#         self.file_path = './data/faces/'\n",
    "#         f=open(\"final_train_tag_dict.txt\",\"r\")\n",
    "#         self.label_dict=eval(f.read())\n",
    "#         f.close()\n",
    "        self.img_ndarray = img_ndarray\n",
    "        self.label_ndarray = label_ndarray\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "#         label = list(self.label_dict.values())[index-1]\n",
    "#         img_id = list(self.label_dict.keys())[index-1]\n",
    "#         img_path = self.file_path+str(img_id)+\".jpg\"\n",
    "#         img = np.array(Image.open(img_path)) \n",
    "        label = self.label_ndarray[index-1, :]# 怎么从 self.label_array里面拿一个出来呢? (1, 209) -> 如何拿(1, 1)\n",
    "        img =self.img_ndarray[index-1, :]# 同上\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_ndarray.shape[0]\n",
    "\n",
    "cat_dataset = Cat_dataset(x, y)\n",
    "    \n",
    "# 构建DataLoader对象\n",
    "dataloader = torch.utils.data.DataLoader(cat_dataset,batch_size = 10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([9, 64, 64, 3])\n",
      "torch.Size([9, 2])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader迭代产生训练数据提供给模型\n",
    "for i in range(1):\n",
    "    for index,(img,label) in enumerate(dataloader):\n",
    "        print(img.shape)\n",
    "        print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch:0, train loss: -0.0\n",
      "epoch: 0, batch:10, train loss: 0.6940057277679443\n",
      "epoch: 0, batch:20, train loss: 0.6713345646858215\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 1, batch:0, train loss: 0.6027773022651672\n",
      "epoch: 1, batch:10, train loss: 0.6943357586860657\n",
      "epoch: 1, batch:20, train loss: 0.6516874432563782\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 2, batch:0, train loss: 0.6595511436462402\n",
      "epoch: 2, batch:10, train loss: 0.6959975361824036\n",
      "epoch: 2, batch:20, train loss: 0.6336060166358948\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 3, batch:0, train loss: 0.6461549401283264\n",
      "epoch: 3, batch:10, train loss: 0.698104202747345\n",
      "epoch: 3, batch:20, train loss: 0.6178710460662842\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 4, batch:0, train loss: 0.6346340179443359\n",
      "epoch: 4, batch:10, train loss: 0.7004951238632202\n",
      "epoch: 4, batch:20, train loss: 0.604137659072876\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 5, batch:0, train loss: 0.6246932148933411\n",
      "epoch: 5, batch:10, train loss: 0.7030490636825562\n",
      "epoch: 5, batch:20, train loss: 0.5921162366867065\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 6, batch:0, train loss: 0.6160872578620911\n",
      "epoch: 6, batch:10, train loss: 0.7056746482849121\n",
      "epoch: 6, batch:20, train loss: 0.5815644860267639\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 7, batch:0, train loss: 0.6086129546165466\n",
      "epoch: 7, batch:10, train loss: 0.7083050012588501\n",
      "epoch: 7, batch:20, train loss: 0.572277307510376\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 8, batch:0, train loss: 0.6021010279655457\n",
      "epoch: 8, batch:10, train loss: 0.7108911871910095\n",
      "epoch: 8, batch:20, train loss: 0.5640828609466553\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 9, batch:0, train loss: 0.5964102149009705\n",
      "epoch: 9, batch:10, train loss: 0.7133990526199341\n",
      "epoch: 9, batch:20, train loss: 0.5568353533744812\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 10, batch:0, train loss: 0.5914229154586792\n",
      "epoch: 10, batch:10, train loss: 0.715804934501648\n",
      "epoch: 10, batch:20, train loss: 0.5504105687141418\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 11, batch:0, train loss: 0.5870400071144104\n",
      "epoch: 11, batch:10, train loss: 0.7180936932563782\n",
      "epoch: 11, batch:20, train loss: 0.5447035431861877\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 12, batch:0, train loss: 0.5831782817840576\n",
      "epoch: 12, batch:10, train loss: 0.7202564477920532\n",
      "epoch: 12, batch:20, train loss: 0.5396242737770081\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 13, batch:0, train loss: 0.5797673463821411\n",
      "epoch: 13, batch:10, train loss: 0.722288966178894\n",
      "epoch: 13, batch:20, train loss: 0.5350953936576843\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 14, batch:0, train loss: 0.5767478942871094\n",
      "epoch: 14, batch:10, train loss: 0.7241905927658081\n",
      "epoch: 14, batch:20, train loss: 0.531050443649292\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 15, batch:0, train loss: 0.5740691423416138\n",
      "epoch: 15, batch:10, train loss: 0.7259629964828491\n",
      "epoch: 15, batch:20, train loss: 0.527432382106781\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 16, batch:0, train loss: 0.5716878771781921\n",
      "epoch: 16, batch:10, train loss: 0.7276099920272827\n",
      "epoch: 16, batch:20, train loss: 0.5241913199424744\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 17, batch:0, train loss: 0.5695673227310181\n",
      "epoch: 17, batch:10, train loss: 0.7291359901428223\n",
      "epoch: 17, batch:20, train loss: 0.5212843418121338\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 18, batch:0, train loss: 0.5676755309104919\n",
      "epoch: 18, batch:10, train loss: 0.7305470705032349\n",
      "epoch: 18, batch:20, train loss: 0.5186738967895508\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 19, batch:0, train loss: 0.5659851431846619\n",
      "epoch: 19, batch:10, train loss: 0.7318490743637085\n",
      "epoch: 19, batch:20, train loss: 0.5163270831108093\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 20, batch:0, train loss: 0.5644725561141968\n",
      "epoch: 20, batch:10, train loss: 0.7330485582351685\n",
      "epoch: 20, batch:20, train loss: 0.5142151713371277\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 21, batch:0, train loss: 0.5631171464920044\n",
      "epoch: 21, batch:10, train loss: 0.7341519594192505\n",
      "epoch: 21, batch:20, train loss: 0.5123128890991211\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 22, batch:0, train loss: 0.5619010925292969\n",
      "epoch: 22, batch:10, train loss: 0.7351657152175903\n",
      "epoch: 22, batch:20, train loss: 0.5105979442596436\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 23, batch:0, train loss: 0.5608088374137878\n",
      "epoch: 23, batch:10, train loss: 0.7360960841178894\n",
      "epoch: 23, batch:20, train loss: 0.509050726890564\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 24, batch:0, train loss: 0.5598267316818237\n",
      "epoch: 24, batch:10, train loss: 0.7369490265846252\n",
      "epoch: 24, batch:20, train loss: 0.5076539516448975\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 25, batch:0, train loss: 0.5589427351951599\n",
      "epoch: 25, batch:10, train loss: 0.7377304434776306\n",
      "epoch: 25, batch:20, train loss: 0.5063918828964233\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 26, batch:0, train loss: 0.5581463575363159\n",
      "epoch: 26, batch:10, train loss: 0.7384456396102905\n",
      "epoch: 26, batch:20, train loss: 0.5052512288093567\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 27, batch:0, train loss: 0.557428240776062\n",
      "epoch: 27, batch:10, train loss: 0.7390998601913452\n",
      "epoch: 27, batch:20, train loss: 0.5042194128036499\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 28, batch:0, train loss: 0.5567805171012878\n",
      "epoch: 28, batch:10, train loss: 0.7396979331970215\n",
      "epoch: 28, batch:20, train loss: 0.5032857060432434\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 29, batch:0, train loss: 0.5561955571174622\n",
      "epoch: 29, batch:10, train loss: 0.7402445673942566\n",
      "epoch: 29, batch:20, train loss: 0.5024406313896179\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 30, batch:0, train loss: 0.555666983127594\n",
      "epoch: 30, batch:10, train loss: 0.7407437562942505\n",
      "epoch: 30, batch:20, train loss: 0.5016750693321228\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 31, batch:0, train loss: 0.5551892518997192\n",
      "epoch: 31, batch:10, train loss: 0.7411993741989136\n",
      "epoch: 31, batch:20, train loss: 0.5009815692901611\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 32, batch:0, train loss: 0.5547570586204529\n",
      "epoch: 32, batch:10, train loss: 0.7416154146194458\n",
      "epoch: 32, batch:20, train loss: 0.5003529787063599\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 33, batch:0, train loss: 0.5543660521507263\n",
      "epoch: 33, batch:10, train loss: 0.7419945597648621\n",
      "epoch: 33, batch:20, train loss: 0.49978309869766235\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 34, batch:0, train loss: 0.55401211977005\n",
      "epoch: 34, batch:10, train loss: 0.7423406839370728\n",
      "epoch: 34, batch:20, train loss: 0.4992664158344269\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 35, batch:0, train loss: 0.5536913871765137\n",
      "epoch: 35, batch:10, train loss: 0.7426561713218689\n",
      "epoch: 35, batch:20, train loss: 0.49879762530326843\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 36, batch:0, train loss: 0.553400993347168\n",
      "epoch: 36, batch:10, train loss: 0.7429436445236206\n",
      "epoch: 36, batch:20, train loss: 0.4983724057674408\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 37, batch:0, train loss: 0.5531376600265503\n",
      "epoch: 37, batch:10, train loss: 0.743205726146698\n",
      "epoch: 37, batch:20, train loss: 0.49798643589019775\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 38, batch:0, train loss: 0.5528990030288696\n",
      "epoch: 38, batch:10, train loss: 0.7434444427490234\n",
      "epoch: 38, batch:20, train loss: 0.4976360499858856\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 39, batch:0, train loss: 0.5526825189590454\n",
      "epoch: 39, batch:10, train loss: 0.7436621189117432\n",
      "epoch: 39, batch:20, train loss: 0.4973180592060089\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 40, batch:0, train loss: 0.5524860620498657\n",
      "epoch: 40, batch:10, train loss: 0.7438602447509766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, batch:20, train loss: 0.4970293641090393\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 41, batch:0, train loss: 0.5523079037666321\n",
      "epoch: 41, batch:10, train loss: 0.7440406084060669\n",
      "epoch: 41, batch:20, train loss: 0.4967671036720276\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 42, batch:0, train loss: 0.5521462559700012\n",
      "epoch: 42, batch:10, train loss: 0.7442049980163574\n",
      "epoch: 42, batch:20, train loss: 0.49652889370918274\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 43, batch:0, train loss: 0.5519994497299194\n",
      "epoch: 43, batch:10, train loss: 0.7443546056747437\n",
      "epoch: 43, batch:20, train loss: 0.49631261825561523\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 44, batch:0, train loss: 0.551866352558136\n",
      "epoch: 44, batch:10, train loss: 0.7444907426834106\n",
      "epoch: 44, batch:20, train loss: 0.496116042137146\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 45, batch:0, train loss: 0.5517454147338867\n",
      "epoch: 45, batch:10, train loss: 0.7446147203445435\n",
      "epoch: 45, batch:20, train loss: 0.4959374964237213\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 46, batch:0, train loss: 0.5516354441642761\n",
      "epoch: 46, batch:10, train loss: 0.744727611541748\n",
      "epoch: 46, batch:20, train loss: 0.4957752227783203\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 47, batch:0, train loss: 0.5515357255935669\n",
      "epoch: 47, batch:10, train loss: 0.7448303699493408\n",
      "epoch: 47, batch:20, train loss: 0.49562788009643555\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 48, batch:0, train loss: 0.5514451265335083\n",
      "epoch: 48, batch:10, train loss: 0.7449238300323486\n",
      "epoch: 48, batch:20, train loss: 0.49549388885498047\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 49, batch:0, train loss: 0.5513628125190735\n",
      "epoch: 49, batch:10, train loss: 0.7450089454650879\n",
      "epoch: 49, batch:20, train loss: 0.49537214636802673\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 50, batch:0, train loss: 0.5512880086898804\n",
      "epoch: 50, batch:10, train loss: 0.7450861930847168\n",
      "epoch: 50, batch:20, train loss: 0.49526146054267883\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 51, batch:0, train loss: 0.5512200593948364\n",
      "epoch: 51, batch:10, train loss: 0.7451567649841309\n",
      "epoch: 51, batch:20, train loss: 0.495160847902298\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 52, batch:0, train loss: 0.5511583089828491\n",
      "epoch: 52, batch:10, train loss: 0.7452208399772644\n",
      "epoch: 52, batch:20, train loss: 0.4950694441795349\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 53, batch:0, train loss: 0.5511021018028259\n",
      "epoch: 53, batch:10, train loss: 0.7452791333198547\n",
      "epoch: 53, batch:20, train loss: 0.4949863851070404\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 54, batch:0, train loss: 0.551051139831543\n",
      "epoch: 54, batch:10, train loss: 0.7453321218490601\n",
      "epoch: 54, batch:20, train loss: 0.4949108362197876\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 55, batch:0, train loss: 0.5510047674179077\n",
      "epoch: 55, batch:10, train loss: 0.7453804016113281\n",
      "epoch: 55, batch:20, train loss: 0.49484217166900635\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 56, batch:0, train loss: 0.5509626865386963\n",
      "epoch: 56, batch:10, train loss: 0.7454243898391724\n",
      "epoch: 56, batch:20, train loss: 0.4947797358036041\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 57, batch:0, train loss: 0.5509243011474609\n",
      "epoch: 57, batch:10, train loss: 0.7454642653465271\n",
      "epoch: 57, batch:20, train loss: 0.4947229027748108\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 58, batch:0, train loss: 0.5508896112442017\n",
      "epoch: 58, batch:10, train loss: 0.7455006837844849\n",
      "epoch: 58, batch:20, train loss: 0.4946713447570801\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 59, batch:0, train loss: 0.5508579015731812\n",
      "epoch: 59, batch:10, train loss: 0.7455335855484009\n",
      "epoch: 59, batch:20, train loss: 0.49462446570396423\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 60, batch:0, train loss: 0.5508291721343994\n",
      "epoch: 60, batch:10, train loss: 0.745563805103302\n",
      "epoch: 60, batch:20, train loss: 0.4945816993713379\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 61, batch:0, train loss: 0.550803005695343\n",
      "epoch: 61, batch:10, train loss: 0.7455911636352539\n",
      "epoch: 61, batch:20, train loss: 0.4945429563522339\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 62, batch:0, train loss: 0.5507791638374329\n",
      "epoch: 62, batch:10, train loss: 0.74561607837677\n",
      "epoch: 62, batch:20, train loss: 0.4945075809955597\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 63, batch:0, train loss: 0.5507575869560242\n",
      "epoch: 63, batch:10, train loss: 0.7456387281417847\n",
      "epoch: 63, batch:20, train loss: 0.49447551369667053\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 64, batch:0, train loss: 0.5507378578186035\n",
      "epoch: 64, batch:10, train loss: 0.7456592321395874\n",
      "epoch: 64, batch:20, train loss: 0.494446337223053\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 65, batch:0, train loss: 0.5507199168205261\n",
      "epoch: 65, batch:10, train loss: 0.7456780672073364\n",
      "epoch: 65, batch:20, train loss: 0.49441978335380554\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 66, batch:0, train loss: 0.5507038235664368\n",
      "epoch: 66, batch:10, train loss: 0.7456951141357422\n",
      "epoch: 66, batch:20, train loss: 0.4943956732749939\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 67, batch:0, train loss: 0.5506889820098877\n",
      "epoch: 67, batch:10, train loss: 0.7457104921340942\n",
      "epoch: 67, batch:20, train loss: 0.49437373876571655\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 68, batch:0, train loss: 0.5506755709648132\n",
      "epoch: 68, batch:10, train loss: 0.7457247972488403\n",
      "epoch: 68, batch:20, train loss: 0.4943537712097168\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 69, batch:0, train loss: 0.5506632924079895\n",
      "epoch: 69, batch:10, train loss: 0.7457374334335327\n",
      "epoch: 69, batch:20, train loss: 0.49433571100234985\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 70, batch:0, train loss: 0.5506520867347717\n",
      "epoch: 70, batch:10, train loss: 0.7457491755485535\n",
      "epoch: 70, batch:20, train loss: 0.4943191707134247\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 71, batch:0, train loss: 0.5506421327590942\n",
      "epoch: 71, batch:10, train loss: 0.7457597255706787\n",
      "epoch: 71, batch:20, train loss: 0.4943042993545532\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 72, batch:0, train loss: 0.5506329536437988\n",
      "epoch: 72, batch:10, train loss: 0.7457693815231323\n",
      "epoch: 72, batch:20, train loss: 0.4942905008792877\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 73, batch:0, train loss: 0.5506245493888855\n",
      "epoch: 73, batch:10, train loss: 0.7457782030105591\n",
      "epoch: 73, batch:20, train loss: 0.49427807331085205\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 74, batch:0, train loss: 0.5506169199943542\n",
      "epoch: 74, batch:10, train loss: 0.745786190032959\n",
      "epoch: 74, batch:20, train loss: 0.4942668378353119\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 75, batch:0, train loss: 0.5506100058555603\n",
      "epoch: 75, batch:10, train loss: 0.745793342590332\n",
      "epoch: 75, batch:20, train loss: 0.4942566156387329\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 76, batch:0, train loss: 0.5506037473678589\n",
      "epoch: 76, batch:10, train loss: 0.7458000183105469\n",
      "epoch: 76, batch:20, train loss: 0.49424728751182556\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 77, batch:0, train loss: 0.5505980253219604\n",
      "epoch: 77, batch:10, train loss: 0.7458060383796692\n",
      "epoch: 77, batch:20, train loss: 0.4942387342453003\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 78, batch:0, train loss: 0.5505927801132202\n",
      "epoch: 78, batch:10, train loss: 0.7458114624023438\n",
      "epoch: 78, batch:20, train loss: 0.49423107504844666\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 79, batch:0, train loss: 0.550588071346283\n",
      "epoch: 79, batch:10, train loss: 0.7458164095878601\n",
      "epoch: 79, batch:20, train loss: 0.49422401189804077\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 80, batch:0, train loss: 0.5505838394165039\n",
      "epoch: 80, batch:10, train loss: 0.7458208799362183\n",
      "epoch: 80, batch:20, train loss: 0.4942176640033722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 81, batch:0, train loss: 0.550579845905304\n",
      "epoch: 81, batch:10, train loss: 0.7458250522613525\n",
      "epoch: 81, batch:20, train loss: 0.49421194195747375\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 82, batch:0, train loss: 0.550576388835907\n",
      "epoch: 82, batch:10, train loss: 0.7458288073539734\n",
      "epoch: 82, batch:20, train loss: 0.49420663714408875\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 83, batch:0, train loss: 0.5505731701850891\n",
      "epoch: 83, batch:10, train loss: 0.7458321452140808\n",
      "epoch: 83, batch:20, train loss: 0.4942018687725067\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 84, batch:0, train loss: 0.5505701899528503\n",
      "epoch: 84, batch:10, train loss: 0.7458351850509644\n",
      "epoch: 84, batch:20, train loss: 0.4941975176334381\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 85, batch:0, train loss: 0.5505675077438354\n",
      "epoch: 85, batch:10, train loss: 0.7458379864692688\n",
      "epoch: 85, batch:20, train loss: 0.494193434715271\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 86, batch:0, train loss: 0.5505651235580444\n",
      "epoch: 86, batch:10, train loss: 0.7458405494689941\n",
      "epoch: 86, batch:20, train loss: 0.49418988823890686\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 87, batch:0, train loss: 0.550562858581543\n",
      "epoch: 87, batch:10, train loss: 0.7458429336547852\n",
      "epoch: 87, batch:20, train loss: 0.4941866099834442\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 88, batch:0, train loss: 0.5505608916282654\n",
      "epoch: 88, batch:10, train loss: 0.7458450198173523\n",
      "epoch: 88, batch:20, train loss: 0.49418365955352783\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 89, batch:0, train loss: 0.5505590438842773\n",
      "epoch: 89, batch:10, train loss: 0.7458469867706299\n",
      "epoch: 89, batch:20, train loss: 0.49418094754219055\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 90, batch:0, train loss: 0.5505573749542236\n",
      "epoch: 90, batch:10, train loss: 0.7458486557006836\n",
      "epoch: 90, batch:20, train loss: 0.49417844414711\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 91, batch:0, train loss: 0.5505558848381042\n",
      "epoch: 91, batch:10, train loss: 0.7458503246307373\n",
      "epoch: 91, batch:20, train loss: 0.4941762387752533\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 92, batch:0, train loss: 0.5505544543266296\n",
      "epoch: 92, batch:10, train loss: 0.7458516955375671\n",
      "epoch: 92, batch:20, train loss: 0.49417421221733093\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 93, batch:0, train loss: 0.5505533218383789\n",
      "epoch: 93, batch:10, train loss: 0.745853066444397\n",
      "epoch: 93, batch:20, train loss: 0.4941723048686981\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 94, batch:0, train loss: 0.5505521893501282\n",
      "epoch: 94, batch:10, train loss: 0.7458541989326477\n",
      "epoch: 94, batch:20, train loss: 0.4941706657409668\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 95, batch:0, train loss: 0.5505511164665222\n",
      "epoch: 95, batch:10, train loss: 0.7458553314208984\n",
      "epoch: 95, batch:20, train loss: 0.49416911602020264\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 96, batch:0, train loss: 0.5505501627922058\n",
      "epoch: 96, batch:10, train loss: 0.7458562850952148\n",
      "epoch: 96, batch:20, train loss: 0.4941677451133728\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 97, batch:0, train loss: 0.5505493879318237\n",
      "epoch: 97, batch:10, train loss: 0.7458571195602417\n",
      "epoch: 97, batch:20, train loss: 0.4941664934158325\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 98, batch:0, train loss: 0.5505485534667969\n",
      "epoch: 98, batch:10, train loss: 0.7458580136299133\n",
      "epoch: 98, batch:20, train loss: 0.4941653609275818\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 99, batch:0, train loss: 0.5505478382110596\n",
      "epoch: 99, batch:10, train loss: 0.7458587884902954\n",
      "epoch: 99, batch:20, train loss: 0.49416425824165344\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 100, batch:0, train loss: 0.550547182559967\n",
      "epoch: 100, batch:10, train loss: 0.7458593845367432\n",
      "epoch: 100, batch:20, train loss: 0.4941633641719818\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 101, batch:0, train loss: 0.5505465269088745\n",
      "epoch: 101, batch:10, train loss: 0.7458600997924805\n",
      "epoch: 101, batch:20, train loss: 0.4941624402999878\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 102, batch:0, train loss: 0.5505460500717163\n",
      "epoch: 102, batch:10, train loss: 0.7458605766296387\n",
      "epoch: 102, batch:20, train loss: 0.49416160583496094\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 103, batch:0, train loss: 0.5505455732345581\n",
      "epoch: 103, batch:10, train loss: 0.7458610534667969\n",
      "epoch: 103, batch:20, train loss: 0.494160920381546\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 104, batch:0, train loss: 0.5505452156066895\n",
      "epoch: 104, batch:10, train loss: 0.7458616495132446\n",
      "epoch: 104, batch:20, train loss: 0.4941602349281311\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 105, batch:0, train loss: 0.550544798374176\n",
      "epoch: 105, batch:10, train loss: 0.7458620071411133\n",
      "epoch: 105, batch:20, train loss: 0.4941595792770386\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 106, batch:0, train loss: 0.5505443811416626\n",
      "epoch: 106, batch:10, train loss: 0.7458623051643372\n",
      "epoch: 106, batch:20, train loss: 0.494159072637558\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 107, batch:0, train loss: 0.550544023513794\n",
      "epoch: 107, batch:10, train loss: 0.7458626627922058\n",
      "epoch: 107, batch:20, train loss: 0.49415862560272217\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 108, batch:0, train loss: 0.5505437850952148\n",
      "epoch: 108, batch:10, train loss: 0.7458630800247192\n",
      "epoch: 108, batch:20, train loss: 0.49415814876556396\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 109, batch:0, train loss: 0.5505434274673462\n",
      "epoch: 109, batch:10, train loss: 0.7458633184432983\n",
      "epoch: 109, batch:20, train loss: 0.4941577911376953\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 110, batch:0, train loss: 0.5505431890487671\n",
      "epoch: 110, batch:10, train loss: 0.7458636164665222\n",
      "epoch: 110, batch:20, train loss: 0.49415743350982666\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 111, batch:0, train loss: 0.5505430102348328\n",
      "epoch: 111, batch:10, train loss: 0.7458637952804565\n",
      "epoch: 111, batch:20, train loss: 0.49415698647499084\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 112, batch:0, train loss: 0.5505427122116089\n",
      "epoch: 112, batch:10, train loss: 0.7458640336990356\n",
      "epoch: 112, batch:20, train loss: 0.49415677785873413\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 113, batch:0, train loss: 0.5505425333976746\n",
      "epoch: 113, batch:10, train loss: 0.7458642721176147\n",
      "epoch: 113, batch:20, train loss: 0.4941564202308655\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 114, batch:0, train loss: 0.5505423545837402\n",
      "epoch: 114, batch:10, train loss: 0.7458645105361938\n",
      "epoch: 114, batch:20, train loss: 0.49415621161460876\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 115, batch:0, train loss: 0.5505422949790955\n",
      "epoch: 115, batch:10, train loss: 0.7458646297454834\n",
      "epoch: 115, batch:20, train loss: 0.49415600299835205\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 116, batch:0, train loss: 0.5505421757698059\n",
      "epoch: 116, batch:10, train loss: 0.745864748954773\n",
      "epoch: 116, batch:20, train loss: 0.49415576457977295\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 117, batch:0, train loss: 0.5505419969558716\n",
      "epoch: 117, batch:10, train loss: 0.7458648681640625\n",
      "epoch: 117, batch:20, train loss: 0.4941556751728058\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 118, batch:0, train loss: 0.5505419373512268\n",
      "epoch: 118, batch:10, train loss: 0.745864987373352\n",
      "epoch: 118, batch:20, train loss: 0.49415552616119385\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 119, batch:0, train loss: 0.550541877746582\n",
      "epoch: 119, batch:10, train loss: 0.7458651065826416\n",
      "epoch: 119, batch:20, train loss: 0.49415525794029236\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 120, batch:0, train loss: 0.5505416989326477\n",
      "epoch: 120, batch:10, train loss: 0.7458652257919312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120, batch:20, train loss: 0.49415507912635803\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 121, batch:0, train loss: 0.5505415201187134\n",
      "epoch: 121, batch:10, train loss: 0.7458653450012207\n",
      "epoch: 121, batch:20, train loss: 0.4941549301147461\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 122, batch:0, train loss: 0.5505415201187134\n",
      "epoch: 122, batch:10, train loss: 0.7458653450012207\n",
      "epoch: 122, batch:20, train loss: 0.4941548705101013\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 123, batch:0, train loss: 0.5505414009094238\n",
      "epoch: 123, batch:10, train loss: 0.745865523815155\n",
      "epoch: 123, batch:20, train loss: 0.49415478110313416\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 124, batch:0, train loss: 0.5505414009094238\n",
      "epoch: 124, batch:10, train loss: 0.7458655834197998\n",
      "epoch: 124, batch:20, train loss: 0.49415460228919983\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 125, batch:0, train loss: 0.5505412817001343\n",
      "epoch: 125, batch:10, train loss: 0.7458655834197998\n",
      "epoch: 125, batch:20, train loss: 0.49415451288223267\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 126, batch:0, train loss: 0.5505412817001343\n",
      "epoch: 126, batch:10, train loss: 0.7458657026290894\n",
      "epoch: 126, batch:20, train loss: 0.4941544532775879\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 127, batch:0, train loss: 0.5505411028862\n",
      "epoch: 127, batch:10, train loss: 0.7458657026290894\n",
      "epoch: 127, batch:20, train loss: 0.49415433406829834\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 128, batch:0, train loss: 0.5505412220954895\n",
      "epoch: 128, batch:10, train loss: 0.7458658218383789\n",
      "epoch: 128, batch:20, train loss: 0.49415433406829834\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 129, batch:0, train loss: 0.5505411028862\n",
      "epoch: 129, batch:10, train loss: 0.7458658218383789\n",
      "epoch: 129, batch:20, train loss: 0.4941541850566864\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 130, batch:0, train loss: 0.5505410432815552\n",
      "epoch: 130, batch:10, train loss: 0.7458658218383789\n",
      "epoch: 130, batch:20, train loss: 0.4941541850566864\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 131, batch:0, train loss: 0.5505409836769104\n",
      "epoch: 131, batch:10, train loss: 0.7458660006523132\n",
      "epoch: 131, batch:20, train loss: 0.4941541254520416\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 132, batch:0, train loss: 0.5505409836769104\n",
      "epoch: 132, batch:10, train loss: 0.7458659410476685\n",
      "epoch: 132, batch:20, train loss: 0.49415403604507446\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 133, batch:0, train loss: 0.5505409240722656\n",
      "epoch: 133, batch:10, train loss: 0.745866060256958\n",
      "epoch: 133, batch:20, train loss: 0.49415403604507446\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 134, batch:0, train loss: 0.5505409240722656\n",
      "epoch: 134, batch:10, train loss: 0.7458659410476685\n",
      "epoch: 134, batch:20, train loss: 0.4941539168357849\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 135, batch:0, train loss: 0.5505408644676208\n",
      "epoch: 135, batch:10, train loss: 0.745866060256958\n",
      "epoch: 135, batch:20, train loss: 0.49415385723114014\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 136, batch:0, train loss: 0.5505408048629761\n",
      "epoch: 136, batch:10, train loss: 0.7458661198616028\n",
      "epoch: 136, batch:20, train loss: 0.4941539168357849\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 137, batch:0, train loss: 0.5505408644676208\n",
      "epoch: 137, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 137, batch:20, train loss: 0.494153767824173\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 138, batch:0, train loss: 0.5505408048629761\n",
      "epoch: 138, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 138, batch:20, train loss: 0.494153767824173\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 139, batch:0, train loss: 0.5505408048629761\n",
      "epoch: 139, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 139, batch:20, train loss: 0.494153767824173\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 140, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 140, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 140, batch:20, train loss: 0.49415382742881775\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 141, batch:0, train loss: 0.5505408048629761\n",
      "epoch: 141, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 141, batch:20, train loss: 0.49415382742881775\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 142, batch:0, train loss: 0.5505408048629761\n",
      "epoch: 142, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 142, batch:20, train loss: 0.49415382742881775\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 143, batch:0, train loss: 0.5505408048629761\n",
      "epoch: 143, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 143, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 144, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 144, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 144, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 145, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 145, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 145, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 146, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 146, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 146, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 147, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 147, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 147, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 148, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 148, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 148, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 149, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 149, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 149, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 150, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 150, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 150, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 151, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 151, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 151, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 152, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 152, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 152, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 153, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 153, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 153, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 154, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 154, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 154, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 155, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 155, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 155, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 156, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 156, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 156, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 157, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 157, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 157, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 158, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 158, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 158, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 159, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 159, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 159, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 160, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 160, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 160, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 161, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 161, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 161, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 162, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 162, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 162, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 163, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 163, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 163, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 164, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 164, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 164, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 165, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 165, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 165, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 166, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 166, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 166, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 167, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 167, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 167, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 168, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 168, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 168, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 169, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 169, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 169, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 170, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 170, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 170, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 171, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 171, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 171, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 172, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 172, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 172, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 173, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 173, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 173, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 174, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 174, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 174, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 175, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 175, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 175, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 176, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 176, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 176, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 177, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 177, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 177, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 178, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 178, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 178, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 179, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 179, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 179, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 180, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 180, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 180, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 181, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 181, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 181, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 182, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 182, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 182, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 183, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 183, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 183, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 184, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 184, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 184, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 185, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 185, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 185, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 186, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 186, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 186, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 187, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 187, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 187, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 188, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 188, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 188, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 189, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 189, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 189, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 190, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 190, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 190, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 191, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 191, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 191, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 192, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 192, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 192, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 193, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 193, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 193, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 194, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 194, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 194, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 195, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 195, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 195, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 196, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 196, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 196, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 197, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 197, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 197, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 198, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 198, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 198, batch:20, train loss: 0.4941537082195282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 199, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 199, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 199, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 200, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 200, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 200, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 201, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 201, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 201, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 202, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 202, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 202, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 203, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 203, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 203, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 204, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 204, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 204, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 205, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 205, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 205, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 206, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 206, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 206, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 207, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 207, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 207, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 208, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 208, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 208, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 209, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 209, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 209, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 210, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 210, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 210, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 211, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 211, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 211, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 212, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 212, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 212, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 213, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 213, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 213, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 214, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 214, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 214, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 215, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 215, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 215, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 216, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 216, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 216, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 217, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 217, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 217, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 218, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 218, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 218, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 219, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 219, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 219, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 220, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 220, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 220, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 221, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 221, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 221, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 222, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 222, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 222, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 223, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 223, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 223, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 224, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 224, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 224, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 225, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 225, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 225, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 226, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 226, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 226, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 227, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 227, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 227, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 228, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 228, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 228, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 229, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 229, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 229, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 230, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 230, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 230, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 231, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 231, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 231, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 232, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 232, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 232, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 233, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 233, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 233, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 234, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 234, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 234, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 235, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 235, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 235, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 236, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 236, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 236, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 237, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 237, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 237, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 238, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 238, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 238, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 239, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 239, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 239, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 240, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 240, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 240, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 241, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 241, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 241, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 242, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 242, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 242, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 243, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 243, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 243, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 244, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 244, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 244, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 245, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 245, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 245, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 246, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 246, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 246, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 247, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 247, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 247, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 248, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 248, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 248, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 249, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 249, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 249, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 250, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 250, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 250, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 251, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 251, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 251, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 252, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 252, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 252, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 253, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 253, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 253, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 254, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 254, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 254, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 255, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 255, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 255, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 256, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 256, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 256, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 257, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 257, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 257, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 258, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 258, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 258, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 259, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 259, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 259, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 260, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 260, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 260, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 261, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 261, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 261, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 262, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 262, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 262, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 263, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 263, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 263, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 264, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 264, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 264, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 265, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 265, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 265, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 266, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 266, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 266, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 267, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 267, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 267, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 268, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 268, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 268, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 269, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 269, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 269, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 270, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 270, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 270, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 271, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 271, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 271, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 272, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 272, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 272, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 273, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 273, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 273, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 274, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 274, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 274, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 275, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 275, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 275, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 276, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 276, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 276, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 277, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 277, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 277, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 278, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 278, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 278, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 279, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 279, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 279, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 280, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 280, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 280, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 281, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 281, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 281, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 282, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 282, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 282, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 283, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 283, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 283, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 284, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 284, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 284, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 285, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 285, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 285, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 286, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 286, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 286, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 287, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 287, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 287, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 288, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 288, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 288, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 289, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 289, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 289, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 290, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 290, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 290, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 291, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 291, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 291, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 292, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 292, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 292, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 293, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 293, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 293, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 294, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 294, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 294, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 295, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 295, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 295, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 296, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 296, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 296, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 297, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 297, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 297, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 298, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 298, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 298, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 299, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 299, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 299, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 300, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 300, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 300, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 301, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 301, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 301, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 302, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 302, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 302, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 303, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 303, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 303, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 304, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 304, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 304, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 305, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 305, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 305, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 306, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 306, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 306, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 307, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 307, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 307, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 308, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 308, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 308, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 309, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 309, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 309, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 310, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 310, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 310, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 311, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 311, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 311, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 312, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 312, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 312, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 313, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 313, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 313, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 314, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 314, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 314, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 315, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 315, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 315, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 316, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 316, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 316, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 317, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 317, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 317, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 318, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 318, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 318, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 319, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 319, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 319, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 320, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 320, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 320, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 321, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 321, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 321, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 322, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 322, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 322, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 323, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 323, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 323, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 324, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 324, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 324, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 325, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 325, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 325, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 326, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 326, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 326, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 327, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 327, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 327, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 328, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 328, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 328, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 329, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 329, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 329, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 330, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 330, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 330, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 331, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 331, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 331, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 332, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 332, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 332, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 333, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 333, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 333, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 334, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 334, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 334, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 335, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 335, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 335, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 336, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 336, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 336, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 337, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 337, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 337, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 338, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 338, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 338, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 339, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 339, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 339, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 340, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 340, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 340, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 341, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 341, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 341, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 342, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 342, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 342, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 343, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 343, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 343, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 344, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 344, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 344, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 345, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 345, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 345, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 346, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 346, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 346, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 347, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 347, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 347, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 348, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 348, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 348, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 349, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 349, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 349, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 350, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 350, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 350, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 351, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 351, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 351, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 352, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 352, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 352, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 353, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 353, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 353, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 354, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 354, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 354, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 355, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 355, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 355, batch:20, train loss: 0.4941537082195282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 356, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 356, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 356, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 357, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 357, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 357, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 358, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 358, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 358, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 359, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 359, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 359, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 360, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 360, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 360, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 361, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 361, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 361, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 362, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 362, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 362, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 363, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 363, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 363, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 364, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 364, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 364, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 365, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 365, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 365, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 366, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 366, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 366, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 367, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 367, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 367, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 368, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 368, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 368, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 369, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 369, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 369, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 370, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 370, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 370, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 371, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 371, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 371, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 372, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 372, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 372, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 373, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 373, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 373, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 374, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 374, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 374, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 375, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 375, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 375, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 376, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 376, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 376, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 377, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 377, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 377, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 378, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 378, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 378, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 379, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 379, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 379, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 380, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 380, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 380, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 381, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 381, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 381, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 382, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 382, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 382, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 383, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 383, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 383, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 384, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 384, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 384, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 385, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 385, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 385, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 386, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 386, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 386, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 387, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 387, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 387, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 388, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 388, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 388, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 389, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 389, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 389, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 390, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 390, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 390, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 391, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 391, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 391, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 392, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 392, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 392, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 393, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 393, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 393, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 394, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 394, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 394, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 395, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 395, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 395, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 396, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 396, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 396, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 397, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 397, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 397, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 398, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 398, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 398, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 399, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 399, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 399, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 400, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 400, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 400, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 401, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 401, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 401, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 402, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 402, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 402, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 403, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 403, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 403, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 404, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 404, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 404, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 405, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 405, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 405, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 406, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 406, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 406, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 407, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 407, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 407, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 408, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 408, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 408, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 409, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 409, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 409, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 410, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 410, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 410, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 411, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 411, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 411, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 412, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 412, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 412, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 413, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 413, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 413, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 414, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 414, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 414, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 415, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 415, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 415, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 416, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 416, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 416, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 417, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 417, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 417, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 418, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 418, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 418, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 419, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 419, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 419, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 420, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 420, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 420, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 421, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 421, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 421, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 422, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 422, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 422, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 423, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 423, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 423, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 424, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 424, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 424, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 425, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 425, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 425, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 426, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 426, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 426, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 427, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 427, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 427, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 428, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 428, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 428, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 429, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 429, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 429, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 430, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 430, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 430, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 431, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 431, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 431, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 432, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 432, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 432, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 433, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 433, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 433, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 434, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 434, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 434, batch:20, train loss: 0.4941537082195282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 435, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 435, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 435, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 436, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 436, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 436, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 437, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 437, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 437, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 438, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 438, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 438, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 439, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 439, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 439, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 440, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 440, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 440, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 441, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 441, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 441, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 442, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 442, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 442, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 443, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 443, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 443, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 444, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 444, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 444, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 445, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 445, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 445, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 446, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 446, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 446, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 447, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 447, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 447, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 448, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 448, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 448, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 449, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 449, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 449, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 450, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 450, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 450, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 451, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 451, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 451, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 452, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 452, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 452, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 453, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 453, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 453, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 454, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 454, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 454, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 455, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 455, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 455, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 456, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 456, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 456, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 457, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 457, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 457, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 458, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 458, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 458, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 459, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 459, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 459, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 460, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 460, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 460, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 461, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 461, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 461, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 462, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 462, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 462, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 463, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 463, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 463, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 464, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 464, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 464, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 465, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 465, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 465, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 466, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 466, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 466, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 467, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 467, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 467, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 468, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 468, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 468, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 469, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 469, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 469, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 470, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 470, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 470, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 471, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 471, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 471, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 472, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 472, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 472, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 473, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 473, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 473, batch:20, train loss: 0.4941537082195282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 474, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 474, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 474, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 475, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 475, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 475, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 476, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 476, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 476, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 477, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 477, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 477, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 478, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 478, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 478, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 479, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 479, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 479, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 480, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 480, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 480, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 481, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 481, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 481, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 482, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 482, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 482, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 483, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 483, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 483, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 484, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 484, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 484, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 485, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 485, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 485, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 486, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 486, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 486, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 487, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 487, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 487, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 488, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 488, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 488, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 489, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 489, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 489, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 490, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 490, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 490, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 491, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 491, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 491, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 492, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 492, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 492, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 493, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 493, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 493, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 494, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 494, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 494, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 495, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 495, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 495, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 496, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 496, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 496, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 497, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 497, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 497, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 498, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 498, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 498, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 499, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 499, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 499, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "for j in range(num_epochs):\n",
    "    for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        \n",
    "        # 前向传播\n",
    "        out = net(batch_x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, batch_y)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch: {j}, batch:{i}, train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "    \n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "\n",
    "        result_train = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "        result_test = (np.argmax(net(test_x_orig).detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "        \n",
    "\n",
    "        print(\"train Acc：\", np.mean(result_train),'\\ntest Acc：', np.mean(result_test), '\\n')\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result_train))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务9: \n",
    "使用任务8中的DataLoader替换任务7中自定义的batch数据提取器 \n",
    "\n",
    "要求: \\\n",
    "batch_size = 10, shuffle = True (打乱数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "from torch.utils import data\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = Adam(net.parameters(),lr=lr)\n",
    "# 定义epoch数量\n",
    "num_epochs = 200\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2,已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "\n",
    "# 数据\n",
    "y = torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "# batch_size设置\n",
    "batch_size = 10\n",
    "\n",
    "class Cat_dataset(data.Dataset):\n",
    "    def __init__(self, img_ndarray, label_ndarray):\n",
    "#         self.file_path = './data/faces/'\n",
    "#         f=open(\"final_train_tag_dict.txt\",\"r\")\n",
    "#         self.label_dict=eval(f.read())\n",
    "#         f.close()\n",
    "        self.img_ndarray = img_ndarray\n",
    "        self.label_ndarray = label_ndarray\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "#         label = list(self.label_dict.values())[index-1]\n",
    "#         img_id = list(self.label_dict.keys())[index-1]\n",
    "#         img_path = self.file_path+str(img_id)+\".jpg\"\n",
    "#         img = np.array(Image.open(img_path)) \n",
    "        label = self.label_ndarray[index-1, :]# 怎么从 self.label_array里面拿一个出来呢? (1, 209) -> 如何拿(1, 1)\n",
    "        img =self.img_ndarray[index-1, :]# 同上\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_ndarray.shape[0]\n",
    "\n",
    "cat_dataset = Cat_dataset(x, y)\n",
    "    \n",
    "# 构建DataLoader对象\n",
    "dataloader = torch.utils.data.DataLoader(cat_dataset,batch_size=10,shuffle=True)\n",
    "\n",
    "\n",
    "# 分开每batch数据\n",
    "#batch_number = int(y.shape[0]/10)\n",
    "\n",
    "# 假如不能整除,则last_batch_data_number表示: 该epoch中,最后一个batch的数据的数量\n",
    "#last_batch_data_number = y.shape[0] % 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([9, 64, 64, 3])\n",
      "torch.Size([9, 2])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for index,(img,label) in enumerate(dataloader):\n",
    "        print(img.shape)\n",
    "        print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 0, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 0, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 1, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 1, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 1, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 2, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 2, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 2, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 3, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 3, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 3, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 4, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 4, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 4, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 5, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 5, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 5, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 6, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 6, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 6, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 7, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 7, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 7, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 8, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 8, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 8, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 9, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 9, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 9, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 10, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 10, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 10, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 11, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 11, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 11, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 12, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 12, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 12, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 13, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 13, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 13, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 14, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 14, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 14, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 15, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 15, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 15, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 16, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 16, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 16, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 17, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 17, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 17, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 18, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 18, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 18, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 19, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 19, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 19, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 20, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 20, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 20, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 21, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 21, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 21, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 22, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 22, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 22, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 23, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 23, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 23, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 24, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 24, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 24, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 25, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 25, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 25, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 26, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 26, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 26, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 27, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 27, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 27, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 28, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 28, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 28, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 29, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 29, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 29, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 30, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 30, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 30, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 31, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 31, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 31, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 32, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 32, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 32, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 33, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 33, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 33, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 34, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 34, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 34, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 35, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 35, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 35, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 36, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 36, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 36, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 37, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 37, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 37, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 38, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 38, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 38, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 39, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 39, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 39, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 40, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 40, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 41, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 41, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 41, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 42, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 42, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 42, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 43, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 43, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 43, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 44, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 44, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 44, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 45, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 45, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 45, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 46, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 46, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 46, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 47, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 47, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 47, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 48, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 48, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 48, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 49, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 49, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 49, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 50, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 50, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 50, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 51, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 51, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 51, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 52, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 52, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 52, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 53, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 53, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 53, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 54, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 54, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 54, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 55, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 55, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 55, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 56, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 56, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 56, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 57, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 57, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 57, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 58, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 58, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 58, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 59, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 59, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 59, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 60, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 60, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 60, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 61, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 61, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 61, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 62, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 62, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 62, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 63, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 63, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 63, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 64, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 64, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 64, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 65, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 65, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 65, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 66, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 66, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 66, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 67, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 67, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 67, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 68, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 68, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 68, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 69, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 69, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 69, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 70, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 70, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 70, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 71, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 71, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 71, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 72, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 72, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 72, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 73, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 73, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 73, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 74, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 74, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 74, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 75, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 75, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 75, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 76, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 76, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 76, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 77, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 77, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 77, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 78, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 78, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 78, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 79, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 79, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 79, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 80, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 80, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 81, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 81, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 81, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 82, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 82, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 82, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 83, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 83, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 83, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 84, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 84, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 84, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 85, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 85, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 85, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 86, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 86, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 86, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 87, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 87, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 87, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 88, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 88, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 88, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 89, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 89, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 89, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 90, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 90, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 90, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 91, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 91, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 91, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 92, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 92, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 92, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 93, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 93, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 93, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 94, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 94, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 94, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 95, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 95, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 95, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 96, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 96, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 96, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 97, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 97, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 97, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 98, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 98, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 98, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 99, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 99, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 99, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 100, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 100, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 100, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 101, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 101, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 101, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 102, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 102, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 102, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 103, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 103, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 103, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 104, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 104, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 104, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 105, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 105, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 105, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 106, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 106, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 106, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 107, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 107, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 107, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 108, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 108, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 108, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 109, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 109, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 109, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 110, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 110, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 110, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 111, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 111, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 111, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 112, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 112, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 112, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 113, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 113, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 113, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 114, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 114, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 114, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 115, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 115, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 115, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 116, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 116, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 116, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 117, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 117, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 117, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 118, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 118, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 118, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 119, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 119, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 119, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 120, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 120, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 121, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 121, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 121, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 122, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 122, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 122, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 123, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 123, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 123, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 124, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 124, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 124, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 125, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 125, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 125, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 126, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 126, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 126, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 127, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 127, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 127, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 128, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 128, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 128, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 129, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 129, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 129, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 130, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 130, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 130, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 131, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 131, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 131, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 132, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 132, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 132, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 133, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 133, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 133, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 134, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 134, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 134, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 135, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 135, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 135, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 136, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 136, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 136, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 137, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 137, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 137, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 138, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 138, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 138, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 139, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 139, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 139, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 140, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 140, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 140, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 141, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 141, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 141, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 142, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 142, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 142, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 143, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 143, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 143, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 144, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 144, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 144, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 145, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 145, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 145, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 146, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 146, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 146, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 147, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 147, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 147, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 148, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 148, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 148, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 149, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 149, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 149, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 150, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 150, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 150, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 151, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 151, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 151, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 152, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 152, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 152, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 153, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 153, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 153, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 154, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 154, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 154, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 155, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 155, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 155, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 156, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 156, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 156, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 157, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 157, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 157, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 158, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 158, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 158, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 159, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 159, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 159, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 160, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 160, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 160, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 161, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 161, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 161, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 162, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 162, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 162, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 163, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 163, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 163, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 164, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 164, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 164, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 165, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 165, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 165, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 166, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 166, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 166, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 167, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 167, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 167, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 168, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 168, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 168, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 169, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 169, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 169, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 170, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 170, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 170, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 171, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 171, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 171, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 172, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 172, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 172, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 173, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 173, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 173, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 174, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 174, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 174, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 175, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 175, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 175, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 176, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 176, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 176, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 177, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 177, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 177, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 178, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 178, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 178, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 179, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 179, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 179, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 180, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 180, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 180, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 181, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 181, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 181, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 182, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 182, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 182, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 183, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 183, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 183, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 184, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 184, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 184, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 185, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 185, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 185, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 186, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 186, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 186, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 187, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 187, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 187, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 188, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 188, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 188, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 189, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 189, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 189, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 190, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 190, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 190, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 191, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 191, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 191, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 192, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 192, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 192, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 193, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 193, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 193, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 194, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 194, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 194, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 195, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 195, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 195, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 196, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 196, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 196, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 197, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 197, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 197, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 198, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 198, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 198, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 199, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 199, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 199, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 200, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 200, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 200, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 201, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 201, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 201, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 202, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 202, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 202, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 203, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 203, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 203, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 204, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 204, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 204, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 205, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 205, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 205, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 206, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 206, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 206, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 207, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 207, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 207, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 208, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 208, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 208, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 209, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 209, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 209, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 210, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 210, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 210, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 211, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 211, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 211, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 212, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 212, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 212, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 213, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 213, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 213, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 214, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 214, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 214, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 215, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 215, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 215, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 216, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 216, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 216, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 217, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 217, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 217, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 218, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 218, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 218, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 219, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 219, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 219, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 220, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 220, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 220, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 221, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 221, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 221, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 222, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 222, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 222, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 223, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 223, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 223, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 224, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 224, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 224, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 225, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 225, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 225, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 226, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 226, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 226, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 227, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 227, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 227, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 228, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 228, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 228, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 229, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 229, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 229, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 230, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 230, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 230, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 231, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 231, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 231, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 232, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 232, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 232, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 233, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 233, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 233, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 234, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 234, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 234, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 235, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 235, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 235, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 236, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 236, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 236, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 237, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 237, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 237, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 238, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 238, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 238, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 239, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 239, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 239, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 240, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 240, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 240, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 241, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 241, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 241, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 242, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 242, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 242, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 243, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 243, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 243, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 244, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 244, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 244, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 245, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 245, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 245, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 246, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 246, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 246, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 247, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 247, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 247, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 248, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 248, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 248, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 249, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 249, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 249, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 250, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 250, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 250, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 251, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 251, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 251, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 252, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 252, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 252, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 253, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 253, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 253, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 254, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 254, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 254, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 255, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 255, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 255, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 256, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 256, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 256, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 257, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 257, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 257, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 258, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 258, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 258, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 259, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 259, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 259, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 260, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 260, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 260, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 261, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 261, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 261, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 262, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 262, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 262, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 263, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 263, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 263, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 264, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 264, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 264, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 265, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 265, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 265, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 266, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 266, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 266, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 267, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 267, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 267, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 268, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 268, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 268, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 269, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 269, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 269, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 270, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 270, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 270, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 271, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 271, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 271, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 272, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 272, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 272, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 273, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 273, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 273, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 274, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 274, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 274, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 275, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 275, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 275, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 276, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 276, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 276, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 277, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 277, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 277, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 278, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 278, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 278, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 279, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 279, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 279, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 280, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 280, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 280, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 281, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 281, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 281, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 282, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 282, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 282, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 283, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 283, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 283, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 284, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 284, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 284, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 285, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 285, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 285, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 286, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 286, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 286, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 287, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 287, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 287, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 288, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 288, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 288, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 289, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 289, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 289, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 290, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 290, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 290, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 291, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 291, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 291, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 292, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 292, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 292, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 293, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 293, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 293, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 294, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 294, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 294, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 295, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 295, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 295, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 296, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 296, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 296, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 297, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 297, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 297, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 298, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 298, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 298, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 299, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 299, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 299, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 300, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 300, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 300, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 301, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 301, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 301, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 302, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 302, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 302, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 303, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 303, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 303, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 304, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 304, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 304, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 305, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 305, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 305, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 306, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 306, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 306, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 307, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 307, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 307, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 308, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 308, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 308, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 309, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 309, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 309, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 310, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 310, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 310, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 311, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 311, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 311, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 312, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 312, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 312, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 313, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 313, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 313, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 314, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 314, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 314, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 315, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 315, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 315, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 316, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 316, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 316, batch:20, train loss: 0.4941537082195282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 317, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 317, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 317, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 318, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 318, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 318, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 319, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 319, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 319, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 320, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 320, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 320, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 321, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 321, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 321, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 322, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 322, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 322, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 323, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 323, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 323, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 324, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 324, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 324, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 325, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 325, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 325, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 326, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 326, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 326, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 327, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 327, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 327, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 328, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 328, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 328, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 329, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 329, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 329, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 330, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 330, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 330, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 331, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 331, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 331, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 332, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 332, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 332, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 333, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 333, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 333, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 334, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 334, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 334, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 335, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 335, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 335, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 336, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 336, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 336, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 337, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 337, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 337, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 338, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 338, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 338, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 339, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 339, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 339, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 340, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 340, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 340, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 341, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 341, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 341, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 342, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 342, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 342, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 343, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 343, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 343, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 344, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 344, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 344, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 345, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 345, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 345, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 346, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 346, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 346, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 347, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 347, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 347, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 348, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 348, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 348, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 349, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 349, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 349, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 350, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 350, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 350, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 351, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 351, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 351, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 352, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 352, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 352, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 353, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 353, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 353, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 354, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 354, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 354, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 355, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 355, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 355, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 356, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 356, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 356, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 357, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 357, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 357, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 358, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 358, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 358, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 359, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 359, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 359, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 360, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 360, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 360, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 361, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 361, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 361, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 362, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 362, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 362, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 363, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 363, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 363, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 364, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 364, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 364, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 365, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 365, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 365, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 366, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 366, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 366, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 367, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 367, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 367, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 368, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 368, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 368, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 369, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 369, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 369, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 370, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 370, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 370, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 371, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 371, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 371, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 372, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 372, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 372, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 373, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 373, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 373, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 374, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 374, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 374, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 375, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 375, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 375, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 376, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 376, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 376, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 377, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 377, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 377, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 378, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 378, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 378, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 379, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 379, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 379, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 380, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 380, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 380, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 381, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 381, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 381, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 382, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 382, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 382, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 383, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 383, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 383, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 384, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 384, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 384, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 385, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 385, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 385, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 386, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 386, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 386, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 387, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 387, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 387, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 388, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 388, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 388, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 389, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 389, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 389, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 390, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 390, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 390, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 391, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 391, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 391, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 392, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 392, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 392, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 393, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 393, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 393, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 394, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 394, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 394, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 395, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 395, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 395, batch:20, train loss: 0.4941537082195282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 396, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 396, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 396, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 397, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 397, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 397, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 398, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 398, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 398, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 399, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 399, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 399, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 400, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 400, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 400, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 401, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 401, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 401, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 402, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 402, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 402, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 403, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 403, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 403, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 404, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 404, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 404, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 405, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 405, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 405, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 406, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 406, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 406, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 407, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 407, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 407, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 408, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 408, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 408, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 409, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 409, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 409, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 410, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 410, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 410, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 411, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 411, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 411, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 412, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 412, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 412, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 413, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 413, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 413, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 414, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 414, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 414, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 415, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 415, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 415, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 416, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 416, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 416, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 417, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 417, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 417, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 418, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 418, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 418, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 419, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 419, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 419, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 420, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 420, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 420, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 421, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 421, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 421, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 422, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 422, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 422, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 423, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 423, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 423, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 424, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 424, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 424, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 425, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 425, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 425, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 426, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 426, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 426, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 427, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 427, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 427, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 428, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 428, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 428, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 429, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 429, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 429, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 430, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 430, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 430, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 431, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 431, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 431, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 432, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 432, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 432, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 433, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 433, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 433, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 434, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 434, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 434, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 435, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 435, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 435, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 436, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 436, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 436, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 437, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 437, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 437, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 438, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 438, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 438, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 439, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 439, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 439, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 440, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 440, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 440, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 441, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 441, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 441, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 442, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 442, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 442, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 443, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 443, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 443, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 444, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 444, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 444, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 445, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 445, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 445, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 446, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 446, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 446, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 447, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 447, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 447, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 448, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 448, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 448, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 449, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 449, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 449, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 450, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 450, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 450, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 451, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 451, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 451, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 452, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 452, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 452, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 453, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 453, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 453, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 454, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 454, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 454, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 455, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 455, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 455, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 456, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 456, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 456, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 457, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 457, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 457, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 458, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 458, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 458, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 459, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 459, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 459, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 460, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 460, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 460, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 461, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 461, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 461, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 462, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 462, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 462, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 463, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 463, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 463, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 464, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 464, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 464, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 465, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 465, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 465, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 466, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 466, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 466, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 467, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 467, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 467, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 468, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 468, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 468, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 469, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 469, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 469, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 470, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 470, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 470, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 471, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 471, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 471, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 472, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 472, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 472, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 473, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 473, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 473, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 474, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 474, batch:10, train loss: 0.7458661794662476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 474, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 475, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 475, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 475, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 476, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 476, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 476, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 477, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 477, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 477, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 478, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 478, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 478, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 479, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 479, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 479, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 480, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 480, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 480, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 481, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 481, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 481, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 482, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 482, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 482, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 483, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 483, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 483, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 484, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 484, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 484, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 485, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 485, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 485, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 486, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 486, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 486, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 487, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 487, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 487, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 488, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 488, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 488, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 489, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 489, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 489, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 490, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 490, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 490, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 491, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 491, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 491, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 492, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 492, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 492, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 493, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 493, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 493, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 494, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 494, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 494, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 495, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 495, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 495, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 496, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 496, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 496, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 497, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 497, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 497, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 498, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 498, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 498, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 499, batch:0, train loss: 0.5505407452583313\n",
      "epoch: 499, batch:10, train loss: 0.7458661794662476\n",
      "epoch: 499, batch:20, train loss: 0.4941537082195282\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for j in range(num_epochs):\n",
    "\n",
    "#    for i in range(batch_number):\n",
    "        \n",
    "        # 如果不是该epoch里面最后一组数据\n",
    " #       if i != (batch_number - 1):\n",
    "#             print(y[i * batch_size: (i+1) * batch_size, :].shape)\n",
    "#            batch_y = y[i * batch_size: (i+1) * batch_size, :]\n",
    "#            batch_x = x[i * batch_size: (i+1) * batch_size, :]\n",
    " #       else:\n",
    "#             print(y[i * batch_size: , :].shape)\n",
    " #           batch_y = y[i * batch_size: , :]\n",
    " #           batch_x = x[i * batch_size: , :]\n",
    "        \n",
    "\n",
    "for j in range(num_epochs):\n",
    "    for i, (batch_x, batch_y) in enumerate(dataloader):      \n",
    "        # 前向传播\n",
    "        out = net(batch_x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, batch_y)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch: {j}, batch:{i}, train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "    \n",
    "\n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result_train = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "        result_test = (np.argmax(net(test_x_orig).detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "        \n",
    "\n",
    "        print(\"train Acc：\", np.mean(result_train),'\\ntest Acc：', np.mean(result_test), '\\n')\n",
    "    \n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result_train))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
