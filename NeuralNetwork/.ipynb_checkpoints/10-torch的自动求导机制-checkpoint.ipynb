{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-torch的自动求导机制\n",
    "\n",
    "参考资料：叶子节点和tensor的requires_grad参数 \\\n",
    "https://zhuanlan.zhihu.com/p/85506092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "tensor([5.])\n",
      "grad_fn: \n",
      "  None None <AddBackward0 object at 0x000001F8D4BFBA88> <AddBackward0 object at 0x000001F8D4C04A48> <MulBackward0 object at 0x000001F8D4C04AC8>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "# 计算\n",
    "# y = a * (w + 1)\n",
    "\n",
    "# 需要计算梯度-requires_grad=True\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# 前向传播\n",
    "a = torch.add(w, x)\n",
    "\n",
    "# 保存非叶子节点a的梯度\n",
    "# a.retain_grad()\n",
    "\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "# y = a * (w + 1)\n",
    "\n",
    "# print(x1.is_leaf)\n",
    "print(x.is_leaf)\n",
    "print(w.is_leaf)\n",
    "\n",
    "print(a.is_leaf)\n",
    "print(b.is_leaf)\n",
    "\n",
    "print(y.is_leaf)\n",
    "\n",
    "# 反向传播-自动求导\n",
    "y.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# torch.Tensor\n",
    "# np.ndarray\n",
    "\n",
    "# 查看创建张良所使用的函数\n",
    "print(\"grad_fn: \\n \", w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183], grad_fn=<ExpBackward>)\n",
      "tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "# x = 1\n",
    "# saved_tensors e\n",
    "\n",
    "# 0.1 * e\n",
    "# 网络层数深，可能会引起梯度消失/爆炸的问题\n",
    "\n",
    "class Exp(Function):                    # 此层计算e^x\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):                # 模型前向\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)   # 保存所需内容，以备backward时使用，所需的结果会被保存在saved_tensors元组中；此处仅能保存tensor类型变量，若其余类型变量（Int等），可直接赋予ctx作为成员变量，也可以达到保存效果\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):     # 模型梯度反传\n",
    "        result, = ctx.saved_tensors     # 取出forward中保存的result\n",
    "        return grad_output * result     # 计算梯度并返回\n",
    "\n",
    "# 尝试使用\n",
    "x = torch.tensor([1.], requires_grad=True)  # 需要设置tensor的requires_grad属性为True，才会进行梯度反传\n",
    "ret = Exp.apply(x)                          # 使用apply方法调用自定义autograd function\n",
    "print(ret)                                  # tensor([2.7183], grad_fn=<ExpBackward>)\n",
    "ret.backward()                              # 反传梯度\n",
    "print(x.grad)                               # tensor([2.7183])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始前向传播\n",
      "output tensor([1.0843])\n",
      "output tensor([0.6369])\n",
      "完成, z_pred2 =  tensor([0.6369], grad_fn=<MultiplyAddBackward>)\n",
      "相当于默认z_true=0时，开始反向传播\n",
      "=======================================\n",
      "z_pred2 tensor([1.])\n",
      "=======================================\n",
      "=======================================\n",
      "z_pred2 tensor([0.5330])\n",
      "=======================================\n",
      "tensor([0.3967]) tensor([0.5330]) tensor([0.5330])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\29533\\anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    " \n",
    "# 类需要继承Function类，此处forward和backward都是静态方法\n",
    "# 实现linear层\n",
    "\n",
    "class MultiplyAdd(Function):  \n",
    "                                                             \n",
    "    @staticmethod                                  \n",
    "    def forward(ctx, w, x, b):                 \n",
    "        ctx.save_for_backward(w,x)    #保存参数,这跟前一篇的self.save_for_backward()是一样的\n",
    "        output = w * x + b\n",
    "        print(\"output\", output)\n",
    "        return output                        \n",
    "         \n",
    "    @staticmethod                                 \n",
    "    def backward(ctx, grad_output):    #获取保存的参数,这跟前一篇的self.saved_variables()是一样的\n",
    "        w,x = ctx.saved_variables  \n",
    "\n",
    "        grad_w = grad_output * x\n",
    "        grad_x = grad_output * w\n",
    "        grad_b = grad_output * 1\n",
    "        print(\"=======================================\")\n",
    "        print(\"z_pred2\",grad_output)\n",
    "        print(\"=======================================\")\n",
    "        return grad_w, grad_x, grad_b  # backward输入参数和forward输出参数必须一一对应\n",
    "\n",
    "x = torch.ones(1,requires_grad=True)  # x 是1，所以grad_w=1\n",
    "w = torch.rand(1,requires_grad=True)  # w 是随机的，所以grad_x=随机的一个数\n",
    "b = torch.rand(1,requires_grad=True)  # grad_b 恒等于1\n",
    "\n",
    "\n",
    "w2 = torch.rand(1,requires_grad=True)  # w 是随机的，所以grad_x=随机的一个数\n",
    "b2 = torch.rand(1,requires_grad=True)  # grad_b 恒等于1\n",
    "\n",
    "\n",
    "\n",
    "print('开始前向传播')\n",
    "z_pred = MultiplyAdd.apply(w, x, b)   # forward,这里的前向传播是不一样的，这里没有使用函数去包装自定义的类，而是直接使用apply方法\n",
    "z_pred.retain_grad()\n",
    "\n",
    "# for i in range(200):\n",
    "#     w2 = torch.rand(1,requires_grad=True)  # w 是随机的，所以grad_x=随机的一个数\n",
    "#     b2 = torch.rand(1,requires_grad=True)  # grad_b 恒等于1\n",
    "#     z_pred2=MultiplyAdd.apply(w2, z_pred2, b2)   # forward,这里的前向传播是不一样的，这里没有使用函数去包装自定义的类，而是直接使用apply方法\n",
    "\n",
    "\n",
    "z_pred2=MultiplyAdd.apply(w2, z_pred, b2)   # forward,这里的前向传播是不一样的，这里没有使用函数去包装自定义的类，而是直接使用apply方法\n",
    "print(\"完成, z_pred2 = \",z_pred2)\n",
    "\n",
    "print('相当于默认z_true=0时，开始反向传播')\n",
    "z_pred2.backward()                   # backward\n",
    "\n",
    "\n",
    "# print(\"z_pred.grad\", z_pred.grad)\n",
    "\n",
    "print(x.grad, w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfDefinedRelu(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        ctx.save_for_backward(inp)\n",
    "        return torch.where(inp < 0., torch.zeros_like(inp), inp)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        inp, = ctx.saved_tensors\n",
    "        print(\"grad_output:\\n\",grad_output)\n",
    "        print(\"torch.where：\\n,\", torch.where(inp < 0, torch.zeros_like(inp),\n",
    "                                         torch.ones_like(inp)))\n",
    "        return grad_output * torch.where(inp < 0, torch.zeros_like(inp),\n",
    "                                         torch.ones_like(inp))\n",
    "\n",
    "class Relu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = SelfDefinedRelu.apply(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6382, grad_fn=<SumBackward0>)\n",
      "grad_output:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "torch.where：\n",
      ", tensor([0., 1., 0., 0., 0., 0., 1., 0., 1., 0.])\n",
      "output2: \n",
      " tensor([0.0000, 0.2010, 0.0000, 0.0000, 0.0000, 0.0000, 0.3184, 0.0000, 0.1188,\n",
      "        0.0000], grad_fn=<SelfDefinedReluBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "torch.relu(..)\n",
    "\n",
    "relu = torch.nn.RELU()\n",
    "relu(...)\n",
    "\n",
    "np_in = np.array([-5])\n",
    "tensor_in = torch.tensor(np_in)\n",
    "# 也可以这样：tensor_in = torch.tensor([-5])\n",
    "\n",
    "'''\n",
    "拓展阅读（不要求记住，因为只是作为一个工具的拓展使用参考，实际使用时可随时查阅）：\n",
    "torch.tensor()和torch.Tensor()的区别\n",
    "https://blog.csdn.net/weixin_42018112/article/details/91383574\n",
    "'''\n",
    "\n",
    "# (0,1) - 0.5 -> (-0.5, 0.5)\n",
    "inp = torch.rand(10,requires_grad=True) - 0.5\n",
    "\n",
    "net = Relu()\n",
    "\n",
    "# output1 = net(tensor_in)\n",
    "output2 = net(inp)\n",
    "print(output2.sum())\n",
    "# 只有标量才能反向传播\n",
    "# output2.backward()\n",
    "output2.sum().backward()\n",
    "\n",
    "# print(\"\",output1)\n",
    "print(\"output2: \\n\",output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2754, 0.0739],\n",
      "        [0.6594, 0.2331]])\n",
      "tensor([[0., 0.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)\n",
    "\n",
    "# a\n",
    "# [[0, 0],\n",
    "#  [1, 0]]\n",
    "\n",
    "# zeros_like a\n",
    "# [[0, 0],\n",
    "#  [0, 0]]\n",
    "\n",
    "# ones_like a\n",
    "# [[1, 1],\n",
    "#  [1, 1]]\n",
    "\n",
    "torch_where = torch.where(a < 0.5, torch.zeros_like(a), torch.ones_like(a))\n",
    "\n",
    "print(a)\n",
    "print(torch_where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行运算： $f(x) = ax^2 + bx + c$\n",
    "\n",
    "$df/dx = 2ax + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = a*x**2 + b*x + c\n",
    "# 需要被求导\n",
    "\n",
    "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad =True)# x\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2)+ b*x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0., 0.],\n",
      "        [1., 2.]], requires_grad=True)\n",
      "y: tensor([[1., 1.],\n",
      "        [0., 1.]], grad_fn=<AddBackward0>)\n",
      "tensor(3., grad_fn=<SumBackward0>)\n",
      "tensor([[-2., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "\n",
    "# RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "# y.backward()\n",
    "\n",
    "y.sum().backward()\n",
    "\n",
    "dy_dx = x.grad\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "x: tensor([[0.,0.],\n",
    "[1.,2.]], requires_grad=True)\n",
    "y: tensor([[1.,1.],\n",
    "[0.,1.]], grad_fn=<AddBackward0>)\n",
    "x_grad:\n",
    " tensor([[-2.,-2.],\n",
    "[0.,2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求以下函数的二阶导数：$$f(x) = ax^2 + bx + c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 求 f(x) = a*x**2 + b*x + c\n",
    "\n",
    "x = torch.tensor(0.0,requires_grad =True)# x\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "y = a*torch.pow(x,2)+ b*x + c\n",
    "\n",
    "# 设置为将允许创建更⾼阶的导数\n",
    "# create_graph  = True \n",
    "\n",
    "dy_dx = torch.autograd.grad(y,x,create_graph=True)[0]\n",
    "print(dy_dx.data)\n",
    "\n",
    "# 求⼆阶导数\n",
    "dy2_dx2 = torch.autograd.grad(dy_dx,x)[0]\n",
    "print(dy2_dx2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) tensor(1.)\n",
      "False False\n",
      "tensor(1.) None\n",
      "tensor(3.) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "# 需要被求导\n",
    "x1 = torch.tensor(1.0,requires_grad =True)# x\n",
    "x2 = torch.tensor(2.0,requires_grad =True)\n",
    "y1 = x1*x2\n",
    "y2 = x1+x2\n",
    "\n",
    "y1.retain_grad()\n",
    "y2.retain_grad()\n",
    "\n",
    "# 允许同时对多个⾃变量求导数\n",
    "(dy1_dx1,dy1_dx2)= torch.autograd.grad(outputs=y1,\n",
    "                inputs =[x1,x2],retain_graph =True)\n",
    "print(dy1_dx1,dy1_dx2)\n",
    "\n",
    "# 如果有多个因变量，相当于把多个因变量的梯度结果求和\n",
    "\n",
    "print(y1.is_leaf, y2.is_leaf)\n",
    "print(y1.grad,y2.grad)\n",
    "\n",
    "(dy12_dx1,dy12_dx2)= torch.autograd.grad(outputs=[y1,y2],\n",
    "            inputs =[x1,x2])\n",
    "print(dy12_dx1,dy12_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
