{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy的ndarray和Tensor的区别\n",
    "\n",
    "参考资料：\n",
    "PyTorch中torch.tensor与torch.Tensor的区别详解\n",
    "https://www.jb51.net/article/186764.htm\n",
    "\n",
    "**注：pytorch里的Tensor与tensorflow无任何关系，仅仅只是pytorch里面的一个数据类型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\29533\\anaconda3\\envs\\pytorch1.7\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "创建一个Tensor并设置requires_grad=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_zeros: \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]], requires_grad=True)\n",
      "x_ones: \n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "x_randn: \n",
      " tensor([[-2.0333,  2.0763],\n",
      "        [-2.4553, -1.7934]], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "# 分别创建全0、全1、随机的tensor\n",
    "x_zeros = torch.zeros(2, 2, requires_grad=True)\n",
    "x_ones = torch.ones(2, 2, requires_grad=True)\n",
    "x_randn = torch.randn(2, 2, requires_grad=True) # 缺失情况下默认 requires_grad = False\n",
    "\n",
    "print(\"x_zeros: \\n\", x_zeros)\n",
    "print(\"x_ones: \\n\", x_ones)\n",
    "print(\"x_randn: \\n\", x_randn)\n",
    "print()## numpy array 和 Tensor相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy array 和 Tensor 相互转换\n",
    "\n",
    "参考资料：\n",
    "torch里面的Tensor、as_tensor、tensor以及from_numpy究竟有何区别? \\\n",
    "https://www.136.la/jingpin/show-208975.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor 转 numpy array\n",
    "# RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "# a = x_zeros.numpy()\n",
    "\n",
    "a = x_zeros.detach().numpy()\n",
    "\n",
    "a_tensor = torch.from_numpy(a)\n",
    "a_tensor = torch.tensor(a)\n",
    "a_tensor = torch.as_tensor(a)\n",
    "a_tensor = torch.Tensor(a)\n",
    "\n",
    "print(a_tensor)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy的ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_zeros_np: \n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "x_ones_np_np: \n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "x_rand_np_np: \n",
      " [[-0.68159888 -0.25185985]\n",
      " [ 1.36658599 -1.45654863]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分别创建全0、全1、随机的ndarray数组\n",
    "\n",
    "x_zeros_np = np.zeros((2, 2))\n",
    "x_ones_np = np.ones((2, 2))\n",
    "x_rand_np = np.random.randn(2, 2)\n",
    "\n",
    "print(\"x_zeros_np: \\n\", x_zeros_np)\n",
    "print(\"x_ones_np_np: \\n\", x_ones_np)\n",
    "print(\"x_rand_np_np: \\n\", x_rand_np)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "y的ndarray.grad_fn:  <AddBackward0 object at 0x000001A8D2C7B0C8>\n"
     ]
    }
   ],
   "source": [
    "y = x_ones + 2\n",
    "print(y)\n",
    "print(\"y的ndarray.grad_fn: \",y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有grad_fn, 而y是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn。\n",
    "\n",
    "像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x_ones.is_leaf, y.is_leaf) # True False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来点复杂度运算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "\n",
    "out_mean = z.mean()\n",
    "z_sum = z.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要求查Tensor.sum()的函数使用方法去是实现以下要求\n",
    "\n",
    "\n",
    "### 对 z.sum(None) 中的None进行填空\n",
    "\n",
    "\n",
    "### (查询函数需要传入的参数)\n",
    "### 参考: https://blog.csdn.net/hahameier/article/details/103742831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor(108., grad_fn=<SumBackward0>)\n",
      "\n",
      "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
      "torch.Size([2])\n",
      "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 第1个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_sum_dim1 = z.sum(dim=1)\n",
    "# 第0个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_sum_dim0 = z.sum(dim=0)\n",
    "\n",
    "print(z)\n",
    "print(out_mean)\n",
    "print(z_sum)\n",
    "\n",
    "print()\n",
    "print(z_sum_dim1)\n",
    "print(z_sum_dim1.shape)\n",
    "print(z_sum_dim0)\n",
    "print(z_sum_dim0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果\n",
    "\n",
    "```python\n",
    "tensor([[27., 27.],\n",
    "        [27., 27.]], grad_fn=<MulBackward0>)\n",
    "    \n",
    "tensor(27., grad_fn=<MeanBackward0>)\n",
    "tensor(108., grad_fn=<SumBackward0>)\n",
    "\n",
    "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
    "torch.Size([2])\n",
    "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
    "torch.Size([2])\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "z_re: \n",
      " tensor([[[27.]],\n",
      "\n",
      "        [[27.]],\n",
      "\n",
      "        [[27.]],\n",
      "\n",
      "        [[27.]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "z_re_sum_dim0: \n",
      " tensor([[108.]], grad_fn=<SumBackward1>)\n",
      "z_re_sum_dim1: \n",
      " tensor([[27.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [27.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z_re = z.reshape(4,1,1)\n",
    "print()\n",
    "print(\"z_re: \\n\",z_re)\n",
    "\n",
    "# 第0个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_re_sum_dim0 = z_re.sum(dim=0)\n",
    "print(\"z_re_sum_dim0: \\n\",z_re_sum_dim0)\n",
    "\n",
    "# 第1个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_re_sum_dim1 = z_re.sum(dim=1)\n",
    "print(\"z_re_sum_dim1: \\n\",z_re_sum_dim1)\n",
    "\n",
    "\n",
    "# 所有的第二个维度的元素相加\n",
    "# 第二个维度的结构: [[27.]]\n",
    "# 第二个维度数组中包含的所有元素: [27.]\n",
    "\n",
    "# 所有的第二个维度的元素相加结果\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "\n",
    "# 最后变成：\n",
    "# [[27.],\n",
    "#  [27.],\n",
    "#  [27.],\n",
    "#  [27.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果\n",
    "\n",
    "```python\n",
    "z_re: \n",
    " tensor([[[27.]],\n",
    "\n",
    "        [[27.]],\n",
    "\n",
    "        [[27.]],\n",
    "\n",
    "        [[27.]]], grad_fn=<ReshapeAliasBackward0>)\n",
    "z_re_sum_dim0: \n",
    " tensor([[108.]], grad_fn=<SumBackward1>)\n",
    "z_re_sum_dim1: \n",
    " tensor([[27.],\n",
    "        [27.],\n",
    "        [27.],\n",
    "        [27.]], grad_fn=<SumBackward1>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过.requires_grad_()来用in-place的方式改变requires_grad属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001A8D2CE0C08>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "因为out是一个标量，所以调用backward()时不需要指定求导变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward() # 等价于 out.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看out关于x的梯度 undefined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x_ones.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反向传播之前\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "反向传播之后\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x_ones.sum()\n",
    "out2.backward()\n",
    "\n",
    "x_ones.grad.data.zero_()\n",
    "print(\"反向传播之前\")\n",
    "print(x_ones.grad)\n",
    "out2.backward()\n",
    "print(\"反向传播之后\")\n",
    "print(x_ones.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图 Computer Graph(CG) 是用来描述运算的有向无环图\n",
    "\n",
    "计算图（Computer Graph）是用来描述运算的有向无环图\n",
    "CG包括两个主要元素：结点（Node）、边（Edge）\n",
    "\n",
    "Node表示数据\n",
    "Edge表示运算\n",
    "如果想使用非leaf结点的梯度。则需要在反向传播之前调用.retain_grad() (不用记)\n",
    "\n",
    "**CG作用：方便梯度求取及反向传播**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例\n",
    "#### y = (x+w)*w，则当w=1，x=2时，y对w的grad是多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n",
      "<MulBackward0 object at 0x0000027DCA7DB448>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "b = torch.tensor([6.], requires_grad=True)\n",
    "w = torch.tensor([5.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "y = torch.mul(a, w)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机的从零开始实现\n",
    "\n",
    "下面，我们一起来动手实现一个多层感知机。首先导入实现所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from func.dnn_app_utils_v2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获取和读取数据\n",
    "\n",
    "这里继续使用上一次实验使用的数据集,但本次使用构建神经网络的框架pytorch来进行构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Example of a picture\n",
    "index = 7\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\n",
    "\n",
    "# 标准化\n",
    "train_x_orig = train_x_orig/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义、初始化模型参数\n",
    "\n",
    "我们提供的数据集中图像形状为 $64 \\times 64 \\times 3$，类别数为2。本节中我们依然使用长度为 $64 \\times 64 \\times 3 = 12288$ 的向量表示每一张图像。因此，输入个数为12288$，输出类别个数为2。实验中，我们设超参数隐藏单元个数为256。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens, num_hiddens2 = 12288, 2, 256, 128\n",
    "\n",
    "def params_init_func():\n",
    "    W1_np = np.random.normal(0, 0.01, (num_inputs, num_hiddens))\n",
    "    W2_np = np.random.normal(0, 0.01, (num_hiddens, num_outputs))\n",
    "\n",
    "    W1 = torch.tensor(W1_np, dtype=torch.float, requires_grad = True)\n",
    "    b1 = torch.zeros(num_hiddens, dtype=torch.float, requires_grad = True)\n",
    "\n",
    "    W2 = torch.tensor(W2_np, dtype=torch.float, requires_grad = True)\n",
    "    b2 = torch.zeros(num_outputs, dtype=torch.float, requires_grad = True)\n",
    "\n",
    "    params = [W1, b1, W2, b2]\n",
    "    # params = [W1, b1]\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义激活函数(该步骤非必须)\n",
    "\n",
    "这里我们使用基础的`max`函数来实现ReLU，而非直接调用`relu`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义模型\n",
    "\n",
    "我们通过`view`函数将每张原始图像改成长度为`num_inputs`的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_inputs, num_outputs, num_hiddens,num_hiddens2 = 12288, 2, 256, 128\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# 继承 torch.nn.Module\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, params_init_func, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        params = params_init_func()\n",
    "        \n",
    "        for param in params:\n",
    "            param.requires_grad_(requires_grad=True)\n",
    "        \n",
    "        self.W1 = nn.Parameter(params[0].float())\n",
    "        self.b1 = nn.Parameter(params[1].float())\n",
    "\n",
    "        self.W2 = nn.Parameter(params[2].float())\n",
    "        self.b2 = nn.Parameter(params[3].float())\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view((-1, num_inputs))\n",
    "        # add matmul relu\n",
    "        H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "#         print(self.W1.grad)\n",
    "        H = torch.matmul(H, self.W2) + self.b2\n",
    "        return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义损失函数\n",
    "\n",
    "为了得到更好的数值稳定性，我们直接使用PyTorch提供的包括softmax运算和交叉熵损失计算的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练模型\n",
    "\n",
    "构建模型,定义optimizer,传递模型需要优化的参数给optimizer,然后开始训练,我们在这里设超参数迭代周期数为500，学习率为0.01。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 0.6850559115409851\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 1 train loss: 0.6696770191192627\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 2 train loss: 0.6606928110122681\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 3 train loss: 0.6552541851997375\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 4 train loss: 0.6519327163696289\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 5 train loss: 0.6498247385025024\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 6 train loss: 0.6483960151672363\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 7 train loss: 0.6473532319068909\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 0.6465044617652893\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 9 train loss: 0.645763099193573\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 10 train loss: 0.6450769305229187\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 11 train loss: 0.6444337368011475\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 12 train loss: 0.6438231468200684\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 13 train loss: 0.643235981464386\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 14 train loss: 0.6426619291305542\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 15 train loss: 0.6420894861221313\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 16 train loss: 0.6415172219276428\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 17 train loss: 0.640945553779602\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 18 train loss: 0.6403714418411255\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 19 train loss: 0.6397982239723206\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 20 train loss: 0.6392276287078857\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 21 train loss: 0.6386659145355225\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 22 train loss: 0.6381072402000427\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 23 train loss: 0.6375525593757629\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 24 train loss: 0.6369972229003906\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 25 train loss: 0.6364357471466064\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 26 train loss: 0.6358665227890015\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 27 train loss: 0.6352934837341309\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 28 train loss: 0.6347142457962036\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 29 train loss: 0.6341263651847839\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 30 train loss: 0.6335335373878479\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 31 train loss: 0.6329367756843567\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 32 train loss: 0.6323388814926147\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 33 train loss: 0.6317408084869385\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 34 train loss: 0.6311356425285339\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 35 train loss: 0.6305264830589294\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 36 train loss: 0.6299145221710205\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 37 train loss: 0.6292951703071594\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 38 train loss: 0.6286678314208984\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 39 train loss: 0.6280325651168823\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 40 train loss: 0.6273965835571289\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 41 train loss: 0.6267548203468323\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 42 train loss: 0.6261110901832581\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 43 train loss: 0.625461995601654\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 44 train loss: 0.6248070597648621\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 45 train loss: 0.6241438388824463\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 46 train loss: 0.623474657535553\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 47 train loss: 0.6227980852127075\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 48 train loss: 0.6221117973327637\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 49 train loss: 0.6214165091514587\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 50 train loss: 0.620710551738739\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 51 train loss: 0.6199971437454224\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 52 train loss: 0.619278609752655\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 53 train loss: 0.6185517907142639\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 54 train loss: 0.6178143620491028\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 55 train loss: 0.617068886756897\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 56 train loss: 0.6163219809532166\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 57 train loss: 0.6155713200569153\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 58 train loss: 0.6148144006729126\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 59 train loss: 0.614051342010498\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 60 train loss: 0.6132809519767761\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 61 train loss: 0.6125014424324036\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 62 train loss: 0.611711859703064\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 63 train loss: 0.610914409160614\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 64 train loss: 0.610113799571991\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 65 train loss: 0.6093072891235352\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 66 train loss: 0.6084924936294556\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 67 train loss: 0.6076737642288208\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 68 train loss: 0.6068481802940369\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 69 train loss: 0.6060163378715515\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 70 train loss: 0.6051737070083618\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 71 train loss: 0.6043204665184021\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 72 train loss: 0.60345458984375\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 73 train loss: 0.6025819182395935\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 74 train loss: 0.601705014705658\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 75 train loss: 0.600821852684021\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 76 train loss: 0.5999244451522827\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 77 train loss: 0.5990195274353027\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 78 train loss: 0.5981101989746094\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 79 train loss: 0.5971961617469788\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 80 train loss: 0.5962815284729004\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 81 train loss: 0.5953589081764221\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 82 train loss: 0.594430148601532\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 83 train loss: 0.5934916734695435\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 84 train loss: 0.5925472378730774\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 85 train loss: 0.591596782207489\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 86 train loss: 0.5906391143798828\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 87 train loss: 0.5896755456924438\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 88 train loss: 0.5887022018432617\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 89 train loss: 0.5877230167388916\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 90 train loss: 0.5867414474487305\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 91 train loss: 0.5857552289962769\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 92 train loss: 0.584763765335083\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 93 train loss: 0.583762526512146\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 94 train loss: 0.5827546715736389\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 95 train loss: 0.5817457437515259\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 96 train loss: 0.5807299017906189\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 97 train loss: 0.579704999923706\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 98 train loss: 0.5786779522895813\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 99 train loss: 0.5776462554931641\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 100 train loss: 0.5766103267669678\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 101 train loss: 0.5755628943443298\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 102 train loss: 0.5745089054107666\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 103 train loss: 0.5734514594078064\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 104 train loss: 0.5723893046379089\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 105 train loss: 0.5713174939155579\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 106 train loss: 0.5702388286590576\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 107 train loss: 0.569150984287262\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 108 train loss: 0.5680601596832275\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 109 train loss: 0.566956639289856\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 110 train loss: 0.5658441781997681\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 111 train loss: 0.5647289752960205\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 112 train loss: 0.56361323595047\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 113 train loss: 0.5624942183494568\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 114 train loss: 0.5613692402839661\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 115 train loss: 0.560234010219574\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 116 train loss: 0.5590970516204834\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 117 train loss: 0.5579589605331421\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 118 train loss: 0.5568163990974426\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 119 train loss: 0.5556680560112\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 120 train loss: 0.5545142889022827\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 121 train loss: 0.5533618330955505\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 122 train loss: 0.5521999001502991\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 123 train loss: 0.551024854183197\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 124 train loss: 0.5498536229133606\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 125 train loss: 0.548677384853363\n",
      "开始测试\n",
      "Acc： 0.7081339712918661 \n",
      "\n",
      "epoch: 126 train loss: 0.547497034072876\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 127 train loss: 0.5463166236877441\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 128 train loss: 0.545129120349884\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 129 train loss: 0.5439320206642151\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 130 train loss: 0.5427333116531372\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 131 train loss: 0.5415396690368652\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 132 train loss: 0.54033362865448\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 133 train loss: 0.5391346216201782\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 134 train loss: 0.5379199385643005\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 135 train loss: 0.536711573600769\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 136 train loss: 0.535504162311554\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 137 train loss: 0.5342816114425659\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 138 train loss: 0.5330599546432495\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 139 train loss: 0.5318368077278137\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 140 train loss: 0.5305988788604736\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 141 train loss: 0.529357373714447\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 142 train loss: 0.5281084179878235\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 143 train loss: 0.5268643498420715\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 144 train loss: 0.5256194472312927\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 145 train loss: 0.5243724584579468\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 146 train loss: 0.5231156349182129\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 147 train loss: 0.5218654870986938\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 148 train loss: 0.520612359046936\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 149 train loss: 0.5193530321121216\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 150 train loss: 0.5181078314781189\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 151 train loss: 0.5168342590332031\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 152 train loss: 0.5155591368675232\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 153 train loss: 0.5142993330955505\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 154 train loss: 0.5130490660667419\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 155 train loss: 0.5117930173873901\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 156 train loss: 0.5105180144309998\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 157 train loss: 0.5092687010765076\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 158 train loss: 0.5079928636550903\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 159 train loss: 0.5067212581634521\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 160 train loss: 0.5054185390472412\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 161 train loss: 0.5041460990905762\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 162 train loss: 0.5028626918792725\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 163 train loss: 0.501588761806488\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 164 train loss: 0.5002752542495728\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 165 train loss: 0.49898770451545715\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 166 train loss: 0.4977015554904938\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 167 train loss: 0.4964194893836975\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 168 train loss: 0.4950839579105377\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 169 train loss: 0.49381211400032043\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 170 train loss: 0.49248990416526794\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 171 train loss: 0.49122053384780884\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 172 train loss: 0.48994719982147217\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 173 train loss: 0.4886741042137146\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 174 train loss: 0.48736244440078735\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 175 train loss: 0.4861190915107727\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 176 train loss: 0.4848644733428955\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 177 train loss: 0.48365041613578796\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 178 train loss: 0.48241695761680603\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 179 train loss: 0.48117005825042725\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 180 train loss: 0.4799244999885559\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 181 train loss: 0.4786965847015381\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 182 train loss: 0.47746652364730835\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 183 train loss: 0.47632959485054016\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 184 train loss: 0.4750567078590393\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 185 train loss: 0.47395840287208557\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 186 train loss: 0.47270625829696655\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 187 train loss: 0.471691370010376\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 188 train loss: 0.4705255627632141\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 189 train loss: 0.4695349931716919\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 190 train loss: 0.46842166781425476\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 191 train loss: 0.4677816927433014\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 192 train loss: 0.46695104241371155\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 193 train loss: 0.4664986729621887\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 194 train loss: 0.4659399688243866\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 195 train loss: 0.4658948481082916\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 196 train loss: 0.4655429422855377\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 197 train loss: 0.46589186787605286\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 198 train loss: 0.465690940618515\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 199 train loss: 0.46664443612098694\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 200 train loss: 0.46668297052383423\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 201 train loss: 0.4685414433479309\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 202 train loss: 0.4686332643032074\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 203 train loss: 0.471087783575058\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 204 train loss: 0.4710409939289093\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 205 train loss: 0.47454342246055603\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 206 train loss: 0.4735846519470215\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 207 train loss: 0.4769814610481262\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 208 train loss: 0.4753134250640869\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 209 train loss: 0.4779071807861328\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 210 train loss: 0.4752967059612274\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 211 train loss: 0.4774714708328247\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 212 train loss: 0.47504252195358276\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 213 train loss: 0.47751784324645996\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 214 train loss: 0.4745095372200012\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 215 train loss: 0.47714465856552124\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 216 train loss: 0.4738958179950714\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 217 train loss: 0.475776344537735\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 218 train loss: 0.4722500741481781\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 219 train loss: 0.47335055470466614\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 220 train loss: 0.4699903428554535\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 221 train loss: 0.4701250493526459\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 222 train loss: 0.466826856136322\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 223 train loss: 0.4676797688007355\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 224 train loss: 0.4652402997016907\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 225 train loss: 0.46661895513534546\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 226 train loss: 0.46484801173210144\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 227 train loss: 0.4668644368648529\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 228 train loss: 0.46558669209480286\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 229 train loss: 0.4678570330142975\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 230 train loss: 0.4650431275367737\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 231 train loss: 0.467342346906662\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 232 train loss: 0.4643949270248413\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 233 train loss: 0.46644124388694763\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 234 train loss: 0.4625864028930664\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 235 train loss: 0.4647481441497803\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 236 train loss: 0.46175917983055115\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 237 train loss: 0.4646017253398895\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 238 train loss: 0.46224355697631836\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 239 train loss: 0.46431148052215576\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 240 train loss: 0.4606926739215851\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 241 train loss: 0.4624505937099457\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 242 train loss: 0.45863085985183716\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 243 train loss: 0.4607386887073517\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 244 train loss: 0.4580368399620056\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 245 train loss: 0.4592248797416687\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 246 train loss: 0.45553016662597656\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 247 train loss: 0.45704829692840576\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 248 train loss: 0.45407938957214355\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 249 train loss: 0.45560774207115173\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 250 train loss: 0.45279309153556824\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 251 train loss: 0.4545234143733978\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 252 train loss: 0.45196062326431274\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 253 train loss: 0.4535970091819763\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 254 train loss: 0.45167264342308044\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 255 train loss: 0.45310190320014954\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 256 train loss: 0.4503474235534668\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 257 train loss: 0.4518926739692688\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 258 train loss: 0.44892317056655884\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 259 train loss: 0.450672447681427\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 260 train loss: 0.44810473918914795\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 261 train loss: 0.45013195276260376\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 262 train loss: 0.44746240973472595\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 263 train loss: 0.4489095211029053\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 264 train loss: 0.4451265335083008\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 265 train loss: 0.4474361836910248\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 266 train loss: 0.44439083337783813\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 267 train loss: 0.44596198201179504\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 268 train loss: 0.44330063462257385\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 269 train loss: 0.44522371888160706\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 270 train loss: 0.44314754009246826\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 271 train loss: 0.444092333316803\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 272 train loss: 0.43822577595710754\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 273 train loss: 0.4416956901550293\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 274 train loss: 0.44030892848968506\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 275 train loss: 0.44242414832115173\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 276 train loss: 0.4399905800819397\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 277 train loss: 0.44087350368499756\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 278 train loss: 0.4358047842979431\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 279 train loss: 0.43758538365364075\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 280 train loss: 0.4351481795310974\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 281 train loss: 0.43632960319519043\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 282 train loss: 0.4321926534175873\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 283 train loss: 0.43512919545173645\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 284 train loss: 0.4321323335170746\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 285 train loss: 0.434383362531662\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 286 train loss: 0.43121835589408875\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 287 train loss: 0.4337511658668518\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 288 train loss: 0.42990124225616455\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 289 train loss: 0.4326001703739166\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 290 train loss: 0.42966634035110474\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 291 train loss: 0.4318205416202545\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 292 train loss: 0.4270020127296448\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 293 train loss: 0.42806312441825867\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 294 train loss: 0.423063188791275\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 295 train loss: 0.42650651931762695\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 296 train loss: 0.4244714677333832\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 297 train loss: 0.4264368414878845\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 298 train loss: 0.4234757423400879\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 299 train loss: 0.42624279856681824\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 300 train loss: 0.4229256808757782\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 301 train loss: 0.42354440689086914\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 302 train loss: 0.41878026723861694\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 303 train loss: 0.4216824769973755\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 304 train loss: 0.4184391498565674\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 305 train loss: 0.42013099789619446\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 306 train loss: 0.4171401262283325\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 307 train loss: 0.41987818479537964\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 308 train loss: 0.4160575270652771\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 309 train loss: 0.41713646054267883\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 310 train loss: 0.41295987367630005\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 311 train loss: 0.4155255854129791\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 312 train loss: 0.4128895401954651\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 313 train loss: 0.41516828536987305\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 314 train loss: 0.411391019821167\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 315 train loss: 0.4142930209636688\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 316 train loss: 0.4109106957912445\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 317 train loss: 0.41247817873954773\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 318 train loss: 0.40823423862457275\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 319 train loss: 0.41074731945991516\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 320 train loss: 0.40647852420806885\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 321 train loss: 0.40830111503601074\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 322 train loss: 0.405521035194397\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 323 train loss: 0.40853869915008545\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 324 train loss: 0.4064956307411194\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 325 train loss: 0.40819039940834045\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 326 train loss: 0.403538316488266\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 327 train loss: 0.40597546100616455\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 328 train loss: 0.4024380147457123\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 329 train loss: 0.4043503403663635\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 330 train loss: 0.3989141583442688\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 331 train loss: 0.40114787220954895\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 332 train loss: 0.39795979857444763\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 333 train loss: 0.4001005291938782\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 334 train loss: 0.39631587266921997\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 335 train loss: 0.3985709547996521\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 336 train loss: 0.39710596203804016\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 337 train loss: 0.3989289700984955\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 338 train loss: 0.39501696825027466\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 339 train loss: 0.39645662903785706\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 340 train loss: 0.3929847776889801\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 341 train loss: 0.3940795063972473\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 342 train loss: 0.38967442512512207\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 343 train loss: 0.39134639501571655\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 344 train loss: 0.3880116939544678\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 345 train loss: 0.3905061185359955\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 346 train loss: 0.3887670636177063\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 347 train loss: 0.3914978802204132\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 348 train loss: 0.3888479471206665\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 349 train loss: 0.39035850763320923\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 350 train loss: 0.38589656352996826\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 351 train loss: 0.38745245337486267\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 352 train loss: 0.38282954692840576\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 353 train loss: 0.3846011459827423\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 354 train loss: 0.38171473145484924\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 355 train loss: 0.38350048661231995\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 356 train loss: 0.3809420168399811\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 357 train loss: 0.3828125\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 358 train loss: 0.3798266649246216\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 359 train loss: 0.38087931275367737\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 360 train loss: 0.3764570951461792\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 361 train loss: 0.3774636685848236\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 362 train loss: 0.37277257442474365\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 363 train loss: 0.37348318099975586\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 364 train loss: 0.3715627193450928\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 365 train loss: 0.3737330436706543\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 366 train loss: 0.3730274438858032\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 367 train loss: 0.37494802474975586\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 368 train loss: 0.3735576570034027\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 369 train loss: 0.3743019998073578\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 370 train loss: 0.37149614095687866\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 371 train loss: 0.37181952595710754\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 372 train loss: 0.3688833713531494\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 373 train loss: 0.36940616369247437\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 374 train loss: 0.36646249890327454\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 375 train loss: 0.36661452054977417\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 376 train loss: 0.3640110194683075\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 377 train loss: 0.3646725118160248\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 378 train loss: 0.36070922017097473\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 379 train loss: 0.3629167675971985\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 380 train loss: 0.3622766137123108\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 381 train loss: 0.3639059364795685\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 382 train loss: 0.3625645935535431\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 383 train loss: 0.3640407621860504\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 384 train loss: 0.36308160424232483\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 385 train loss: 0.3631075322628021\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 386 train loss: 0.3583986461162567\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 387 train loss: 0.3597923517227173\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 388 train loss: 0.35819685459136963\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 389 train loss: 0.358994722366333\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 390 train loss: 0.35480138659477234\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 391 train loss: 0.3572786748409271\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 392 train loss: 0.35545530915260315\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 393 train loss: 0.3572714030742645\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 394 train loss: 0.3533251881599426\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 395 train loss: 0.3552281856536865\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 396 train loss: 0.35385891795158386\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 397 train loss: 0.3541634678840637\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 398 train loss: 0.35079675912857056\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 399 train loss: 0.35172104835510254\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 400 train loss: 0.3489125072956085\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 401 train loss: 0.3498614728450775\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 402 train loss: 0.34567540884017944\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 403 train loss: 0.347491592168808\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 404 train loss: 0.34581562876701355\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 405 train loss: 0.3484211564064026\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 406 train loss: 0.3481070399284363\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 407 train loss: 0.3491295576095581\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 408 train loss: 0.34561869502067566\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 409 train loss: 0.3469725549221039\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 410 train loss: 0.3428177237510681\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 411 train loss: 0.3451996445655823\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 412 train loss: 0.3422306478023529\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 413 train loss: 0.34467262029647827\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 414 train loss: 0.3430892825126648\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 415 train loss: 0.34403833746910095\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 416 train loss: 0.3394014239311218\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 417 train loss: 0.33978649973869324\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 418 train loss: 0.3341802954673767\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 419 train loss: 0.3358636498451233\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 420 train loss: 0.3323642909526825\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 421 train loss: 0.33501729369163513\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 422 train loss: 0.33439549803733826\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 423 train loss: 0.3350873291492462\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 424 train loss: 0.3331946134567261\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 425 train loss: 0.3344302475452423\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 426 train loss: 0.32988840341567993\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 427 train loss: 0.3309006989002228\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 428 train loss: 0.32851073145866394\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 429 train loss: 0.3304455876350403\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 430 train loss: 0.32756704092025757\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 431 train loss: 0.32952046394348145\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 432 train loss: 0.3267364799976349\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 433 train loss: 0.3274436891078949\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 434 train loss: 0.324712872505188\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 435 train loss: 0.326068252325058\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 436 train loss: 0.3218884766101837\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 437 train loss: 0.3239908218383789\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 438 train loss: 0.3224303424358368\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 439 train loss: 0.32453203201293945\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 440 train loss: 0.3233269453048706\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 441 train loss: 0.32513993978500366\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 442 train loss: 0.32191002368927\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 443 train loss: 0.32295742630958557\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 444 train loss: 0.3194893002510071\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 445 train loss: 0.3204711079597473\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 446 train loss: 0.31596168875694275\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 447 train loss: 0.31793153285980225\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 448 train loss: 0.3135946989059448\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 449 train loss: 0.31563064455986023\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 450 train loss: 0.31481754779815674\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 451 train loss: 0.31551194190979004\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 452 train loss: 0.31378838419914246\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 453 train loss: 0.31417569518089294\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 454 train loss: 0.31136107444763184\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 455 train loss: 0.31134793162345886\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 456 train loss: 0.3068387806415558\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 457 train loss: 0.3068968951702118\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 458 train loss: 0.3033166825771332\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 459 train loss: 0.3056221008300781\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 460 train loss: 0.3048134446144104\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 461 train loss: 0.3046121597290039\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 462 train loss: 0.3049347400665283\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 463 train loss: 0.3072843551635742\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 464 train loss: 0.3061564862728119\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 465 train loss: 0.3054472506046295\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 466 train loss: 0.30156177282333374\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 467 train loss: 0.3017500638961792\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 468 train loss: 0.2982370853424072\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 469 train loss: 0.297850638628006\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 470 train loss: 0.29388898611068726\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 471 train loss: 0.29662975668907166\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 472 train loss: 0.2942504584789276\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 473 train loss: 0.2942759394645691\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 474 train loss: 0.2934342920780182\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 475 train loss: 0.29355180263519287\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 476 train loss: 0.29593610763549805\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 477 train loss: 0.2967294454574585\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 478 train loss: 0.29419809579849243\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 479 train loss: 0.2929006814956665\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 480 train loss: 0.29154065251350403\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 481 train loss: 0.2922528088092804\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 482 train loss: 0.288312166929245\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 483 train loss: 0.2888530492782593\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 484 train loss: 0.2892134487628937\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 485 train loss: 0.2905086278915405\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 486 train loss: 0.28923746943473816\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 487 train loss: 0.28808873891830444\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 488 train loss: 0.28729960322380066\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 489 train loss: 0.28706398606300354\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 490 train loss: 0.28339704871177673\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 491 train loss: 0.2835334539413452\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 492 train loss: 0.2839721143245697\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 493 train loss: 0.28400951623916626\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 494 train loss: 0.28081849217414856\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 495 train loss: 0.2823086977005005\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 496 train loss: 0.2803637981414795\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 497 train loss: 0.28045567870140076\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 498 train loss: 0.2791968286037445\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 499 train loss: 0.28065893054008484\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 创建模型对象\n",
    "net = Net(params_init_func, num_inputs)\n",
    "net.train()\n",
    "\n",
    "# 创建optimizer对象\n",
    "\n",
    "# 设定学习率learning rate(lr)\n",
    "lr = 0.01\n",
    "\n",
    "# 随机梯度下降\n",
    "optim = SGD(net.parameters(),lr=lr)\n",
    "\n",
    "# 记录测试的cost和acc\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 500\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "# print(x)\n",
    "    \n",
    "for i in range(num_epochs):\n",
    "    j = 0\n",
    "    \n",
    "    # 前向传播\n",
    "    out = net(x)\n",
    "    \n",
    "    # 计算loss\n",
    "    ls = loss(out, y)\n",
    "    print(f\"epoch: {i} train loss:\", ls.item())\n",
    "    \n",
    "    # 反向传播\n",
    "    ls.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optim.step()\n",
    "    \n",
    "    # 清空梯度\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    \n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "\n",
    "        result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "#         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "#         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "        print(\"Acc：\", np.mean(result),'\\n')\n",
    "    \n",
    "    \n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
      " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0]\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
      " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Acc： 0.9043062200956937\n"
     ]
    }
   ],
   "source": [
    "# print(np.argmax(net(x).detach().numpy(),axis=1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(np.argmax(net(x).detach().numpy(), axis=1))\n",
    "\n",
    "    result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "    print(\"\\nAcc：\", np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集说明\n",
    "本项目提供了一个数据集(data.h5,用load_data()读取),其中包含：一组标记为cat(1)或non-cat(0)的图像训练集(209张图片,209个标签),一组标记为cat(1)或non-cat(0)的测试图像测试集(50张图片,50个标签). 每个图像形状都是（64, 64, 3），其中3代表3个通道（RGB）\n",
    "\n",
    "## 任务要求\n",
    "\n",
    "### 必选项：\n",
    "\n",
    "**任务0.** 完整阅读一遍本notebook, 填写里面所需的代码\n",
    "\n",
    "**任务1.** 尝试用本实验项目提供的测试集数据(变量名为test_x_orig, test_y)对上面训练好的模型进行测试\n",
    "\n",
    "**任务2.** 按照表格要求调整各项参数，然后进行实验，填写表格\n",
    "\n",
    "**任务3.** 尝试更换优化器，将随机梯度下降(SGD)换成Adam,然后将lr设置成0.0001, 使用训练集对模型进行训练和测试\n",
    "\n",
    "**任务4.** 尝试在神经网络的初始化(init)阶段用torch.nn.linear()定义self.linear，并且用self.linear替换所有的torch.matmul(X, self.W1) + self.b1 \\\n",
    "(使用SGD, lr = 0.001, 使用本项目提供的原版np.random.normal(),不需要变更里面的参数)\n",
    "\n",
    "**任务5.** 搭建一个新模型, 要求如下:\\\n",
    "5-1. 参考上面示例模型搭建步骤，尝试搭建一个4层的模型(4个linear层，其中包括输出层) \\\n",
    "5-2. 每一层的神经元个数为128/256/512/2 (最后为2,因为是二分类) \\\n",
    "5.3. 每一层linear的bias设置为True\n",
    "5-4. 前三层linear,每层输出接一个relu,即一层神经网络为: (linear->relu), 然后整个网络: (linear->relu) * 3 \\-> linear \\\n",
    "5-5. 在模型初始化时，使用xavier参数初始化方法对权重矩阵$W_i$进行初始化，偏置项$b_i$初始化置0 **(下标i，表示第i层)** \\\n",
    "5-6. 优化器使用SGD, lr = 0.001 \\\n",
    "5-7. 设置训练迭代epoch为200 \\\n",
    "5-8. 上述实验结束后,使用Adam作为优化器,lr不变,再进行一次实验,观察结果\n",
    "\n",
    "---\n",
    "\n",
    "### 加分项：\n",
    "\n",
    "**任务7/8/9为一个系列**\n",
    "\n",
    "**任务6.** 在图像数据输入神经网络训练之前，使用torchvision库对图像数据进行增强\n",
    "\n",
    "**任务7.** 将训练集中的209个数据分批(batch)输入训练,要求: 一个batch的大小为10(batch_size = 10), 并且回答为何需要将数据分批训练\n",
    "\n",
    "**任务8.** 使用torch内置的工具,构建一个数据读取器(dataloader)来读取数据\n",
    "\n",
    "**任务9.** 使用**任务8**中的DataLoader替换任务7中自定义的batch数据提取器 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务1:\n",
    "尝试用本实验项目提供的测试集数据对上面训练好的模型进行测试 \\\n",
    "(测试集的图像数据及标签的变量名分别为test_x_orig和test_y)\n",
    "\n",
    "**(将代码写在下方)** \\\n",
    "**参考代码:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0\n",
      " 1 1 0 0 1 1 0 1 0 1 1 1 0]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
      " 1 0 0 1 1 1 0 0 0 1 1 1 0]\n",
      "Acc： 0.8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_x_orig = torch.tensor(test_x_orig / 255).float()\n",
    "\n",
    "y_test_true =torch.tensor(np.eye(2)[test_y.reshape(-1)]).float()\n",
    "\n",
    "# print(x_test_pred)\n",
    "\n",
    "# 测试部分\n",
    "with torch.no_grad():\n",
    "    print(\"开始使用测试集测试\")\n",
    "    x_test_pred = net(test_x_orig)\n",
    "    result = (np.argmax(x_test_pred.detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "    \n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(x_test_pred.detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y_test_true.numpy(),axis=1))\n",
    "    print(\"Acc：\", np.mean(result),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务2: \n",
    "按照表格要求调整各项参数，然后进行实验，将实验结果填写至表格 \\\n",
    "**(实验完成后需要提交表格)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了代码复用和方便将下列训练神经网络的代码打包到一个函数里面,命名为train_epochs()\n",
    "\n",
    "\n",
    "```python\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # 前向传播\n",
    "    out = net(x)\n",
    "\n",
    "    # 计算loss\n",
    "    ls = loss(out, y)\n",
    "    print(f\"epoch: {i} train loss:\", ls.item())\n",
    "\n",
    "    # 反向传播\n",
    "    ls.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    optim.step()\n",
    "\n",
    "    # 清空梯度\n",
    "    optim.zero_grad()\n",
    "\n",
    "\n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "#         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "#         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "        print(\"Acc：\", np.mean(result),'\\n')\n",
    "\n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, num_epochs, loss_fn, optimizer, train_x, train_y):\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # 前向传播\n",
    "        out = net(x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, y)\n",
    "        print(f\"epoch: {i} train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "        # 测试部分\n",
    "        with torch.no_grad():\n",
    "            print(\"开始测试\")\n",
    "    #         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "            result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    #         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    #         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "            print(\"Acc：\", np.mean(result),'\\n')\n",
    "\n",
    "        # record the cost every 10 training epoch\n",
    "        if i % 10 == 0:\n",
    "            costs.append(ls.item())\n",
    "            accs.append(np.mean(result))\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.plot(np.squeeze(accs))\n",
    "    plt.ylabel('cost/acc')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(lr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务3: \n",
    "尝试更换优化器，将SGD换成Adam,然后使用将lr设置成0.0001,然后使用训练集和测试集对模型进行训练和测试 \\\n",
    "**(将优化器定义在下方,并再次在下方编写模型训练的代码,然后训练,保留训练过程中输出的结果)**  \\\n",
    "**参考代码:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 7.557444095611572\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 1 train loss: 97.1741943359375\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 2 train loss: 55.1905517578125\n",
      "开始测试\n",
      "Acc： 0.41148325358851673 \n",
      "\n",
      "epoch: 3 train loss: 3.7451276779174805\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 4 train loss: 21.66668701171875\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 5 train loss: 29.067663192749023\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 6 train loss: 29.26552963256836\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 7 train loss: 26.08334732055664\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 21.476099014282227\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 9 train loss: 15.989295959472656\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 10 train loss: 10.012821197509766\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 11 train loss: 4.213324546813965\n",
      "开始测试\n",
      "Acc： 0.4688995215311005 \n",
      "\n",
      "epoch: 12 train loss: 1.5533479452133179\n",
      "开始测试\n",
      "Acc： 0.3827751196172249 \n",
      "\n",
      "epoch: 13 train loss: 5.2504682540893555\n",
      "开始测试\n",
      "Acc： 0.3684210526315789 \n",
      "\n",
      "epoch: 14 train loss: 6.420332908630371\n",
      "开始测试\n",
      "Acc： 0.37799043062200954 \n",
      "\n",
      "epoch: 15 train loss: 5.653313159942627\n",
      "开始测试\n",
      "Acc： 0.3875598086124402 \n",
      "\n",
      "epoch: 16 train loss: 3.640965461730957\n",
      "开始测试\n",
      "Acc： 0.4688995215311005 \n",
      "\n",
      "epoch: 17 train loss: 1.2577705383300781\n",
      "开始测试\n",
      "Acc： 0.645933014354067 \n",
      "\n",
      "epoch: 18 train loss: 1.5650159120559692\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 19 train loss: 2.6734602451324463\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 20 train loss: 3.275329351425171\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 21 train loss: 3.381920099258423\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 22 train loss: 3.106299877166748\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 23 train loss: 2.536993980407715\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 24 train loss: 1.80387282371521\n",
      "开始测试\n",
      "Acc： 0.631578947368421 \n",
      "\n",
      "epoch: 25 train loss: 1.1187127828598022\n",
      "开始测试\n",
      "Acc： 0.5645933014354066 \n",
      "\n",
      "epoch: 26 train loss: 0.9097498655319214\n",
      "开始测试\n",
      "Acc： 0.4354066985645933 \n",
      "\n",
      "epoch: 27 train loss: 1.46286940574646\n",
      "开始测试\n",
      "Acc： 0.4449760765550239 \n",
      "\n",
      "epoch: 28 train loss: 1.829315423965454\n",
      "开始测试\n",
      "Acc： 0.45933014354066987 \n",
      "\n",
      "epoch: 29 train loss: 1.540104627609253\n",
      "开始测试\n",
      "Acc： 0.569377990430622 \n",
      "\n",
      "epoch: 30 train loss: 0.8758993148803711\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 31 train loss: 1.028584599494934\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 32 train loss: 1.1535793542861938\n",
      "开始测试\n",
      "Acc： 0.6411483253588517 \n",
      "\n",
      "epoch: 33 train loss: 1.01334547996521\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 34 train loss: 0.8150902986526489\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 35 train loss: 0.7354522943496704\n",
      "开始测试\n",
      "Acc： 0.6028708133971292 \n",
      "\n",
      "epoch: 36 train loss: 0.7565736770629883\n",
      "开始测试\n",
      "Acc： 0.5933014354066986 \n",
      "\n",
      "epoch: 37 train loss: 0.783082127571106\n",
      "开始测试\n",
      "Acc： 0.6124401913875598 \n",
      "\n",
      "epoch: 38 train loss: 0.7723066210746765\n",
      "开始测试\n",
      "Acc： 0.631578947368421 \n",
      "\n",
      "epoch: 39 train loss: 0.7302977442741394\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 40 train loss: 0.7131356596946716\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 41 train loss: 0.7259891629219055\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 42 train loss: 0.7262193560600281\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 43 train loss: 0.699550449848175\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 44 train loss: 0.6680872440338135\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 45 train loss: 0.6581116318702698\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 46 train loss: 0.6602397561073303\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 47 train loss: 0.6631717085838318\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 48 train loss: 0.6555656790733337\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 49 train loss: 0.6395368576049805\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 50 train loss: 0.6364827156066895\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 51 train loss: 0.6367799043655396\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 52 train loss: 0.6255323886871338\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 53 train loss: 0.6116541028022766\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 54 train loss: 0.603804886341095\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 55 train loss: 0.5965060591697693\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 56 train loss: 0.5873354077339172\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 57 train loss: 0.5746402740478516\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 58 train loss: 0.5605233311653137\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 59 train loss: 0.5524008870124817\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 60 train loss: 0.5520749688148499\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 61 train loss: 0.5474782586097717\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 62 train loss: 0.5344376564025879\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 63 train loss: 0.5247921347618103\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 64 train loss: 0.5203551054000854\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 65 train loss: 0.5147494673728943\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 66 train loss: 0.5077885985374451\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 67 train loss: 0.500401496887207\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 68 train loss: 0.49687984585762024\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 69 train loss: 0.4895200729370117\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 70 train loss: 0.4800412952899933\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 71 train loss: 0.4718332886695862\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 72 train loss: 0.4657844603061676\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 73 train loss: 0.46225690841674805\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 74 train loss: 0.4558960795402527\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 75 train loss: 0.4465378522872925\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 76 train loss: 0.4411218762397766\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 77 train loss: 0.43716996908187866\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 78 train loss: 0.4311593472957611\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 79 train loss: 0.422053724527359\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 80 train loss: 0.416994571685791\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 81 train loss: 0.40846604108810425\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 82 train loss: 0.40380555391311646\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 83 train loss: 0.396950364112854\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 84 train loss: 0.3921775221824646\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 85 train loss: 0.3876647651195526\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 86 train loss: 0.3857560157775879\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 87 train loss: 0.37912094593048096\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 88 train loss: 0.37272074818611145\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 89 train loss: 0.3703736662864685\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 90 train loss: 0.36331871151924133\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 91 train loss: 0.35934188961982727\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 92 train loss: 0.3536025881767273\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 93 train loss: 0.34993401169776917\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 94 train loss: 0.34635308384895325\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 95 train loss: 0.3415851593017578\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 96 train loss: 0.34248921275138855\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 97 train loss: 0.3334725797176361\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 98 train loss: 0.33054840564727783\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 99 train loss: 0.32537198066711426\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 100 train loss: 0.3236103057861328\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 101 train loss: 0.31750860810279846\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 102 train loss: 0.31604430079460144\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 103 train loss: 0.30988654494285583\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 104 train loss: 0.30725646018981934\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 105 train loss: 0.3033061623573303\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 106 train loss: 0.3000708520412445\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 107 train loss: 0.2966795563697815\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 108 train loss: 0.2930021584033966\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 109 train loss: 0.28945106267929077\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 110 train loss: 0.2861779034137726\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 111 train loss: 0.2830316722393036\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 112 train loss: 0.2800911068916321\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 113 train loss: 0.277252733707428\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 114 train loss: 0.27421489357948303\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 115 train loss: 0.27361860871315\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 116 train loss: 0.2691889703273773\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 117 train loss: 0.2682201862335205\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 118 train loss: 0.26380452513694763\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 119 train loss: 0.26602694392204285\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 120 train loss: 0.26807186007499695\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 121 train loss: 0.2711888253688812\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 122 train loss: 0.2629372179508209\n",
      "开始测试\n",
      "Acc： 0.9138755980861244 \n",
      "\n",
      "epoch: 123 train loss: 0.2529916763305664\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 124 train loss: 0.2734868824481964\n",
      "开始测试\n",
      "Acc： 0.9138755980861244 \n",
      "\n",
      "epoch: 125 train loss: 0.26004666090011597\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 126 train loss: 0.2790520191192627\n",
      "开始测试\n",
      "Acc： 0.9138755980861244 \n",
      "\n",
      "epoch: 127 train loss: 0.2702406048774719\n",
      "开始测试\n",
      "Acc： 0.9138755980861244 \n",
      "\n",
      "epoch: 128 train loss: 0.25365421175956726\n",
      "开始测试\n",
      "Acc： 0.9282296650717703 \n",
      "\n",
      "epoch: 129 train loss: 0.2445903718471527\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 130 train loss: 0.2654374837875366\n",
      "开始测试\n",
      "Acc： 0.9234449760765551 \n",
      "\n",
      "epoch: 131 train loss: 0.24862992763519287\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 132 train loss: 0.2696855366230011\n",
      "开始测试\n",
      "Acc： 0.9090909090909091 \n",
      "\n",
      "epoch: 133 train loss: 0.2671089768409729\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 134 train loss: 0.24725405871868134\n",
      "开始测试\n",
      "Acc： 0.9234449760765551 \n",
      "\n",
      "epoch: 135 train loss: 0.23972360789775848\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 136 train loss: 0.2516420781612396\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 137 train loss: 0.23018890619277954\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 138 train loss: 0.23839615285396576\n",
      "开始测试\n",
      "Acc： 0.9282296650717703 \n",
      "\n",
      "epoch: 139 train loss: 0.24391412734985352\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 140 train loss: 0.23817743360996246\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 141 train loss: 0.22653187811374664\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 142 train loss: 0.21974702179431915\n",
      "开始测试\n",
      "Acc： 0.9234449760765551 \n",
      "\n",
      "epoch: 143 train loss: 0.23710913956165314\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 144 train loss: 0.21832460165023804\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 145 train loss: 0.2237376868724823\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 146 train loss: 0.23126929998397827\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 147 train loss: 0.22721418738365173\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 148 train loss: 0.21643365919589996\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 149 train loss: 0.2107270359992981\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 150 train loss: 0.208838552236557\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 151 train loss: 0.2175171822309494\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 152 train loss: 0.2044527530670166\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 153 train loss: 0.2129097729921341\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 154 train loss: 0.21755927801132202\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 155 train loss: 0.21115176379680634\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 156 train loss: 0.20463408529758453\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 157 train loss: 0.20106495916843414\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 158 train loss: 0.1943918913602829\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 159 train loss: 0.20523856580257416\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 160 train loss: 0.19234181940555573\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 161 train loss: 0.1993011087179184\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 162 train loss: 0.19944308698177338\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 163 train loss: 0.19494526088237762\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 164 train loss: 0.19271758198738098\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 165 train loss: 0.18913425505161285\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 166 train loss: 0.1817186027765274\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 167 train loss: 0.18503594398498535\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 168 train loss: 0.17874248325824738\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 169 train loss: 0.17899800837039948\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 170 train loss: 0.18014168739318848\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 171 train loss: 0.17657557129859924\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 172 train loss: 0.1712377518415451\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 173 train loss: 0.1715400218963623\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 174 train loss: 0.1686536818742752\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 175 train loss: 0.16833852231502533\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 176 train loss: 0.16695179045200348\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 177 train loss: 0.16582845151424408\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 178 train loss: 0.16354037821292877\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 179 train loss: 0.1598920375108719\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 180 train loss: 0.16213861107826233\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 181 train loss: 0.16117946803569794\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 182 train loss: 0.1665671169757843\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 183 train loss: 0.16770018637180328\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 184 train loss: 0.162955641746521\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 185 train loss: 0.1567566990852356\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 186 train loss: 0.15085111558437347\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 187 train loss: 0.16991585493087769\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 188 train loss: 0.1536894291639328\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 189 train loss: 0.1651402860879898\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 190 train loss: 0.1692265421152115\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 191 train loss: 0.16314978897571564\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 192 train loss: 0.15490995347499847\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 193 train loss: 0.14828354120254517\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 194 train loss: 0.15061894059181213\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 195 train loss: 0.14444521069526672\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 196 train loss: 0.1436719000339508\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 197 train loss: 0.14486968517303467\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 198 train loss: 0.14251947402954102\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 199 train loss: 0.1372949630022049\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 200 train loss: 0.13963912427425385\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 201 train loss: 0.13607685267925262\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 202 train loss: 0.13873855769634247\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 203 train loss: 0.13799402117729187\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 204 train loss: 0.13520096242427826\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 205 train loss: 0.13102954626083374\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 206 train loss: 0.13577678799629211\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 207 train loss: 0.12898783385753632\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 208 train loss: 0.13142965734004974\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 209 train loss: 0.13093410432338715\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 210 train loss: 0.12687599658966064\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 211 train loss: 0.12433275580406189\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 212 train loss: 0.1291402280330658\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 213 train loss: 0.12262146174907684\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 214 train loss: 0.12464788556098938\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 215 train loss: 0.12477751821279526\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 216 train loss: 0.12207160890102386\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 217 train loss: 0.11900448799133301\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 218 train loss: 0.12153203785419464\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 219 train loss: 0.11699479073286057\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 220 train loss: 0.11702662706375122\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 221 train loss: 0.11682185530662537\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 222 train loss: 0.11567618697881699\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 223 train loss: 0.1140088438987732\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 224 train loss: 0.11322727799415588\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 225 train loss: 0.11287107318639755\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 226 train loss: 0.11174660921096802\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 227 train loss: 0.11157141625881195\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 228 train loss: 0.11098939925432205\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 229 train loss: 0.10994180291891098\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 230 train loss: 0.10879241675138474\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 231 train loss: 0.10869832336902618\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 232 train loss: 0.10753673315048218\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 233 train loss: 0.10685911774635315\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 234 train loss: 0.10647441446781158\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 235 train loss: 0.10584893077611923\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 236 train loss: 0.10500530153512955\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 237 train loss: 0.10419740527868271\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 238 train loss: 0.10399563610553741\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 239 train loss: 0.10311379283666611\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 240 train loss: 0.10282237827777863\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 241 train loss: 0.10231787711381912\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 242 train loss: 0.10160203278064728\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 243 train loss: 0.10077954083681107\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 244 train loss: 0.10028518736362457\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 245 train loss: 0.09979552030563354\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 246 train loss: 0.09897544980049133\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 247 train loss: 0.09865337610244751\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 248 train loss: 0.09818500280380249\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 249 train loss: 0.09758968651294708\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 250 train loss: 0.09683816879987717\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 251 train loss: 0.09665953367948532\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 252 train loss: 0.09579828381538391\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 253 train loss: 0.09545496851205826\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 254 train loss: 0.09493912756443024\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 255 train loss: 0.09422429651021957\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 256 train loss: 0.09381385147571564\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 257 train loss: 0.09331844002008438\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 258 train loss: 0.09274321794509888\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 259 train loss: 0.09235824644565582\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 260 train loss: 0.09179531782865524\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 261 train loss: 0.09113790839910507\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 262 train loss: 0.09093878418207169\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 263 train loss: 0.09017575532197952\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 264 train loss: 0.0898391529917717\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 265 train loss: 0.0894404873251915\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 266 train loss: 0.08886419981718063\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 267 train loss: 0.08829572051763535\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 268 train loss: 0.08781900256872177\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 269 train loss: 0.08737102150917053\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 270 train loss: 0.08688335120677948\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 271 train loss: 0.08639416098594666\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 272 train loss: 0.08592837303876877\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 273 train loss: 0.08557257801294327\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 274 train loss: 0.08515873551368713\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 275 train loss: 0.08499607443809509\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 276 train loss: 0.08457796275615692\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 277 train loss: 0.08409754186868668\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 278 train loss: 0.08349407464265823\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 279 train loss: 0.08284620195627213\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 280 train loss: 0.08306965976953506\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 281 train loss: 0.08235888183116913\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 282 train loss: 0.08238988369703293\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 283 train loss: 0.08232381939888\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 284 train loss: 0.08188080042600632\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 285 train loss: 0.08111909031867981\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 286 train loss: 0.08028177917003632\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 287 train loss: 0.07949739694595337\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 288 train loss: 0.08109826594591141\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 289 train loss: 0.07915244251489639\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 290 train loss: 0.07962616533041\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 291 train loss: 0.08011842519044876\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 292 train loss: 0.0797138661146164\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 293 train loss: 0.07875514775514603\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 294 train loss: 0.07751753181219101\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 295 train loss: 0.07661069184541702\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 296 train loss: 0.07712580263614655\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 297 train loss: 0.07563271373510361\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 298 train loss: 0.07546262443065643\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 299 train loss: 0.07525868713855743\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 300 train loss: 0.07479291409254074\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 301 train loss: 0.07445206493139267\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 302 train loss: 0.07385093718767166\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 303 train loss: 0.0733918771147728\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 304 train loss: 0.07339822500944138\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 305 train loss: 0.07279284298419952\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 306 train loss: 0.0722038745880127\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 307 train loss: 0.07190456986427307\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 308 train loss: 0.07182007282972336\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 309 train loss: 0.07118603587150574\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 310 train loss: 0.07075772434473038\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 311 train loss: 0.07019132375717163\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 312 train loss: 0.06969430297613144\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 313 train loss: 0.06981503218412399\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 314 train loss: 0.06908861547708511\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 315 train loss: 0.06905166804790497\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 316 train loss: 0.06891454011201859\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 317 train loss: 0.06866094470024109\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 318 train loss: 0.06819233298301697\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 319 train loss: 0.06754712760448456\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 320 train loss: 0.06680186837911606\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 321 train loss: 0.06640131771564484\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 322 train loss: 0.06666436046361923\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 323 train loss: 0.0656658411026001\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 324 train loss: 0.06558868288993835\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 325 train loss: 0.06548391282558441\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 326 train loss: 0.06520002335309982\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 327 train loss: 0.06474467366933823\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 328 train loss: 0.06417746841907501\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 329 train loss: 0.06353576481342316\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 330 train loss: 0.0630640834569931\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 331 train loss: 0.06389207392930984\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 332 train loss: 0.06265895813703537\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 333 train loss: 0.06305734068155289\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 334 train loss: 0.06349852681159973\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 335 train loss: 0.06340417265892029\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 336 train loss: 0.062673419713974\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 337 train loss: 0.061695318669080734\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 338 train loss: 0.06083960831165314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 339 train loss: 0.060183875262737274\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 340 train loss: 0.061864130198955536\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 341 train loss: 0.059610795229673386\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 342 train loss: 0.06027717515826225\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 343 train loss: 0.061048224568367004\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 344 train loss: 0.06099346652626991\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 345 train loss: 0.0600479431450367\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 346 train loss: 0.05895192176103592\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 347 train loss: 0.0581391267478466\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 348 train loss: 0.057592958211898804\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 349 train loss: 0.05861448496580124\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 350 train loss: 0.056733403354883194\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 351 train loss: 0.05689682066440582\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 352 train loss: 0.05704700946807861\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 353 train loss: 0.057125963270664215\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 354 train loss: 0.05662136897444725\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 355 train loss: 0.055874790996313095\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 356 train loss: 0.05524870753288269\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 357 train loss: 0.054654479026794434\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 358 train loss: 0.05444524437189102\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 359 train loss: 0.05451326444745064\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 360 train loss: 0.05367803946137428\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 361 train loss: 0.0533277802169323\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 362 train loss: 0.05322591960430145\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 363 train loss: 0.05297866463661194\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 364 train loss: 0.05285220965743065\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 365 train loss: 0.05246882513165474\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 366 train loss: 0.05194898694753647\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 367 train loss: 0.051598645746707916\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 368 train loss: 0.05143400654196739\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 369 train loss: 0.051283467561006546\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 370 train loss: 0.05082616209983826\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 371 train loss: 0.050351135432720184\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 372 train loss: 0.050197526812553406\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 373 train loss: 0.050026848912239075\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 374 train loss: 0.049708910286426544\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 375 train loss: 0.04933539405465126\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 376 train loss: 0.048974018543958664\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 377 train loss: 0.04893089458346367\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 378 train loss: 0.04808032512664795\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 379 train loss: 0.04815852642059326\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 380 train loss: 0.047841694205999374\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 381 train loss: 0.04700443521142006\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 382 train loss: 0.04704280570149422\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 383 train loss: 0.04664280265569687\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 384 train loss: 0.04594366252422333\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 385 train loss: 0.04596984386444092\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 386 train loss: 0.04554633051156998\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 387 train loss: 0.044870540499687195\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 388 train loss: 0.044683218002319336\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 389 train loss: 0.044281456619501114\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 390 train loss: 0.04391522705554962\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 391 train loss: 0.04368133097887039\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 392 train loss: 0.043366141617298126\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 393 train loss: 0.04291932284832001\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 394 train loss: 0.04268217459321022\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 395 train loss: 0.042388588190078735\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 396 train loss: 0.04203185439109802\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 397 train loss: 0.04178899899125099\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 398 train loss: 0.04149767383933067\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 399 train loss: 0.0411895215511322\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 400 train loss: 0.04098733514547348\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 401 train loss: 0.040667638182640076\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 402 train loss: 0.0403912253677845\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 403 train loss: 0.040170591324567795\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 404 train loss: 0.039956506341695786\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 405 train loss: 0.03967208415269852\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 406 train loss: 0.03941616415977478\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 407 train loss: 0.03927522525191307\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 408 train loss: 0.03896327316761017\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 409 train loss: 0.038687657564878464\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 410 train loss: 0.038459327071905136\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 411 train loss: 0.038356881588697433\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 412 train loss: 0.03797994181513786\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 413 train loss: 0.03791491687297821\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 414 train loss: 0.037628866732120514\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 415 train loss: 0.037504974752664566\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 416 train loss: 0.0372498519718647\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 417 train loss: 0.03691950812935829\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 418 train loss: 0.03672690689563751\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 419 train loss: 0.036657076328992844\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 420 train loss: 0.03632529452443123\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 421 train loss: 0.036185137927532196\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 422 train loss: 0.03603661060333252\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 423 train loss: 0.03588796406984329\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 424 train loss: 0.03561197593808174\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 425 train loss: 0.03540583699941635\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 426 train loss: 0.03514432534575462\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 427 train loss: 0.03487768769264221\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 428 train loss: 0.03495832160115242\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 429 train loss: 0.034573834389448166\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 430 train loss: 0.03459325432777405\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 431 train loss: 0.0345512293279171\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 432 train loss: 0.034414034336805344\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 433 train loss: 0.03424045443534851\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 434 train loss: 0.03393924608826637\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 435 train loss: 0.033674418926239014\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 436 train loss: 0.03331545740365982\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 437 train loss: 0.03310537710785866\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 438 train loss: 0.033500973135232925\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 439 train loss: 0.03282583877444267\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 440 train loss: 0.03290141746401787\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 441 train loss: 0.0328676700592041\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 442 train loss: 0.03299059718847275\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 443 train loss: 0.032820336520671844\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 444 train loss: 0.03261122480034828\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 445 train loss: 0.032252129167318344\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 446 train loss: 0.03187333792448044\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 447 train loss: 0.03150440752506256\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 448 train loss: 0.031962838023900986\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 449 train loss: 0.031238708645105362\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 450 train loss: 0.03128551319241524\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 451 train loss: 0.03128118813037872\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 452 train loss: 0.031127804890275\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 453 train loss: 0.03083105944097042\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 454 train loss: 0.03081604465842247\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 455 train loss: 0.030412768945097923\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 456 train loss: 0.03022821620106697\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 457 train loss: 0.03011184372007847\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 458 train loss: 0.03005562163889408\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 459 train loss: 0.029821135103702545\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 460 train loss: 0.029622146859765053\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 461 train loss: 0.029519174247980118\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 462 train loss: 0.02937624417245388\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 463 train loss: 0.029220998287200928\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 464 train loss: 0.029080556705594063\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 465 train loss: 0.028898967429995537\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 466 train loss: 0.02874942496418953\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 467 train loss: 0.02862522378563881\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 468 train loss: 0.028458531945943832\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 469 train loss: 0.02833307348191738\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 470 train loss: 0.028258422389626503\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 471 train loss: 0.02811119519174099\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 472 train loss: 0.02792280912399292\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 473 train loss: 0.02784137800335884\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 474 train loss: 0.02770439349114895\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 475 train loss: 0.02753337286412716\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 476 train loss: 0.027434924617409706\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 477 train loss: 0.02728496864438057\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 478 train loss: 0.02716350182890892\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 479 train loss: 0.027018297463655472\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 480 train loss: 0.026911213994026184\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 481 train loss: 0.02678249217569828\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 482 train loss: 0.026657093316316605\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 483 train loss: 0.02653488889336586\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 484 train loss: 0.026456540450453758\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 485 train loss: 0.02631325274705887\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 486 train loss: 0.02618747390806675\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 487 train loss: 0.026077240705490112\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 488 train loss: 0.025955254212021828\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 489 train loss: 0.025854621082544327\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 490 train loss: 0.02572580799460411\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 491 train loss: 0.02562420815229416\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 492 train loss: 0.025524187833070755\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 493 train loss: 0.025435253977775574\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 494 train loss: 0.025315554812550545\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 495 train loss: 0.025183996185660362\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 496 train loss: 0.025064632296562195\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 497 train loss: 0.024982931092381477\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 498 train loss: 0.02482982538640499\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 499 train loss: 0.02479257620871067\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# 定义模型\n",
    "net = Net(params_init_func, num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义optimizer, Adam!!!!\n",
    "lr = 0.0001\n",
    "optim = Adam(net.parameters(),lr=lr)\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 500\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "    \n",
    "train_epochs(net, num_epochs, loss, optim, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('epochs (per fifty)')\n",
    "\n",
    "#设置坐标轴刻度\n",
    "my_x_ticks = np.arange(0, 10, 1)\n",
    "plt.xticks(my_x_ticks)\n",
    "my_y_ticks = np.arange(0, 1, 0.05)\n",
    "plt.yticks(my_y_ticks)\n",
    "\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0\n",
      " 1 0 0 0 0 1 0 1 1 0 1 1 0]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
      " 1 0 0 1 1 1 0 0 0 1 1 1 0]\n",
      "分类正确与否: [ True False  True  True  True  True False  True  True  True False False\n",
      "  True False  True  True  True  True False False  True  True  True  True\n",
      "  True False  True  True False  True False  True  True  True False  True\n",
      "  True  True  True  True False False  True  True False False False  True\n",
      "  True  True]\n",
      "Acc： 0.68 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用测试集进行训练\n",
    "\n",
    "test_x_orig = torch.tensor(test_x_orig / 255).float()\n",
    "\n",
    "y_test_true =torch.tensor(np.eye(2)[test_y.reshape(-1)]).float()\n",
    "\n",
    "# print(x_test_pred)\n",
    "\n",
    "# 测试部分\n",
    "with torch.no_grad():\n",
    "    print(\"开始测试\")\n",
    "    x_test_pred = net(test_x_orig)\n",
    "    result = (np.argmax(x_test_pred.detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "    \n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(x_test_pred.detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y_test_true.numpy(),axis=1))\n",
    "    print(\"分类正确与否:\", result)\n",
    "    print(\"Acc：\", np.mean(result),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务4:  \n",
    "尝试在神经网络的初始化(init)阶段定义self.linear，并用self.linear替换所有的torch.matmul(X, self.W1) + self.b1 \\\n",
    "(使用SGD, lr = 0.001, 使用本项目提供的原版np.random.normal(),不需要变更里面的参数)\n",
    "\n",
    "#### torch.nn.Linear创建出的对象是一个线性变换层函数：\n",
    "$$Y = WX+b$$\n",
    "\n",
    "```python\n",
    "对象创建方法： linear = torch.nn.Linear(in_features, out_features, bias=True)\n",
    "对象调用方法： 输出tensor = linear(输入tensor)\n",
    "```\n",
    "\n",
    "#### 参数bias=False时，变成：\n",
    "$$Y = WX$$\n",
    "\n",
    "#### 实现Y = WX+b, 本项目初始提供的linear层构建/计算方案：\n",
    "\n",
    "init阶段：\n",
    "```python\n",
    "self.W1 = nn.Parameter(torch.tensor((num_inputs, num_hiddens), dtype=torch.float, requires_grad = True))\n",
    "self.b1 = nn.Parameter(torch.zeros(num_hiddens, dtype=torch.float, requires_grad = True))\n",
    "```     \n",
    "forward阶段：\n",
    "```python\n",
    "Y = torch.matmul(X, self.W1) + self.b1\n",
    "```\n",
    "\n",
    "#### 更加方便的linear层构建/使用方式：\n",
    "\n",
    "init阶段：\n",
    "```python\n",
    "self.linear = torch.nn.Linear(num_inputs, num_hiddens, bias=True)\n",
    "```\n",
    "forward阶段：\n",
    "```python\n",
    "Y = self.linear(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**(将重新构建的模型代码写在下方,并且用训练集进行训练和测试,保留训练过程中输出的结果)** \\\n",
    "**参考代码:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1): Linear(in_features=12288, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(num_inputs, num_hiddens, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(num_hiddens, num_outputs, bias=True)\n",
    "        relu=nn.ReLU()\n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "        # 权重初始化\n",
    "        self.linear1.weights = nn.Parameter(torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)),requires_grad = True).float())\n",
    "        self.linear2.weights = nn.Parameter(torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), requires_grad = True).float())\n",
    "        self.linear1.bias = nn.Parameter(torch.tensor(np.zeros((num_hiddens)),requires_grad = True).float())\n",
    "        self.linear2.bias = nn.Parameter(torch.tensor(np.zeros((num_outputs)), requires_grad = True).float())\n",
    "        \n",
    "    def forward(self, X):\n",
    "#         print(X.shape)\n",
    "        X = X.view((-1, num_inputs))\n",
    "        out = self.linear2(relu(self.linear1(X)))\n",
    "        return out\n",
    "    \n",
    "net = Net(num_inputs)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 13.79755973815918\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 1 train loss: 8156.19873046875\n",
      "开始测试\n",
      "Acc： 0.3397129186602871 \n",
      "\n",
      "epoch: 2 train loss: 253.79505920410156\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 3 train loss: 0.6685512661933899\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 4 train loss: 0.6681501865386963\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 5 train loss: 1.1238590478897095\n",
      "开始测试\n",
      "Acc： 0.631578947368421 \n",
      "\n",
      "epoch: 6 train loss: 0.6877303123474121\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 7 train loss: 0.6825872659683228\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 0.6789236664772034\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 9 train loss: 0.6671469807624817\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 10 train loss: 0.6377987861633301\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 11 train loss: 0.6414568424224854\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 12 train loss: 0.6785920262336731\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 13 train loss: 0.675071656703949\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 14 train loss: 0.663548469543457\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 15 train loss: 0.618235170841217\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 16 train loss: 0.6103633046150208\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 17 train loss: 0.6470087766647339\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 18 train loss: 0.6544075608253479\n",
      "开始测试\n",
      "Acc： 0.5645933014354066 \n",
      "\n",
      "epoch: 19 train loss: 0.6911630034446716\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 20 train loss: 0.6768009662628174\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 21 train loss: 0.6747487783432007\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 22 train loss: 0.6727191805839539\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 23 train loss: 0.6665233373641968\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 24 train loss: 0.6536439657211304\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 25 train loss: 0.6468561887741089\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 26 train loss: 0.6434073448181152\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 27 train loss: 0.6554819345474243\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 28 train loss: 0.6479651927947998\n",
      "开始测试\n",
      "Acc： 0.6411483253588517 \n",
      "\n",
      "epoch: 29 train loss: 0.67896968126297\n",
      "开始测试\n",
      "Acc： 0.6507177033492823 \n",
      "\n",
      "epoch: 30 train loss: 0.6715949177742004\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 31 train loss: 0.6525066494941711\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 32 train loss: 0.6727626323699951\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 33 train loss: 0.6805171966552734\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 34 train loss: 0.67954421043396\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 35 train loss: 0.6780081391334534\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 36 train loss: 0.6761945486068726\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 37 train loss: 0.6740193367004395\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 38 train loss: 0.67067950963974\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 39 train loss: 0.6628773808479309\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 40 train loss: 0.6273364424705505\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 41 train loss: 0.7260968685150146\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 42 train loss: 0.6787072420120239\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 43 train loss: 0.6774691343307495\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 44 train loss: 0.6747549176216125\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 45 train loss: 0.6691903471946716\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 46 train loss: 0.6479067206382751\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 47 train loss: 0.6008363962173462\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 48 train loss: 0.5946930646896362\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 49 train loss: 0.65869140625\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 50 train loss: 0.6209304928779602\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 51 train loss: 0.8704201579093933\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 52 train loss: 0.681607186794281\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 53 train loss: 0.6815220713615417\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 54 train loss: 0.6814252734184265\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 55 train loss: 0.6813345551490784\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 56 train loss: 0.6812418699264526\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 57 train loss: 0.6811470985412598\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 58 train loss: 0.6810545325279236\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 59 train loss: 0.680967390537262\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 60 train loss: 0.6808789372444153\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 61 train loss: 0.6807889938354492\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 62 train loss: 0.680697500705719\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 63 train loss: 0.6805958151817322\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 64 train loss: 0.6778385639190674\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 65 train loss: 0.6805008053779602\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 66 train loss: 0.6804000735282898\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 67 train loss: 0.6802963018417358\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 68 train loss: 0.6801906824111938\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 69 train loss: 0.6800920367240906\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 70 train loss: 0.679984986782074\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 71 train loss: 0.6798734664916992\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 72 train loss: 0.6771551370620728\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 73 train loss: 0.6733859777450562\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 74 train loss: 0.6715087890625\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 75 train loss: 0.6698927879333496\n",
      "开始测试\n",
      "Acc： 0.6267942583732058 \n",
      "\n",
      "epoch: 76 train loss: 0.7319287657737732\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 77 train loss: 0.6801242828369141\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 78 train loss: 0.6799654960632324\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 79 train loss: 0.6798098683357239\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 80 train loss: 0.6796718239784241\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 81 train loss: 0.6795105338096619\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 82 train loss: 0.6793597936630249\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 83 train loss: 0.6792281270027161\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 84 train loss: 0.6791225075721741\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 85 train loss: 0.6789172887802124\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 86 train loss: 0.6787718534469604\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 87 train loss: 0.678623616695404\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 88 train loss: 0.6785436868667603\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 89 train loss: 0.6783262491226196\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 90 train loss: 0.6782083511352539\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 91 train loss: 0.6780654788017273\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 92 train loss: 0.6778628826141357\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 93 train loss: 0.6763392686843872\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 94 train loss: 0.6762173771858215\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 95 train loss: 0.6762281060218811\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 96 train loss: 0.6750457882881165\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 97 train loss: 0.6744378805160522\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 98 train loss: 0.6743728518486023\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 99 train loss: 0.6733866333961487\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 100 train loss: 0.6730583906173706\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 101 train loss: 0.6720986366271973\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 102 train loss: 0.6710468530654907\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 103 train loss: 0.6714545488357544\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 104 train loss: 0.6692261099815369\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 105 train loss: 0.6690969467163086\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 106 train loss: 0.6705136895179749\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 107 train loss: 0.6661117076873779\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 108 train loss: 0.6669119000434875\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 109 train loss: 0.6635255813598633\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 110 train loss: 0.6641569137573242\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 111 train loss: 0.6625827550888062\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 112 train loss: 0.6648836731910706\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 113 train loss: 0.657884955406189\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 114 train loss: 0.6655442118644714\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 115 train loss: 0.6538621187210083\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 116 train loss: 0.6505889296531677\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 117 train loss: 0.6505133509635925\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 118 train loss: 0.6747639775276184\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 119 train loss: 0.676376223564148\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 120 train loss: 0.6762404441833496\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 121 train loss: 0.6760984063148499\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 122 train loss: 0.6759763360023499\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 123 train loss: 0.675868034362793\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 124 train loss: 0.6757705211639404\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 125 train loss: 0.6756805777549744\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 126 train loss: 0.6755967736244202\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 127 train loss: 0.675517737865448\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 128 train loss: 0.6754425168037415\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 129 train loss: 0.6753700375556946\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 130 train loss: 0.6753004193305969\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 131 train loss: 0.675233006477356\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 132 train loss: 0.6751675605773926\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 133 train loss: 0.6751037836074829\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 134 train loss: 0.675041675567627\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 135 train loss: 0.6750618815422058\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 136 train loss: 0.6750501394271851\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 137 train loss: 0.6749823689460754\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 138 train loss: 0.6749137043952942\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 139 train loss: 0.6748472452163696\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 140 train loss: 0.6747828722000122\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 141 train loss: 0.6747201681137085\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 142 train loss: 0.6746588349342346\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 143 train loss: 0.674598753452301\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 144 train loss: 0.6745396852493286\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 145 train loss: 0.6744812726974487\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 146 train loss: 0.6744028329849243\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 147 train loss: 0.674413800239563\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 148 train loss: 0.6744728684425354\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 149 train loss: 0.6743637323379517\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 150 train loss: 0.6742829084396362\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 151 train loss: 0.6742135286331177\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 152 train loss: 0.6741504073143005\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 153 train loss: 0.6740906834602356\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 154 train loss: 0.674033522605896\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 155 train loss: 0.6739779114723206\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 156 train loss: 0.6739236116409302\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 157 train loss: 0.673870325088501\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 158 train loss: 0.6738178133964539\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 159 train loss: 0.6737658977508545\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 160 train loss: 0.6737145781517029\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 161 train loss: 0.6736636161804199\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 162 train loss: 0.6736131310462952\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 163 train loss: 0.6735629439353943\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 164 train loss: 0.6735131740570068\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 165 train loss: 0.6734635829925537\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 166 train loss: 0.6734142303466797\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 167 train loss: 0.6733651757240295\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 168 train loss: 0.6733164191246033\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 169 train loss: 0.6732677221298218\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 170 train loss: 0.6732192039489746\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 171 train loss: 0.6731709837913513\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 172 train loss: 0.6731227040290833\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 173 train loss: 0.6730748414993286\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 174 train loss: 0.6730268597602844\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 175 train loss: 0.6729792356491089\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 176 train loss: 0.6729333996772766\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 177 train loss: 0.6728870272636414\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 178 train loss: 0.6728397607803345\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 179 train loss: 0.6727925539016724\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 180 train loss: 0.6727455258369446\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 181 train loss: 0.6726985573768616\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 182 train loss: 0.6726518273353577\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 183 train loss: 0.6726049184799194\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 184 train loss: 0.6725583672523499\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 185 train loss: 0.6725119352340698\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 186 train loss: 0.6724653244018555\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 187 train loss: 0.6724191904067993\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 188 train loss: 0.6723729968070984\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 189 train loss: 0.6723268628120422\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 190 train loss: 0.6722807884216309\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 191 train loss: 0.6722349524497986\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 192 train loss: 0.6721891760826111\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 193 train loss: 0.672143280506134\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 194 train loss: 0.6720976829528809\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 195 train loss: 0.6720521450042725\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 196 train loss: 0.6720066070556641\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 197 train loss: 0.67196124792099\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 198 train loss: 0.6719158887863159\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 199 train loss: 0.6718707084655762\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 200 train loss: 0.6718106269836426\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 201 train loss: 0.660136878490448\n",
      "开始测试\n",
      "Acc： 0.430622009569378 \n",
      "\n",
      "epoch: 202 train loss: 2.43394136428833\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 203 train loss: 0.6745157241821289\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 204 train loss: 0.6752338409423828\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 205 train loss: 0.6753323674201965\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 206 train loss: 0.6752529740333557\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 207 train loss: 0.675179660320282\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 208 train loss: 0.6751106381416321\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 209 train loss: 0.6750455498695374\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 210 train loss: 0.6749058961868286\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 211 train loss: 0.6737627387046814\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 212 train loss: 0.6730665564537048\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 213 train loss: 0.6726096272468567\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 214 train loss: 0.6722873449325562\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 215 train loss: 0.6723769307136536\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 216 train loss: 0.6722437143325806\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 217 train loss: 0.6719908714294434\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 218 train loss: 0.6717937588691711\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 219 train loss: 0.6716800928115845\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 220 train loss: 0.6717774868011475\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 221 train loss: 0.6716010570526123\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 222 train loss: 0.6714549660682678\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 223 train loss: 0.6713300347328186\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 224 train loss: 0.6713627576828003\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 225 train loss: 0.6709579229354858\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 226 train loss: 0.6702709197998047\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 227 train loss: 0.6694653034210205\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 228 train loss: 0.6689632534980774\n",
      "开始测试\n",
      "Acc： 0.6124401913875598 \n",
      "\n",
      "epoch: 229 train loss: 0.6946411728858948\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 230 train loss: 0.6787821650505066\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 231 train loss: 0.6786636114120483\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 232 train loss: 0.6785541772842407\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 233 train loss: 0.6784411072731018\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 234 train loss: 0.6783345937728882\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 235 train loss: 0.6782335638999939\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 236 train loss: 0.6781373620033264\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 237 train loss: 0.6780458092689514\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 238 train loss: 0.6779581904411316\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 239 train loss: 0.6778743267059326\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 240 train loss: 0.6777935028076172\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 241 train loss: 0.6777277588844299\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 242 train loss: 0.6776527762413025\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 243 train loss: 0.6775794625282288\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 244 train loss: 0.6775083541870117\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 245 train loss: 0.6774393916130066\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 246 train loss: 0.677372395992279\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 247 train loss: 0.67730712890625\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 248 train loss: 0.677243709564209\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 249 train loss: 0.6771815419197083\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 250 train loss: 0.6771207451820374\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 251 train loss: 0.6770613193511963\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 252 train loss: 0.677003026008606\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 253 train loss: 0.6769459843635559\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 254 train loss: 0.6768895983695984\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 255 train loss: 0.6768369674682617\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 256 train loss: 0.6767882704734802\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 257 train loss: 0.6767345666885376\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 258 train loss: 0.676681399345398\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 259 train loss: 0.6766288876533508\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 260 train loss: 0.6765772700309753\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 261 train loss: 0.6765263080596924\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 262 train loss: 0.6764758825302124\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 263 train loss: 0.676426112651825\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 264 train loss: 0.6763767600059509\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 265 train loss: 0.676328182220459\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 266 train loss: 0.6762799024581909\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 267 train loss: 0.676232099533081\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 268 train loss: 0.6761847138404846\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 269 train loss: 0.6761379241943359\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 270 train loss: 0.6760914921760559\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 271 train loss: 0.6760452389717102\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 272 train loss: 0.6759994626045227\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 273 train loss: 0.6759539842605591\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 274 train loss: 0.6759088635444641\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 275 train loss: 0.6758639812469482\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 276 train loss: 0.6758195757865906\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 277 train loss: 0.6757752895355225\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 278 train loss: 0.6757313013076782\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 279 train loss: 0.675690233707428\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 280 train loss: 0.6756494641304016\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 281 train loss: 0.6756059527397156\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 282 train loss: 0.6755629777908325\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 283 train loss: 0.6755199432373047\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 284 train loss: 0.6754773259162903\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 285 train loss: 0.6754347681999207\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 286 train loss: 0.6753925681114197\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 287 train loss: 0.6753503680229187\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 288 train loss: 0.6753084659576416\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 289 train loss: 0.6752667427062988\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 290 train loss: 0.6752251982688904\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 291 train loss: 0.675183892250061\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 292 train loss: 0.6751426458358765\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 293 train loss: 0.6751015186309814\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 294 train loss: 0.6750606298446655\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 295 train loss: 0.6750199198722839\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 296 train loss: 0.6749793291091919\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 297 train loss: 0.6749388575553894\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 298 train loss: 0.674898624420166\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 299 train loss: 0.6748584508895874\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 300 train loss: 0.6748183965682983\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 301 train loss: 0.674778401851654\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 302 train loss: 0.6747385859489441\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 303 train loss: 0.6746989488601685\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 304 train loss: 0.6746593117713928\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 305 train loss: 0.674619734287262\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 306 train loss: 0.674580454826355\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 307 train loss: 0.674541175365448\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 308 train loss: 0.6745020747184753\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 309 train loss: 0.6744630932807922\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 310 train loss: 0.6744241714477539\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 311 train loss: 0.6743853092193604\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 312 train loss: 0.6743466258049011\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 313 train loss: 0.6743079423904419\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 314 train loss: 0.674269437789917\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 315 train loss: 0.6742309331893921\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 316 train loss: 0.674192488193512\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 317 train loss: 0.6741541624069214\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 318 train loss: 0.6741160750389099\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 319 train loss: 0.6740780472755432\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 320 train loss: 0.6740416288375854\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 321 train loss: 0.6740052700042725\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 322 train loss: 0.6739671230316162\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 323 train loss: 0.6739293932914734\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 324 train loss: 0.6738916635513306\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 325 train loss: 0.6738539934158325\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 326 train loss: 0.6738163232803345\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 327 train loss: 0.673778772354126\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 328 train loss: 0.673741340637207\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 329 train loss: 0.6737038493156433\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 330 train loss: 0.6736666560173035\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 331 train loss: 0.6736293435096741\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 332 train loss: 0.6735921502113342\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 333 train loss: 0.6735550165176392\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 334 train loss: 0.6735178232192993\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 335 train loss: 0.6734809875488281\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 336 train loss: 0.6734438538551331\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 337 train loss: 0.6734071373939514\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 338 train loss: 0.6733704805374146\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 339 train loss: 0.6733337044715881\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 340 train loss: 0.6732969880104065\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 341 train loss: 0.673260509967804\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 342 train loss: 0.6732238531112671\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 343 train loss: 0.6731873750686646\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 344 train loss: 0.6731508374214172\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 345 train loss: 0.6731145977973938\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 346 train loss: 0.6730782985687256\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 347 train loss: 0.6730420589447021\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 348 train loss: 0.6730058193206787\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 349 train loss: 0.6729696989059448\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 350 train loss: 0.6729336977005005\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 351 train loss: 0.6728975772857666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 352 train loss: 0.672861635684967\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 353 train loss: 0.6728256344795227\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 354 train loss: 0.6727898120880127\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 355 train loss: 0.6727539300918579\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 356 train loss: 0.672718346118927\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 357 train loss: 0.6726824641227722\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 358 train loss: 0.6726468205451965\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 359 train loss: 0.6726113557815552\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 360 train loss: 0.6725757122039795\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 361 train loss: 0.6725402474403381\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 362 train loss: 0.6725048422813416\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 363 train loss: 0.6724693775177002\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 364 train loss: 0.6724340915679932\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 365 train loss: 0.6723988652229309\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 366 train loss: 0.6723635196685791\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 367 train loss: 0.6723284721374512\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 368 train loss: 0.6722931861877441\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 369 train loss: 0.672258198261261\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 370 train loss: 0.6722230911254883\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 371 train loss: 0.6721879839897156\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 372 train loss: 0.6721529960632324\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 373 train loss: 0.6721181869506836\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 374 train loss: 0.67208331823349\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 375 train loss: 0.6720484495162964\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 376 train loss: 0.6720136404037476\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 377 train loss: 0.6719788908958435\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 378 train loss: 0.671944260597229\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 379 train loss: 0.6719096302986145\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 380 train loss: 0.6718750596046448\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 381 train loss: 0.671840488910675\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 382 train loss: 0.6718059182167053\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 383 train loss: 0.6717714071273804\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 384 train loss: 0.6717371344566345\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 385 train loss: 0.6717028021812439\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 386 train loss: 0.6716684699058533\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 387 train loss: 0.6716340780258179\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 388 train loss: 0.6715999245643616\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 389 train loss: 0.671565592288971\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 390 train loss: 0.6715314388275146\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 391 train loss: 0.6714973449707031\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 392 train loss: 0.6714632511138916\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 393 train loss: 0.6714292168617249\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 394 train loss: 0.671395480632782\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 395 train loss: 0.671363115310669\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 396 train loss: 0.6713290214538574\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 397 train loss: 0.6712952256202698\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 398 train loss: 0.6712614297866821\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 399 train loss: 0.6712276339530945\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 400 train loss: 0.6711938977241516\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 401 train loss: 0.6711601614952087\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 402 train loss: 0.6711264252662659\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 403 train loss: 0.671092689037323\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 404 train loss: 0.6710591316223145\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 405 train loss: 0.6710256338119507\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 406 train loss: 0.6709921360015869\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 407 train loss: 0.6709585189819336\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 408 train loss: 0.6709251403808594\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 409 train loss: 0.6708917021751404\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 410 train loss: 0.6708583831787109\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 411 train loss: 0.6708250045776367\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 412 train loss: 0.6707915663719177\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 413 train loss: 0.6707584261894226\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 414 train loss: 0.6707251667976379\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 415 train loss: 0.6706920266151428\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 416 train loss: 0.6706588268280029\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 417 train loss: 0.6706257462501526\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 418 train loss: 0.6705926656723022\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 419 train loss: 0.6705595850944519\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 420 train loss: 0.6705266833305359\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 421 train loss: 0.6704936027526855\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 422 train loss: 0.6704607009887695\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 423 train loss: 0.6704279184341431\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 424 train loss: 0.6703949570655823\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 425 train loss: 0.6703621745109558\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 426 train loss: 0.6703295707702637\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 427 train loss: 0.670296847820282\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 428 train loss: 0.6702640652656555\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 429 train loss: 0.6702314019203186\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 430 train loss: 0.6701987385749817\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 431 train loss: 0.6701661348342896\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 432 train loss: 0.6701335906982422\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 433 train loss: 0.6701011657714844\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 434 train loss: 0.670068621635437\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 435 train loss: 0.6700361371040344\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 436 train loss: 0.6700038909912109\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 437 train loss: 0.6699713468551636\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 438 train loss: 0.6699391007423401\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 439 train loss: 0.6699068546295166\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 440 train loss: 0.6698744297027588\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 441 train loss: 0.6698423624038696\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 442 train loss: 0.6698099970817566\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 443 train loss: 0.669778048992157\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 444 train loss: 0.6697458028793335\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 445 train loss: 0.6697137951850891\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 446 train loss: 0.6696817278862\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 447 train loss: 0.6696497201919556\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 448 train loss: 0.6696177124977112\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 449 train loss: 0.6695858240127563\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 450 train loss: 0.6695539355278015\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 451 train loss: 0.6695219874382019\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 452 train loss: 0.6694902777671814\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 453 train loss: 0.6694583892822266\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 454 train loss: 0.6694267392158508\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 455 train loss: 0.6693949103355408\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 456 train loss: 0.6693631410598755\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 457 train loss: 0.6693315505981445\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 458 train loss: 0.6692997813224792\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 459 train loss: 0.6692683100700378\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 460 train loss: 0.6692367792129517\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 461 train loss: 0.6692051291465759\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 462 train loss: 0.6691736578941345\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 463 train loss: 0.6691422462463379\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 464 train loss: 0.6691107749938965\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 465 train loss: 0.6690794825553894\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 466 train loss: 0.669048011302948\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 467 train loss: 0.6690167188644409\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 468 train loss: 0.6689854860305786\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 469 train loss: 0.6689541339874268\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 470 train loss: 0.6689229011535645\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 471 train loss: 0.6688917279243469\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 472 train loss: 0.6688606142997742\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 473 train loss: 0.6688294410705566\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 474 train loss: 0.6687984466552734\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 475 train loss: 0.6687673330307007\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 476 train loss: 0.6687363386154175\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 477 train loss: 0.668705403804779\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 478 train loss: 0.6686742901802063\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 479 train loss: 0.6686433553695679\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 480 train loss: 0.668612539768219\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 481 train loss: 0.6685817241668701\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 482 train loss: 0.6685507893562317\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 483 train loss: 0.6685199737548828\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 484 train loss: 0.6684891581535339\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 485 train loss: 0.6684585809707642\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 486 train loss: 0.6684277653694153\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 487 train loss: 0.6683971881866455\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 488 train loss: 0.6683664321899414\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 489 train loss: 0.6683359146118164\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 490 train loss: 0.6683053374290466\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 491 train loss: 0.6682747006416321\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 492 train loss: 0.6682441830635071\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 493 train loss: 0.6682137846946716\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 494 train loss: 0.6681833863258362\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 495 train loss: 0.668152928352356\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 496 train loss: 0.6681225299835205\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 497 train loss: 0.6680921912193298\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 498 train loss: 0.6680618524551392\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 499 train loss: 0.6680314540863037\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = SGD(net.parameters(),lr=lr)\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 500\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "    \n",
    "train_epochs(net, num_epochs, loss, optim, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务5: \n",
    "搭建一个新模型, 要求如下:\\\n",
    "5-1. 参考上面示例模型搭建步骤，尝试搭建一个4层的模型(4个linear层，其中包括输出层) \\\n",
    "5-2. 每一层的神经元个数为128/256/512/2 (最后为2,因为是二分类) \\\n",
    "5.3. 每一层linear的bias设置为True\n",
    "5-4. 前三层linear,每层输出接一个relu,即一层神经网络为: (linear->relu), 然后整个网络: (linear->relu) * 3 \\-> linear \\\n",
    "5-5. 在模型初始化时，使用xavier参数初始化方法对权重矩阵$W_i$进行初始化，偏置项$b_i$初始化置0 **(下标i，表示第i层)** \\\n",
    "5-6. 优化器使用SGD, lr = 0.001 \\\n",
    "5-7. 设置训练迭代epoch为200 \\\n",
    "5-8. 上述实验结束后,使用Adam作为优化器,lr不变,再进行一次实验,观察结果\n",
    "\n",
    "#### 与xavier方法相关的参考资料（若仅仅需要使用xavier，一般只用看前两个）：\n",
    "（1）神经网络各种初始化方法（normal，uniform，xavier）的numpy实现以及表现对比\n",
    "https://blog.csdn.net/kane7csdn/article/details/108896031\n",
    "\n",
    "（2）使用torch随机初始化参数 （包括xavier）\n",
    "https://blog.csdn.net/weixin_36893273/article/details/123641399\n",
    "\n",
    "（3）初始化及分布 （对各种初始化参数方法的原理介绍）\n",
    "https://blog.csdn.net/LWD19981223/article/details/124348675\n",
    "\n",
    "---\n",
    "\n",
    "**(代码写在下方)** \\\n",
    "**参考代码:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (ModuleLis): ModuleList(\n",
       "    (0): Linear(in_features=12288, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = 4\n",
    "        self.neuron_num_list = [12288, 128, 256, 512, 2]\n",
    "        \n",
    "        self.ModuleLis = nn.ModuleList()\n",
    "        for i in range(self.layers):\n",
    "            self.ModuleLis.append(torch.nn.Linear(self.neuron_num_list[i], self.neuron_num_list[i+1], bias=True))\n",
    "            \n",
    "            # 权重初始化\n",
    "#             std = np.sqrt(2. / (in_dim + out_dim))\n",
    "#             self.W[layer] = np.random.normal(loc=0., scale=std, size=[out_dim, in_dim])\n",
    "            std = np.sqrt(2. / (self.neuron_num_list[i] + self.neuron_num_list[i+1]))\n",
    "            self.ModuleLis[i].weights = nn.Parameter(torch.tensor(np.random.normal(loc=0., scale=std, size=(self.neuron_num_list[i], self.neuron_num_list[i+1])),requires_grad = True).float())\n",
    "            self.ModuleLis[i].bias = nn.Parameter(torch.tensor(np.zeros((self.neuron_num_list[i+1])), requires_grad = True).float())\n",
    "        \n",
    "        relu=nn.ReLU()\n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = X.view((-1, num_inputs))\n",
    "        for i in range(self.layers - 1):\n",
    "            X = relu(self.ModuleLis[i](X))\n",
    "        return self.ModuleLis[self.layers-1](X)\n",
    "    \n",
    "net = Net(num_inputs)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了代码复用和方便将下列训练神经网络的代码打包到一个函数里面,命名为train_epochs()\n",
    "\n",
    "\n",
    "```python\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # 前向传播\n",
    "    out = net(x)\n",
    "\n",
    "    # 计算loss\n",
    "    ls = loss(out, y)\n",
    "    print(f\"epoch: {i} train loss:\", ls.item())\n",
    "\n",
    "    # 反向传播\n",
    "    ls.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    optim.step()\n",
    "\n",
    "    # 清空梯度\n",
    "    optim.zero_grad()\n",
    "\n",
    "\n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "#         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "#         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "        print(\"Acc：\", np.mean(result),'\\n')\n",
    "\n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, num_epochs, loss_fn, optimizer, train_x, train_y):\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # 前向传播\n",
    "        out = net(x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, y)\n",
    "        print(f\"epoch: {i} train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "        # 测试部分\n",
    "        with torch.no_grad():\n",
    "            print(\"开始测试\")\n",
    "    #         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "            result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    #         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    #         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "            print(\"Acc：\", np.mean(result),'\\n')\n",
    "\n",
    "        # record the cost every 10 training epoch\n",
    "        if i % 10 == 0:\n",
    "            costs.append(ls.item())\n",
    "            accs.append(np.mean(result))\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.plot(np.squeeze(accs))\n",
    "    plt.ylabel('cost/acc')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(lr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 1.668895959854126\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 1 train loss: 449.6410827636719\n",
      "开始测试\n",
      "Acc： 0.3444976076555024 \n",
      "\n",
      "epoch: 2 train loss: 85.47896575927734\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 3 train loss: 78.29110717773438\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 4 train loss: 56.81816482543945\n",
      "开始测试\n",
      "Acc： 0.3492822966507177 \n",
      "\n",
      "epoch: 5 train loss: 30.392114639282227\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 6 train loss: 49.4582633972168\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 7 train loss: 51.00447082519531\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 16.0584716796875\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 9 train loss: 68.29882049560547\n",
      "开始测试\n",
      "Acc： 0.35406698564593303 \n",
      "\n",
      "epoch: 10 train loss: 44.708229064941406\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 11 train loss: 19.475431442260742\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 12 train loss: 32.123756408691406\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 13 train loss: 22.88616180419922\n",
      "开始测试\n",
      "Acc： 0.44019138755980863 \n",
      "\n",
      "epoch: 14 train loss: 1.5789092779159546\n",
      "开始测试\n",
      "Acc： 0.4354066985645933 \n",
      "\n",
      "epoch: 15 train loss: 2.4536330699920654\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 16 train loss: 17.148550033569336\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 17 train loss: 13.188799858093262\n",
      "开始测试\n",
      "Acc： 0.36363636363636365 \n",
      "\n",
      "epoch: 18 train loss: 7.028343200683594\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 19 train loss: 5.6106061935424805\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 20 train loss: 4.2078070640563965\n",
      "开始测试\n",
      "Acc： 0.3588516746411483 \n",
      "\n",
      "epoch: 21 train loss: 8.047118186950684\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 22 train loss: 1.577272653579712\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 23 train loss: 0.7822044491767883\n",
      "开始测试\n",
      "Acc： 0.5454545454545454 \n",
      "\n",
      "epoch: 24 train loss: 1.102453589439392\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 25 train loss: 4.039664268493652\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 26 train loss: 2.7391154766082764\n",
      "开始测试\n",
      "Acc： 0.40669856459330145 \n",
      "\n",
      "epoch: 27 train loss: 3.0347683429718018\n",
      "开始测试\n",
      "Acc： 0.6411483253588517 \n",
      "\n",
      "epoch: 28 train loss: 0.698846697807312\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 29 train loss: 2.4856479167938232\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 30 train loss: 2.0170061588287354\n",
      "开始测试\n",
      "Acc： 0.5645933014354066 \n",
      "\n",
      "epoch: 31 train loss: 1.0827713012695312\n",
      "开始测试\n",
      "Acc： 0.5980861244019139 \n",
      "\n",
      "epoch: 32 train loss: 1.0659117698669434\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 33 train loss: 1.354971170425415\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 34 train loss: 1.1647021770477295\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 35 train loss: 0.5967996120452881\n",
      "开始测试\n",
      "Acc： 0.5980861244019139 \n",
      "\n",
      "epoch: 36 train loss: 0.7398906946182251\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 37 train loss: 0.9841373562812805\n",
      "开始测试\n",
      "Acc： 0.6842105263157895 \n",
      "\n",
      "epoch: 38 train loss: 0.7167461514472961\n",
      "开始测试\n",
      "Acc： 0.5789473684210527 \n",
      "\n",
      "epoch: 39 train loss: 0.9421499371528625\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 40 train loss: 0.5725481510162354\n",
      "开始测试\n",
      "Acc： 0.645933014354067 \n",
      "\n",
      "epoch: 41 train loss: 0.7625588774681091\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 42 train loss: 0.5891761779785156\n",
      "开始测试\n",
      "Acc： 0.5885167464114832 \n",
      "\n",
      "epoch: 43 train loss: 0.7188932299613953\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 44 train loss: 0.5006240010261536\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 45 train loss: 0.5303968191146851\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 46 train loss: 0.5167158246040344\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 47 train loss: 0.53962242603302\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 48 train loss: 0.5246954560279846\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 49 train loss: 0.4796012341976166\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 50 train loss: 0.4869600236415863\n",
      "开始测试\n",
      "Acc： 0.8086124401913876 \n",
      "\n",
      "epoch: 51 train loss: 0.43968042731285095\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 52 train loss: 0.4163351356983185\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 53 train loss: 0.426126629114151\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 54 train loss: 0.39305630326271057\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 55 train loss: 0.3871850073337555\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 56 train loss: 0.3735807240009308\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 57 train loss: 0.3474406898021698\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 58 train loss: 0.34982702136039734\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 59 train loss: 0.3289463222026825\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 60 train loss: 0.3157239556312561\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 61 train loss: 0.3131197690963745\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 62 train loss: 0.2900577187538147\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 63 train loss: 0.27149105072021484\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 64 train loss: 0.26791566610336304\n",
      "开始测试\n",
      "Acc： 0.9043062200956937 \n",
      "\n",
      "epoch: 65 train loss: 0.26588913798332214\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 66 train loss: 0.25584810972213745\n",
      "开始测试\n",
      "Acc： 0.9138755980861244 \n",
      "\n",
      "epoch: 67 train loss: 0.24956226348876953\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 68 train loss: 0.24292440712451935\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 69 train loss: 0.25092995166778564\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 70 train loss: 0.27679312229156494\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 71 train loss: 0.36628207564353943\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 72 train loss: 0.32290175557136536\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 73 train loss: 0.20972399413585663\n",
      "开始测试\n",
      "Acc： 0.9234449760765551 \n",
      "\n",
      "epoch: 74 train loss: 0.21418356895446777\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 75 train loss: 0.23496422171592712\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 76 train loss: 0.15733817219734192\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 77 train loss: 0.19207893311977386\n",
      "开始测试\n",
      "Acc： 0.9138755980861244 \n",
      "\n",
      "epoch: 78 train loss: 0.18112725019454956\n",
      "开始测试\n",
      "Acc： 0.9665071770334929 \n",
      "\n",
      "epoch: 79 train loss: 0.1319740265607834\n",
      "开始测试\n",
      "Acc： 0.9617224880382775 \n",
      "\n",
      "epoch: 80 train loss: 0.12969256937503815\n",
      "开始测试\n",
      "Acc： 0.9425837320574163 \n",
      "\n",
      "epoch: 81 train loss: 0.14951850473880768\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 82 train loss: 0.14447011053562164\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 83 train loss: 0.10402525961399078\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 84 train loss: 0.10066434741020203\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 85 train loss: 0.10792078077793121\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 86 train loss: 0.08290786296129227\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 87 train loss: 0.07617570459842682\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 88 train loss: 0.08826926350593567\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 89 train loss: 0.07760316133499146\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 90 train loss: 0.05818859487771988\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 91 train loss: 0.05262616649270058\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 92 train loss: 0.06083259359002113\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 93 train loss: 0.07909335941076279\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 94 train loss: 0.09008552134037018\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 95 train loss: 0.14520491659641266\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 96 train loss: 0.46564778685569763\n",
      "开始测试\n",
      "Acc： 0.507177033492823 \n",
      "\n",
      "epoch: 97 train loss: 2.2841811180114746\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 98 train loss: 0.7982842922210693\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 99 train loss: 0.35456421971321106\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 100 train loss: 0.29784688353538513\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 101 train loss: 0.30487459897994995\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 102 train loss: 0.34082716703414917\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 103 train loss: 0.5525413155555725\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 104 train loss: 0.5655094385147095\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 105 train loss: 0.7583975195884705\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 106 train loss: 0.3213811218738556\n",
      "开始测试\n",
      "Acc： 0.569377990430622 \n",
      "\n",
      "epoch: 107 train loss: 1.07343327999115\n",
      "开始测试\n",
      "Acc： 0.6602870813397129 \n",
      "\n",
      "epoch: 108 train loss: 1.0716110467910767\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 109 train loss: 0.6820586919784546\n",
      "开始测试\n",
      "Acc： 0.507177033492823 \n",
      "\n",
      "epoch: 110 train loss: 1.677602767944336\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 111 train loss: 0.28851065039634705\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 112 train loss: 1.03878653049469\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 113 train loss: 0.30939844250679016\n",
      "开始测试\n",
      "Acc： 0.6363636363636364 \n",
      "\n",
      "epoch: 114 train loss: 0.820927083492279\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 115 train loss: 0.7842270731925964\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 116 train loss: 0.31845223903656006\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 117 train loss: 0.6599571704864502\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 118 train loss: 0.4568474292755127\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 119 train loss: 0.2804799973964691\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 120 train loss: 0.5829474925994873\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 121 train loss: 0.25868335366249084\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 122 train loss: 0.4453316330909729\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 123 train loss: 0.19504208862781525\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 124 train loss: 0.34027719497680664\n",
      "开始测试\n",
      "Acc： 0.9186602870813397 \n",
      "\n",
      "epoch: 125 train loss: 0.2050236612558365\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 126 train loss: 0.2701989412307739\n",
      "开始测试\n",
      "Acc： 0.9186602870813397 \n",
      "\n",
      "epoch: 127 train loss: 0.19623319804668427\n",
      "开始测试\n",
      "Acc： 0.9282296650717703 \n",
      "\n",
      "epoch: 128 train loss: 0.1956472396850586\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 129 train loss: 0.23360443115234375\n",
      "开始测试\n",
      "Acc： 0.937799043062201 \n",
      "\n",
      "epoch: 130 train loss: 0.15241920948028564\n",
      "开始测试\n",
      "Acc： 0.9138755980861244 \n",
      "\n",
      "epoch: 131 train loss: 0.21767663955688477\n",
      "开始测试\n",
      "Acc： 0.9665071770334929 \n",
      "\n",
      "epoch: 132 train loss: 0.09655973315238953\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 133 train loss: 0.1851734220981598\n",
      "开始测试\n",
      "Acc： 0.9473684210526315 \n",
      "\n",
      "epoch: 134 train loss: 0.14065159857273102\n",
      "开始测试\n",
      "Acc： 0.9521531100478469 \n",
      "\n",
      "epoch: 135 train loss: 0.12597881257534027\n",
      "开始测试\n",
      "Acc： 0.9330143540669856 \n",
      "\n",
      "epoch: 136 train loss: 0.152803435921669\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 137 train loss: 0.07960028201341629\n",
      "开始测试\n",
      "Acc： 0.9569377990430622 \n",
      "\n",
      "epoch: 138 train loss: 0.1228199154138565\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 139 train loss: 0.07010801881551743\n",
      "开始测试\n",
      "Acc： 0.9665071770334929 \n",
      "\n",
      "epoch: 140 train loss: 0.09467308223247528\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 141 train loss: 0.07151451706886292\n",
      "开始测试\n",
      "Acc： 0.9760765550239234 \n",
      "\n",
      "epoch: 142 train loss: 0.07888563722372055\n",
      "开始测试\n",
      "Acc： 0.9808612440191388 \n",
      "\n",
      "epoch: 143 train loss: 0.07124738395214081\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 144 train loss: 0.05906851962208748\n",
      "开始测试\n",
      "Acc： 0.9712918660287081 \n",
      "\n",
      "epoch: 145 train loss: 0.06719807535409927\n",
      "开始测试\n",
      "Acc： 0.9856459330143541 \n",
      "\n",
      "epoch: 146 train loss: 0.05034627392888069\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 147 train loss: 0.05919996276497841\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 148 train loss: 0.040697429329156876\n",
      "开始测试\n",
      "Acc： 0.9904306220095693 \n",
      "\n",
      "epoch: 149 train loss: 0.04885365441441536\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 150 train loss: 0.035149697214365005\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 151 train loss: 0.04139649122953415\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 152 train loss: 0.03001156449317932\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 153 train loss: 0.03395913168787956\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 154 train loss: 0.02676730789244175\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 155 train loss: 0.028152842074632645\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 156 train loss: 0.02472742274403572\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 157 train loss: 0.024834072217345238\n",
      "开始测试\n",
      "Acc： 0.9952153110047847 \n",
      "\n",
      "epoch: 158 train loss: 0.022758467122912407\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 159 train loss: 0.021508177742362022\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 160 train loss: 0.020412055775523186\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 161 train loss: 0.018516892567276955\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 162 train loss: 0.018465399742126465\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 163 train loss: 0.01583945006132126\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 164 train loss: 0.01672421209514141\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 165 train loss: 0.01413298211991787\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 166 train loss: 0.014729524962604046\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 167 train loss: 0.013026977889239788\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 168 train loss: 0.01266749668866396\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 169 train loss: 0.012204804457724094\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 170 train loss: 0.011034377850592136\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 171 train loss: 0.01117068063467741\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 172 train loss: 0.010175547562539577\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 173 train loss: 0.00988828856498003\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 174 train loss: 0.009625832550227642\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 175 train loss: 0.008872700855135918\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 176 train loss: 0.008827010169625282\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 177 train loss: 0.0084342360496521\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 178 train loss: 0.007936217822134495\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 179 train loss: 0.007849355228245258\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 180 train loss: 0.007419595494866371\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 181 train loss: 0.00706960866227746\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 182 train loss: 0.006951899733394384\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 183 train loss: 0.006567361764609814\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 184 train loss: 0.006282155402004719\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 185 train loss: 0.006162947043776512\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 186 train loss: 0.005881086457520723\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 187 train loss: 0.005680016241967678\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 188 train loss: 0.005581234116107225\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 189 train loss: 0.005344137083739042\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 190 train loss: 0.005179066210985184\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 191 train loss: 0.005067920777946711\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 192 train loss: 0.0048662577755749226\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 193 train loss: 0.004726933315396309\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 194 train loss: 0.004653968382626772\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 195 train loss: 0.00449327751994133\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 196 train loss: 0.004339330829679966\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 197 train loss: 0.004252199549227953\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 198 train loss: 0.004128605127334595\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n",
      "epoch: 199 train loss: 0.004020213149487972\n",
      "开始测试\n",
      "Acc： 1.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = Adam(net.parameters(),lr=lr)\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 200\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "train_epochs(net, num_epochs, loss, optim, x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务6: \n",
    "torchvision是pytorch的一个图形库, 主要用来处理图像数据,对图像数据进行数据增强\n",
    "\n",
    "要求:\n",
    "1. 在图像数据输入神经网络训练之前，使用python torchvision库对图像数据进行增强 \n",
    "2. 然后将增强后的图像数据输入到**本项目提供的2层神经网络中**进行训练和测试 \n",
    "\n",
    "参考资料: \\\n",
    "torch学习 (三十三)：Torch之图像增广 \\\n",
    "https://blog.csdn.net/weixin_44575152/article/details/118056405\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务7: \n",
    "我们在之前的实验中,均是一次性将209个图片输入到将训练集中的209个数据分批(batch)输入到**本项目提供的2层神经网络中**训练 \n",
    "\n",
    "要求: 一个batch的大小为10(batch_size = 10), 并且查资料/思考之后回答: 为何需要将数据分批训练 \n",
    "\n",
    "tips:\n",
    "1. 需要关注如何切分ndarray \n",
    "2. batch_size为10,将会分为21批(batch)的数据,最后一个batch的size如何通过程序计算(要求不能用if语句**直接指定**最后一个batchsize为9) \\\n",
    "(用取余操作来算出最后一个batch的size) \n",
    "3. 需要在迭代epoch的for循环中,再增加一层for循环来遍历各个batch的数据\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型/loss/optimizer,设定epochs/batchsize.....\n",
    "\n",
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "from torch.optim import SGD\n",
    "import math\n",
    "\n",
    "# 定义模型\n",
    "net = Net(num_inputs)\n",
    "net.train()\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义loss\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "# 定义optimizer\n",
    "lr = 0.001\n",
    "optim = Adam(net.parameters(),lr=lr)\n",
    "# 定义epoch数量\n",
    "num_epochs = 200\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "#num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "\n",
    "# 数据\n",
    "y = torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "# batch_size设置\n",
    "batch_size = 10\n",
    "\n",
    "# 分开每batch数据\n",
    "batch_number = int(y.shape[0]/10)\n",
    "\n",
    "# 假如不能整除,则last_batch_data_number表示: 该epoch中,最后一个batch的数据的数量\n",
    "last_batch_data_number = y.shape[0] % 10\n",
    "\n",
    "if last_batch_data_number != 0: batch_number = batch_number + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch:0, train loss: 0.6685003042221069\n",
      "epoch: 0, batch:10, train loss: 0.9062056541442871\n",
      "epoch: 0, batch:20, train loss: 0.46385160088539124\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 1, batch:0, train loss: 0.5550352931022644\n",
      "epoch: 1, batch:10, train loss: 0.7651192545890808\n",
      "epoch: 1, batch:20, train loss: 0.5947646498680115\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 2, batch:0, train loss: 0.5674561858177185\n",
      "epoch: 2, batch:10, train loss: 0.7786051630973816\n",
      "epoch: 2, batch:20, train loss: 0.611868143081665\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 3, batch:0, train loss: 0.5509053468704224\n",
      "epoch: 3, batch:10, train loss: 0.8547941446304321\n",
      "epoch: 3, batch:20, train loss: 0.6364132761955261\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 4, batch:0, train loss: 0.6326239705085754\n",
      "epoch: 4, batch:10, train loss: 0.7344573736190796\n",
      "epoch: 4, batch:20, train loss: 0.4208437204360962\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 5, batch:0, train loss: 0.4950571656227112\n",
      "epoch: 5, batch:10, train loss: 0.7362327575683594\n",
      "epoch: 5, batch:20, train loss: 0.5296080708503723\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 6, batch:0, train loss: 0.5387569069862366\n",
      "epoch: 6, batch:10, train loss: 0.7390469312667847\n",
      "epoch: 6, batch:20, train loss: 0.5731782913208008\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 7, batch:0, train loss: 0.5748235583305359\n",
      "epoch: 7, batch:10, train loss: 0.6964380145072937\n",
      "epoch: 7, batch:20, train loss: 0.5575507879257202\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 8, batch:0, train loss: 0.5620260238647461\n",
      "epoch: 8, batch:10, train loss: 0.7609784007072449\n",
      "epoch: 8, batch:20, train loss: 0.538557231426239\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 9, batch:0, train loss: 0.5418913960456848\n",
      "epoch: 9, batch:10, train loss: 0.7946121096611023\n",
      "epoch: 9, batch:20, train loss: 0.5057557821273804\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 10, batch:0, train loss: 0.5205873250961304\n",
      "epoch: 10, batch:10, train loss: 0.8130660057067871\n",
      "epoch: 10, batch:20, train loss: 0.4712277054786682\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 11, batch:0, train loss: 0.5140310525894165\n",
      "epoch: 11, batch:10, train loss: 0.8090526461601257\n",
      "epoch: 11, batch:20, train loss: 0.4251837730407715\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 12, batch:0, train loss: 0.45789846777915955\n",
      "epoch: 12, batch:10, train loss: 0.8759815096855164\n",
      "epoch: 12, batch:20, train loss: 0.4789432883262634\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 13, batch:0, train loss: 0.49417775869369507\n",
      "epoch: 13, batch:10, train loss: 0.7225576639175415\n",
      "epoch: 13, batch:20, train loss: 0.476696640253067\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 14, batch:0, train loss: 0.4515177607536316\n",
      "epoch: 14, batch:10, train loss: 0.6337551474571228\n",
      "epoch: 14, batch:20, train loss: 0.4671013057231903\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 15, batch:0, train loss: 0.4462735652923584\n",
      "epoch: 15, batch:10, train loss: 0.6050058007240295\n",
      "epoch: 15, batch:20, train loss: 0.6009136438369751\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 16, batch:0, train loss: 0.5970267653465271\n",
      "epoch: 16, batch:10, train loss: 0.7144359946250916\n",
      "epoch: 16, batch:20, train loss: 0.6126823425292969\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 17, batch:0, train loss: 0.5778763294219971\n",
      "epoch: 17, batch:10, train loss: 0.7382327318191528\n",
      "epoch: 17, batch:20, train loss: 0.54639732837677\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 18, batch:0, train loss: 0.5457096695899963\n",
      "epoch: 18, batch:10, train loss: 0.7707083225250244\n",
      "epoch: 18, batch:20, train loss: 0.4175788462162018\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 19, batch:0, train loss: 0.3852860629558563\n",
      "epoch: 19, batch:10, train loss: 0.7522862553596497\n",
      "epoch: 19, batch:20, train loss: 0.49201393127441406\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 20, batch:0, train loss: 0.5414113998413086\n",
      "epoch: 20, batch:10, train loss: 0.7482682466506958\n",
      "epoch: 20, batch:20, train loss: 0.4429762363433838\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 21, batch:0, train loss: 0.4995236396789551\n",
      "epoch: 21, batch:10, train loss: 0.7135359048843384\n",
      "epoch: 21, batch:20, train loss: 0.4943053424358368\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 22, batch:0, train loss: 0.5433969497680664\n",
      "epoch: 22, batch:10, train loss: 0.7424778342247009\n",
      "epoch: 22, batch:20, train loss: 0.4487394690513611\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 23, batch:0, train loss: 0.4820219874382019\n",
      "epoch: 23, batch:10, train loss: 0.79136723279953\n",
      "epoch: 23, batch:20, train loss: 0.5019848346710205\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 24, batch:0, train loss: 0.5512787103652954\n",
      "epoch: 24, batch:10, train loss: 0.7299321889877319\n",
      "epoch: 24, batch:20, train loss: 0.3894408643245697\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 25, batch:0, train loss: 0.28378114104270935\n",
      "epoch: 25, batch:10, train loss: 0.6664762496948242\n",
      "epoch: 25, batch:20, train loss: 0.47024378180503845\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 26, batch:0, train loss: 0.47961902618408203\n",
      "epoch: 26, batch:10, train loss: 0.6526419520378113\n",
      "epoch: 26, batch:20, train loss: 0.4362877905368805\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 27, batch:0, train loss: 0.41127651929855347\n",
      "epoch: 27, batch:10, train loss: 0.6193791627883911\n",
      "epoch: 27, batch:20, train loss: 0.4587155878543854\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 28, batch:0, train loss: 0.44814783334732056\n",
      "epoch: 28, batch:10, train loss: 0.6066322326660156\n",
      "epoch: 28, batch:20, train loss: 0.4458327293395996\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 29, batch:0, train loss: 0.3818497061729431\n",
      "epoch: 29, batch:10, train loss: 0.6001290082931519\n",
      "epoch: 29, batch:20, train loss: 0.46201834082603455\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 30, batch:0, train loss: 0.41749733686447144\n",
      "epoch: 30, batch:10, train loss: 0.5666043758392334\n",
      "epoch: 30, batch:20, train loss: 0.5458909869194031\n",
      "开始测试\n",
      "train Acc： 0.4880382775119617 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 31, batch:0, train loss: 0.5652782320976257\n",
      "epoch: 31, batch:10, train loss: 0.6961286664009094\n",
      "epoch: 31, batch:20, train loss: 0.47187545895576477\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 32, batch:0, train loss: 0.43421220779418945\n",
      "epoch: 32, batch:10, train loss: 0.6408882141113281\n",
      "epoch: 32, batch:20, train loss: 0.5291397571563721\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 33, batch:0, train loss: 0.5008472204208374\n",
      "epoch: 33, batch:10, train loss: 0.6679508686065674\n",
      "epoch: 33, batch:20, train loss: 0.5794851779937744\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 34, batch:0, train loss: 0.4688680171966553\n",
      "epoch: 34, batch:10, train loss: 0.5612636804580688\n",
      "epoch: 34, batch:20, train loss: 0.5354724526405334\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 35, batch:0, train loss: 0.5089411735534668\n",
      "epoch: 35, batch:10, train loss: 1.1822038888931274\n",
      "epoch: 35, batch:20, train loss: 0.5672590732574463\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 36, batch:0, train loss: 0.594586968421936\n",
      "epoch: 36, batch:10, train loss: 0.7030876278877258\n",
      "epoch: 36, batch:20, train loss: 0.5167765021324158\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 37, batch:0, train loss: 0.5294718146324158\n",
      "epoch: 37, batch:10, train loss: 0.48821011185646057\n",
      "epoch: 37, batch:20, train loss: 0.5699036717414856\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 38, batch:0, train loss: 0.5955871343612671\n",
      "epoch: 38, batch:10, train loss: 0.7107802629470825\n",
      "epoch: 38, batch:20, train loss: 0.6188715696334839\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 39, batch:0, train loss: 0.5823056101799011\n",
      "epoch: 39, batch:10, train loss: 0.7247052192687988\n",
      "epoch: 39, batch:20, train loss: 0.4545936584472656\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 40, batch:0, train loss: 0.4528183937072754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, batch:10, train loss: 0.6745445132255554\n",
      "epoch: 40, batch:20, train loss: 0.5175518989562988\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 41, batch:0, train loss: 0.525394082069397\n",
      "epoch: 41, batch:10, train loss: 0.7198233008384705\n",
      "epoch: 41, batch:20, train loss: 0.6093246936798096\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 42, batch:0, train loss: 0.5729339122772217\n",
      "epoch: 42, batch:10, train loss: 0.7375342845916748\n",
      "epoch: 42, batch:20, train loss: 0.5640497207641602\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 43, batch:0, train loss: 0.5466595888137817\n",
      "epoch: 43, batch:10, train loss: 0.7748490571975708\n",
      "epoch: 43, batch:20, train loss: 0.5161940455436707\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 44, batch:0, train loss: 0.5230525732040405\n",
      "epoch: 44, batch:10, train loss: 0.8063997030258179\n",
      "epoch: 44, batch:20, train loss: 0.4933421313762665\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 45, batch:0, train loss: 0.5148892402648926\n",
      "epoch: 45, batch:10, train loss: 0.811894416809082\n",
      "epoch: 45, batch:20, train loss: 0.48687443137168884\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 46, batch:0, train loss: 0.5146623849868774\n",
      "epoch: 46, batch:10, train loss: 0.8092886209487915\n",
      "epoch: 46, batch:20, train loss: 0.48046672344207764\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 47, batch:0, train loss: 0.5152109861373901\n",
      "epoch: 47, batch:10, train loss: 0.8078239560127258\n",
      "epoch: 47, batch:20, train loss: 0.4548816680908203\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 48, batch:0, train loss: 0.5154039263725281\n",
      "epoch: 48, batch:10, train loss: 0.8068501353263855\n",
      "epoch: 48, batch:20, train loss: 0.4354458749294281\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 49, batch:0, train loss: 0.5159525871276855\n",
      "epoch: 49, batch:10, train loss: 0.804715633392334\n",
      "epoch: 49, batch:20, train loss: 0.42498448491096497\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 50, batch:0, train loss: 0.5171566605567932\n",
      "epoch: 50, batch:10, train loss: 0.8000866770744324\n",
      "epoch: 50, batch:20, train loss: 0.4188154637813568\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 51, batch:0, train loss: 0.47697630524635315\n",
      "epoch: 51, batch:10, train loss: 0.7413704991340637\n",
      "epoch: 51, batch:20, train loss: 0.40130990743637085\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 52, batch:0, train loss: 0.4196957051753998\n",
      "epoch: 52, batch:10, train loss: 0.7466861605644226\n",
      "epoch: 52, batch:20, train loss: 0.3983076214790344\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 53, batch:0, train loss: 0.33945631980895996\n",
      "epoch: 53, batch:10, train loss: 0.6811904311180115\n",
      "epoch: 53, batch:20, train loss: 0.4198860824108124\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 54, batch:0, train loss: 0.35648223757743835\n",
      "epoch: 54, batch:10, train loss: 0.7214710116386414\n",
      "epoch: 54, batch:20, train loss: 0.4881075620651245\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 55, batch:0, train loss: 0.44670286774635315\n",
      "epoch: 55, batch:10, train loss: 0.6671670079231262\n",
      "epoch: 55, batch:20, train loss: 0.42123159766197205\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 56, batch:0, train loss: 0.3464338183403015\n",
      "epoch: 56, batch:10, train loss: 0.7186815142631531\n",
      "epoch: 56, batch:20, train loss: 0.4675901234149933\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 57, batch:0, train loss: 0.3508836627006531\n",
      "epoch: 57, batch:10, train loss: 0.6569873094558716\n",
      "epoch: 57, batch:20, train loss: 0.49986302852630615\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 58, batch:0, train loss: 0.39752644300460815\n",
      "epoch: 58, batch:10, train loss: 0.6030742526054382\n",
      "epoch: 58, batch:20, train loss: 0.4758627712726593\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 59, batch:0, train loss: 0.3816176950931549\n",
      "epoch: 59, batch:10, train loss: 0.645880937576294\n",
      "epoch: 59, batch:20, train loss: 0.4607248902320862\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 60, batch:0, train loss: 0.3063753545284271\n",
      "epoch: 60, batch:10, train loss: 0.6964279413223267\n",
      "epoch: 60, batch:20, train loss: 0.4615723788738251\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 61, batch:0, train loss: 0.28258806467056274\n",
      "epoch: 61, batch:10, train loss: 0.7013384103775024\n",
      "epoch: 61, batch:20, train loss: 0.4207666218280792\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 62, batch:0, train loss: 0.2354007214307785\n",
      "epoch: 62, batch:10, train loss: 0.6376913785934448\n",
      "epoch: 62, batch:20, train loss: 0.383139431476593\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 63, batch:0, train loss: 0.26232418417930603\n",
      "epoch: 63, batch:10, train loss: 0.6585202813148499\n",
      "epoch: 63, batch:20, train loss: 0.4431964159011841\n",
      "开始测试\n",
      "train Acc： 0.7559808612440191 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 64, batch:0, train loss: 0.23644456267356873\n",
      "epoch: 64, batch:10, train loss: 0.5793911218643188\n",
      "epoch: 64, batch:20, train loss: 0.5219080448150635\n",
      "开始测试\n",
      "train Acc： 0.6985645933014354 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 65, batch:0, train loss: 0.3091242015361786\n",
      "epoch: 65, batch:10, train loss: 0.6008278727531433\n",
      "epoch: 65, batch:20, train loss: 0.585168182849884\n",
      "开始测试\n",
      "train Acc： 0.5550239234449761 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 66, batch:0, train loss: 0.4954160153865814\n",
      "epoch: 66, batch:10, train loss: 0.6845740079879761\n",
      "epoch: 66, batch:20, train loss: 0.47614017128944397\n",
      "开始测试\n",
      "train Acc： 0.7464114832535885 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 67, batch:0, train loss: 0.21050801873207092\n",
      "epoch: 67, batch:10, train loss: 0.6334461569786072\n",
      "epoch: 67, batch:20, train loss: 0.5071032643318176\n",
      "开始测试\n",
      "train Acc： 0.7559808612440191 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 68, batch:0, train loss: 0.21003508567810059\n",
      "epoch: 68, batch:10, train loss: 0.6208010911941528\n",
      "epoch: 68, batch:20, train loss: 0.40111541748046875\n",
      "开始测试\n",
      "train Acc： 0.7607655502392344 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 69, batch:0, train loss: 0.20519356429576874\n",
      "epoch: 69, batch:10, train loss: 0.5952270030975342\n",
      "epoch: 69, batch:20, train loss: 0.5001196265220642\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 70, batch:0, train loss: 0.34939542412757874\n",
      "epoch: 70, batch:10, train loss: 0.6830455660820007\n",
      "epoch: 70, batch:20, train loss: 0.42576858401298523\n",
      "开始测试\n",
      "train Acc： 0.7655502392344498 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 71, batch:0, train loss: 0.2066952884197235\n",
      "epoch: 71, batch:10, train loss: 0.6188777089118958\n",
      "epoch: 71, batch:20, train loss: 0.4500855803489685\n",
      "开始测试\n",
      "train Acc： 0.7655502392344498 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 72, batch:0, train loss: 0.2038315087556839\n",
      "epoch: 72, batch:10, train loss: 0.5759060382843018\n",
      "epoch: 72, batch:20, train loss: 0.4524999260902405\n",
      "开始测试\n",
      "train Acc： 0.7081339712918661 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 73, batch:0, train loss: 0.28174763917922974\n",
      "epoch: 73, batch:10, train loss: 0.6806467771530151\n",
      "epoch: 73, batch:20, train loss: 0.42795926332473755\n",
      "开始测试\n",
      "train Acc： 0.7368421052631579 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 74, batch:0, train loss: 0.25439292192459106\n",
      "epoch: 74, batch:10, train loss: 0.6107255220413208\n",
      "epoch: 74, batch:20, train loss: 0.3579334616661072\n",
      "开始测试\n",
      "train Acc： 0.7703349282296651 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 75, batch:0, train loss: 0.20421472191810608\n",
      "epoch: 75, batch:10, train loss: 0.5653558969497681\n",
      "epoch: 75, batch:20, train loss: 0.6001636981964111\n",
      "开始测试\n",
      "train Acc： 0.5550239234449761 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 76, batch:0, train loss: 0.45165300369262695\n",
      "epoch: 76, batch:10, train loss: 0.6255437731742859\n",
      "epoch: 76, batch:20, train loss: 0.31882649660110474\n",
      "开始测试\n",
      "train Acc： 0.7655502392344498 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 77, batch:0, train loss: 0.2040141373872757\n",
      "epoch: 77, batch:10, train loss: 0.6115063428878784\n",
      "epoch: 77, batch:20, train loss: 0.392952024936676\n",
      "开始测试\n",
      "train Acc： 0.7559808612440191 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 78, batch:0, train loss: 0.21148359775543213\n",
      "epoch: 78, batch:10, train loss: 0.6050645112991333\n",
      "epoch: 78, batch:20, train loss: 0.35226932168006897\n",
      "开始测试\n",
      "train Acc： 0.7751196172248804 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 79, batch:0, train loss: 0.1997365951538086\n",
      "epoch: 79, batch:10, train loss: 0.5999855399131775\n",
      "epoch: 79, batch:20, train loss: 0.3439856469631195\n",
      "开始测试\n",
      "train Acc： 0.7511961722488039 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 80, batch:0, train loss: 0.20243079960346222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80, batch:10, train loss: 0.5966978073120117\n",
      "epoch: 80, batch:20, train loss: 0.34044623374938965\n",
      "开始测试\n",
      "train Acc： 0.7607655502392344 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 81, batch:0, train loss: 0.1974247694015503\n",
      "epoch: 81, batch:10, train loss: 0.6056278347969055\n",
      "epoch: 81, batch:20, train loss: 0.34125828742980957\n",
      "开始测试\n",
      "train Acc： 0.7703349282296651 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 82, batch:0, train loss: 0.19756050407886505\n",
      "epoch: 82, batch:10, train loss: 0.5974029302597046\n",
      "epoch: 82, batch:20, train loss: 0.34041449427604675\n",
      "开始测试\n",
      "train Acc： 0.7751196172248804 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 83, batch:0, train loss: 0.19898143410682678\n",
      "epoch: 83, batch:10, train loss: 0.5275546312332153\n",
      "epoch: 83, batch:20, train loss: 0.576604425907135\n",
      "开始测试\n",
      "train Acc： 0.6889952153110048 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 84, batch:0, train loss: 0.3049802780151367\n",
      "epoch: 84, batch:10, train loss: 0.5928558111190796\n",
      "epoch: 84, batch:20, train loss: 0.3439771831035614\n",
      "开始测试\n",
      "train Acc： 0.7751196172248804 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 85, batch:0, train loss: 0.19859035313129425\n",
      "epoch: 85, batch:10, train loss: 0.5479061007499695\n",
      "epoch: 85, batch:20, train loss: 0.6057115793228149\n",
      "开始测试\n",
      "train Acc： 0.6411483253588517 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 86, batch:0, train loss: 0.3654957711696625\n",
      "epoch: 86, batch:10, train loss: 0.591383695602417\n",
      "epoch: 86, batch:20, train loss: 0.3820230960845947\n",
      "开始测试\n",
      "train Acc： 0.7607655502392344 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 87, batch:0, train loss: 0.2224918156862259\n",
      "epoch: 87, batch:10, train loss: 0.6290765404701233\n",
      "epoch: 87, batch:20, train loss: 0.5285171270370483\n",
      "开始测试\n",
      "train Acc： 0.722488038277512 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 88, batch:0, train loss: 0.2864634394645691\n",
      "epoch: 88, batch:10, train loss: 0.5887006521224976\n",
      "epoch: 88, batch:20, train loss: 0.35246357321739197\n",
      "开始测试\n",
      "train Acc： 0.7703349282296651 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 89, batch:0, train loss: 0.1965664178133011\n",
      "epoch: 89, batch:10, train loss: 0.5887032151222229\n",
      "epoch: 89, batch:20, train loss: 0.35177159309387207\n",
      "开始测试\n",
      "train Acc： 0.7751196172248804 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 90, batch:0, train loss: 0.19588056206703186\n",
      "epoch: 90, batch:10, train loss: 0.5875933170318604\n",
      "epoch: 90, batch:20, train loss: 0.830761730670929\n",
      "开始测试\n",
      "train Acc： 0.7703349282296651 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 91, batch:0, train loss: 0.19512410461902618\n",
      "epoch: 91, batch:10, train loss: 0.5854629874229431\n",
      "epoch: 91, batch:20, train loss: 0.36033594608306885\n",
      "开始测试\n",
      "train Acc： 0.7799043062200957 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 92, batch:0, train loss: 0.19569706916809082\n",
      "epoch: 92, batch:10, train loss: 0.42565464973449707\n",
      "epoch: 92, batch:20, train loss: 0.6242358088493347\n",
      "开始测试\n",
      "train Acc： 0.5645933014354066 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 93, batch:0, train loss: 0.5044175386428833\n",
      "epoch: 93, batch:10, train loss: 0.675163209438324\n",
      "epoch: 93, batch:20, train loss: 0.3389437198638916\n",
      "开始测试\n",
      "train Acc： 0.7751196172248804 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 94, batch:0, train loss: 0.2165047824382782\n",
      "epoch: 94, batch:10, train loss: 0.5206127166748047\n",
      "epoch: 94, batch:20, train loss: 0.33474254608154297\n",
      "开始测试\n",
      "train Acc： 0.7703349282296651 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 95, batch:0, train loss: 0.20413346588611603\n",
      "epoch: 95, batch:10, train loss: 0.5758470892906189\n",
      "epoch: 95, batch:20, train loss: 0.3432966470718384\n",
      "开始测试\n",
      "train Acc： 0.7703349282296651 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 96, batch:0, train loss: 0.202050119638443\n",
      "epoch: 96, batch:10, train loss: 0.5926216244697571\n",
      "epoch: 96, batch:20, train loss: 0.33693528175354004\n",
      "开始测试\n",
      "train Acc： 0.6985645933014354 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 97, batch:0, train loss: 0.22534196078777313\n",
      "epoch: 97, batch:10, train loss: 0.6366661190986633\n",
      "epoch: 97, batch:20, train loss: 0.4470216929912567\n",
      "开始测试\n",
      "train Acc： 0.7272727272727273 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 98, batch:0, train loss: 0.30330589413642883\n",
      "epoch: 98, batch:10, train loss: 0.48802700638771057\n",
      "epoch: 98, batch:20, train loss: 0.4210470914840698\n",
      "开始测试\n",
      "train Acc： 0.69377990430622 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 99, batch:0, train loss: 0.34977471828460693\n",
      "epoch: 99, batch:10, train loss: 0.5966049432754517\n",
      "epoch: 99, batch:20, train loss: 0.3269582688808441\n",
      "开始测试\n",
      "train Acc： 0.8181818181818182 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 100, batch:0, train loss: 0.1972765177488327\n",
      "epoch: 100, batch:10, train loss: 0.5173963308334351\n",
      "epoch: 100, batch:20, train loss: 0.33636707067489624\n",
      "开始测试\n",
      "train Acc： 0.8277511961722488 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 101, batch:0, train loss: 0.19581639766693115\n",
      "epoch: 101, batch:10, train loss: 0.5097174048423767\n",
      "epoch: 101, batch:20, train loss: 0.33269762992858887\n",
      "开始测试\n",
      "train Acc： 0.8277511961722488 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 102, batch:0, train loss: 0.1941516399383545\n",
      "epoch: 102, batch:10, train loss: 0.4319688379764557\n",
      "epoch: 102, batch:20, train loss: 0.4025191366672516\n",
      "开始测试\n",
      "train Acc： 0.7559808612440191 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 103, batch:0, train loss: 0.25200486183166504\n",
      "epoch: 103, batch:10, train loss: 0.5767156481742859\n",
      "epoch: 103, batch:20, train loss: 0.2852991819381714\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 104, batch:0, train loss: 0.19248735904693604\n",
      "epoch: 104, batch:10, train loss: 0.4579194486141205\n",
      "epoch: 104, batch:20, train loss: 0.3884202539920807\n",
      "开始测试\n",
      "train Acc： 0.784688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 105, batch:0, train loss: 0.19296249747276306\n",
      "epoch: 105, batch:10, train loss: 0.42008477449417114\n",
      "epoch: 105, batch:20, train loss: 0.39681118726730347\n",
      "开始测试\n",
      "train Acc： 0.7320574162679426 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 106, batch:0, train loss: 0.3013649582862854\n",
      "epoch: 106, batch:10, train loss: 0.5859718322753906\n",
      "epoch: 106, batch:20, train loss: 0.3582397401332855\n",
      "开始测试\n",
      "train Acc： 0.8373205741626795 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 107, batch:0, train loss: 0.19427990913391113\n",
      "epoch: 107, batch:10, train loss: 0.5458735227584839\n",
      "epoch: 107, batch:20, train loss: 0.2735852301120758\n",
      "开始测试\n",
      "train Acc： 0.8421052631578947 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 108, batch:0, train loss: 0.19340351223945618\n",
      "epoch: 108, batch:10, train loss: 0.4641043543815613\n",
      "epoch: 108, batch:20, train loss: 0.3867207169532776\n",
      "开始测试\n",
      "train Acc： 0.7464114832535885 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 109, batch:0, train loss: 0.2210453450679779\n",
      "epoch: 109, batch:10, train loss: 0.5917697548866272\n",
      "epoch: 109, batch:20, train loss: 0.29439979791641235\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 110, batch:0, train loss: 0.1934109479188919\n",
      "epoch: 110, batch:10, train loss: 0.40744656324386597\n",
      "epoch: 110, batch:20, train loss: 0.28032588958740234\n",
      "开始测试\n",
      "train Acc： 0.8421052631578947 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 111, batch:0, train loss: 0.1923081874847412\n",
      "epoch: 111, batch:10, train loss: 0.38128089904785156\n",
      "epoch: 111, batch:20, train loss: 0.3966270685195923\n",
      "开始测试\n",
      "train Acc： 0.8181818181818182 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 112, batch:0, train loss: 0.19145791232585907\n",
      "epoch: 112, batch:10, train loss: 0.36748066544532776\n",
      "epoch: 112, batch:20, train loss: 0.2953547239303589\n",
      "开始测试\n",
      "train Acc： 0.8516746411483254 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 113, batch:0, train loss: 0.19099727272987366\n",
      "epoch: 113, batch:10, train loss: 0.35069552063941956\n",
      "epoch: 113, batch:20, train loss: 0.3501933813095093\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 114, batch:0, train loss: 0.19152352213859558\n",
      "epoch: 114, batch:10, train loss: 0.340894877910614\n",
      "epoch: 114, batch:20, train loss: 0.5283128619194031\n",
      "开始测试\n",
      "train Acc： 0.7320574162679426 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 115, batch:0, train loss: 0.2175992727279663\n",
      "epoch: 115, batch:10, train loss: 0.38584429025650024\n",
      "epoch: 115, batch:20, train loss: 0.3993149697780609\n",
      "开始测试\n",
      "train Acc： 0.7416267942583732 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 116, batch:0, train loss: 0.23366042971611023\n",
      "epoch: 116, batch:10, train loss: 0.5076186656951904\n",
      "epoch: 116, batch:20, train loss: 0.3835354149341583\n",
      "开始测试\n",
      "train Acc： 0.6794258373205742 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 117, batch:0, train loss: 0.3306625485420227\n",
      "epoch: 117, batch:10, train loss: 0.579352617263794\n",
      "epoch: 117, batch:20, train loss: 0.29754549264907837\n",
      "开始测试\n",
      "train Acc： 0.8277511961722488 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 118, batch:0, train loss: 0.1931418776512146\n",
      "epoch: 118, batch:10, train loss: 0.3987528085708618\n",
      "epoch: 118, batch:20, train loss: 0.3127748966217041\n",
      "开始测试\n",
      "train Acc： 0.8421052631578947 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 119, batch:0, train loss: 0.19210144877433777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 119, batch:10, train loss: 0.38794901967048645\n",
      "epoch: 119, batch:20, train loss: 0.3988614082336426\n",
      "开始测试\n",
      "train Acc： 0.7464114832535885 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 120, batch:0, train loss: 0.21707852184772491\n",
      "epoch: 120, batch:10, train loss: 0.574728786945343\n",
      "epoch: 120, batch:20, train loss: 0.27836906909942627\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 121, batch:0, train loss: 0.1915961652994156\n",
      "epoch: 121, batch:10, train loss: 0.37988418340682983\n",
      "epoch: 121, batch:20, train loss: 0.3974936604499817\n",
      "开始测试\n",
      "train Acc： 0.7894736842105263 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 122, batch:0, train loss: 0.19138659536838531\n",
      "epoch: 122, batch:10, train loss: 0.37235769629478455\n",
      "epoch: 122, batch:20, train loss: 0.2848283648490906\n",
      "开始测试\n",
      "train Acc： 0.8516746411483254 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 123, batch:0, train loss: 0.1910964548587799\n",
      "epoch: 123, batch:10, train loss: 0.47677668929100037\n",
      "epoch: 123, batch:20, train loss: 0.41826313734054565\n",
      "开始测试\n",
      "train Acc： 0.8038277511961722 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 124, batch:0, train loss: 0.19110295176506042\n",
      "epoch: 124, batch:10, train loss: 0.4010039269924164\n",
      "epoch: 124, batch:20, train loss: 0.41257742047309875\n",
      "开始测试\n",
      "train Acc： 0.7799043062200957 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 125, batch:0, train loss: 0.1911981999874115\n",
      "epoch: 125, batch:10, train loss: 0.5732923150062561\n",
      "epoch: 125, batch:20, train loss: 0.28181958198547363\n",
      "开始测试\n",
      "train Acc： 0.8564593301435407 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 126, batch:0, train loss: 0.19120420515537262\n",
      "epoch: 126, batch:10, train loss: 0.3641801178455353\n",
      "epoch: 126, batch:20, train loss: 0.31329146027565\n",
      "开始测试\n",
      "train Acc： 0.8516746411483254 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 127, batch:0, train loss: 0.19095604121685028\n",
      "epoch: 127, batch:10, train loss: 0.3505266308784485\n",
      "epoch: 127, batch:20, train loss: 0.42936402559280396\n",
      "开始测试\n",
      "train Acc： 0.7990430622009569 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 128, batch:0, train loss: 0.19156570732593536\n",
      "epoch: 128, batch:10, train loss: 0.5729212760925293\n",
      "epoch: 128, batch:20, train loss: 0.29261329770088196\n",
      "开始测试\n",
      "train Acc： 0.8421052631578947 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 129, batch:0, train loss: 0.19097670912742615\n",
      "epoch: 129, batch:10, train loss: 0.46439704298973083\n",
      "epoch: 129, batch:20, train loss: 0.339067667722702\n",
      "开始测试\n",
      "train Acc： 0.8373205741626795 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 130, batch:0, train loss: 0.19100119173526764\n",
      "epoch: 130, batch:10, train loss: 0.6326843500137329\n",
      "epoch: 130, batch:20, train loss: 0.41595569252967834\n",
      "开始测试\n",
      "train Acc： 0.6985645933014354 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 131, batch:0, train loss: 0.3221331238746643\n",
      "epoch: 131, batch:10, train loss: 0.48426684737205505\n",
      "epoch: 131, batch:20, train loss: 0.377096951007843\n",
      "开始测试\n",
      "train Acc： 0.7894736842105263 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 132, batch:0, train loss: 0.1920005977153778\n",
      "epoch: 132, batch:10, train loss: 0.575986921787262\n",
      "epoch: 132, batch:20, train loss: 0.2719706594944\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 133, batch:0, train loss: 0.19184911251068115\n",
      "epoch: 133, batch:10, train loss: 0.5750687122344971\n",
      "epoch: 133, batch:20, train loss: 0.38660159707069397\n",
      "开始测试\n",
      "train Acc： 0.784688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 134, batch:0, train loss: 0.19305720925331116\n",
      "epoch: 134, batch:10, train loss: 0.5270653367042542\n",
      "epoch: 134, batch:20, train loss: 0.27636703848838806\n",
      "开始测试\n",
      "train Acc： 0.8421052631578947 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 135, batch:0, train loss: 0.19196340441703796\n",
      "epoch: 135, batch:10, train loss: 0.4577431082725525\n",
      "epoch: 135, batch:20, train loss: 0.39586734771728516\n",
      "开始测试\n",
      "train Acc： 0.6698564593301436 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 136, batch:0, train loss: 0.34103378653526306\n",
      "epoch: 136, batch:10, train loss: 0.5772320032119751\n",
      "epoch: 136, batch:20, train loss: 0.26495516300201416\n",
      "开始测试\n",
      "train Acc： 0.8516746411483254 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 137, batch:0, train loss: 0.19290970265865326\n",
      "epoch: 137, batch:10, train loss: 0.3892768621444702\n",
      "epoch: 137, batch:20, train loss: 0.27314141392707825\n",
      "开始测试\n",
      "train Acc： 0.8516746411483254 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 138, batch:0, train loss: 0.19197602570056915\n",
      "epoch: 138, batch:10, train loss: 0.41218024492263794\n",
      "epoch: 138, batch:20, train loss: 0.41812798380851746\n",
      "开始测试\n",
      "train Acc： 0.6698564593301436 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 139, batch:0, train loss: 0.32254746556282043\n",
      "epoch: 139, batch:10, train loss: 0.5767614841461182\n",
      "epoch: 139, batch:20, train loss: 0.2682323455810547\n",
      "开始测试\n",
      "train Acc： 0.8564593301435407 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 140, batch:0, train loss: 0.19264550507068634\n",
      "epoch: 140, batch:10, train loss: 0.3871178925037384\n",
      "epoch: 140, batch:20, train loss: 0.27556112408638\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 141, batch:0, train loss: 0.19184736907482147\n",
      "epoch: 141, batch:10, train loss: 0.3712637722492218\n",
      "epoch: 141, batch:20, train loss: 0.2840367555618286\n",
      "开始测试\n",
      "train Acc： 0.8516746411483254 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 142, batch:0, train loss: 0.19103534519672394\n",
      "epoch: 142, batch:10, train loss: 0.35961979627609253\n",
      "epoch: 142, batch:20, train loss: 0.2932479977607727\n",
      "开始测试\n",
      "train Acc： 0.8564593301435407 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 143, batch:0, train loss: 0.19100546836853027\n",
      "epoch: 143, batch:10, train loss: 0.3458743691444397\n",
      "epoch: 143, batch:20, train loss: 0.4329374134540558\n",
      "开始测试\n",
      "train Acc： 0.7559808612440191 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 144, batch:0, train loss: 0.1950596272945404\n",
      "epoch: 144, batch:10, train loss: 0.5728815197944641\n",
      "epoch: 144, batch:20, train loss: 0.2865520417690277\n",
      "开始测试\n",
      "train Acc： 0.8516746411483254 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 145, batch:0, train loss: 0.19097770750522614\n",
      "epoch: 145, batch:10, train loss: 1.0788822174072266\n",
      "epoch: 145, batch:20, train loss: 0.413704514503479\n",
      "开始测试\n",
      "train Acc： 0.69377990430622 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 146, batch:0, train loss: 0.3209274709224701\n",
      "epoch: 146, batch:10, train loss: 0.5529991984367371\n",
      "epoch: 146, batch:20, train loss: 0.28739336133003235\n",
      "开始测试\n",
      "train Acc： 0.8038277511961722 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 147, batch:0, train loss: 0.19160708785057068\n",
      "epoch: 147, batch:10, train loss: 0.5764743089675903\n",
      "epoch: 147, batch:20, train loss: 0.37869441509246826\n",
      "开始测试\n",
      "train Acc： 0.7894736842105263 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 148, batch:0, train loss: 0.19246789813041687\n",
      "epoch: 148, batch:10, train loss: 0.3852803409099579\n",
      "epoch: 148, batch:20, train loss: 0.3798770010471344\n",
      "开始测试\n",
      "train Acc： 0.7942583732057417 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 149, batch:0, train loss: 0.19592276215553284\n",
      "epoch: 149, batch:10, train loss: 0.4475312829017639\n",
      "epoch: 149, batch:20, train loss: 0.2717876732349396\n",
      "开始测试\n",
      "train Acc： 0.861244019138756 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 150, batch:0, train loss: 0.1919335424900055\n",
      "epoch: 150, batch:10, train loss: 0.37405160069465637\n",
      "epoch: 150, batch:20, train loss: 0.33940625190734863\n",
      "开始测试\n",
      "train Acc： 0.8277511961722488 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 151, batch:0, train loss: 0.19146588444709778\n",
      "epoch: 151, batch:10, train loss: 0.36294564604759216\n",
      "epoch: 151, batch:20, train loss: 0.41046851873397827\n",
      "开始测试\n",
      "train Acc： 0.7703349282296651 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 152, batch:0, train loss: 0.21411088109016418\n",
      "epoch: 152, batch:10, train loss: 0.4971242845058441\n",
      "epoch: 152, batch:20, train loss: 0.28423455357551575\n",
      "开始测试\n",
      "train Acc： 0.861244019138756 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 153, batch:0, train loss: 0.19102799892425537\n",
      "epoch: 153, batch:10, train loss: 0.3578764796257019\n",
      "epoch: 153, batch:20, train loss: 0.31265199184417725\n",
      "开始测试\n",
      "train Acc： 0.8325358851674641 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 154, batch:0, train loss: 0.19119691848754883\n",
      "epoch: 154, batch:10, train loss: 0.4158616065979004\n",
      "epoch: 154, batch:20, train loss: 0.4260747730731964\n",
      "开始测试\n",
      "train Acc： 0.7894736842105263 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 155, batch:0, train loss: 0.21055133640766144\n",
      "epoch: 155, batch:10, train loss: 0.5587509870529175\n",
      "epoch: 155, batch:20, train loss: 0.28789761662483215\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 156, batch:0, train loss: 0.19096121191978455\n",
      "epoch: 156, batch:10, train loss: 0.3544488549232483\n",
      "epoch: 156, batch:20, train loss: 0.2962043881416321\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 157, batch:0, train loss: 0.19106212258338928\n",
      "epoch: 157, batch:10, train loss: 0.3430352509021759\n",
      "epoch: 157, batch:20, train loss: 0.3711768090724945\n",
      "开始测试\n",
      "train Acc： 0.7990430622009569 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 158, batch:0, train loss: 0.19309675693511963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 158, batch:10, train loss: 0.5737819075584412\n",
      "epoch: 158, batch:20, train loss: 0.29261037707328796\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 159, batch:0, train loss: 0.19098015129566193\n",
      "epoch: 159, batch:10, train loss: 0.35090577602386475\n",
      "epoch: 159, batch:20, train loss: 0.29699838161468506\n",
      "开始测试\n",
      "train Acc： 0.8564593301435407 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 160, batch:0, train loss: 0.19112662971019745\n",
      "epoch: 160, batch:10, train loss: 0.3415538966655731\n",
      "epoch: 160, batch:20, train loss: 0.4393835663795471\n",
      "开始测试\n",
      "train Acc： 0.7894736842105263 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 161, batch:0, train loss: 0.21157069504261017\n",
      "epoch: 161, batch:10, train loss: 0.5732860565185547\n",
      "epoch: 161, batch:20, train loss: 0.2963617444038391\n",
      "开始测试\n",
      "train Acc： 0.8421052631578947 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 162, batch:0, train loss: 0.19100990891456604\n",
      "epoch: 162, batch:10, train loss: 0.34973233938217163\n",
      "epoch: 162, batch:20, train loss: 0.2983194887638092\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 163, batch:0, train loss: 0.19118410348892212\n",
      "epoch: 163, batch:10, train loss: 0.342689573764801\n",
      "epoch: 163, batch:20, train loss: 0.30598166584968567\n",
      "开始测试\n",
      "train Acc： 0.8277511961722488 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 164, batch:0, train loss: 0.19165194034576416\n",
      "epoch: 164, batch:10, train loss: 0.5734531283378601\n",
      "epoch: 164, batch:20, train loss: 0.2933392822742462\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 165, batch:0, train loss: 0.19099530577659607\n",
      "epoch: 165, batch:10, train loss: 0.3484899401664734\n",
      "epoch: 165, batch:20, train loss: 0.29971322417259216\n",
      "开始测试\n",
      "train Acc： 0.8564593301435407 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 166, batch:0, train loss: 0.1912544071674347\n",
      "epoch: 166, batch:10, train loss: 0.3386063277721405\n",
      "epoch: 166, batch:20, train loss: 0.3091581165790558\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 167, batch:0, train loss: 0.19188225269317627\n",
      "epoch: 167, batch:10, train loss: 0.3319203555583954\n",
      "epoch: 167, batch:20, train loss: 0.3355209529399872\n",
      "开始测试\n",
      "train Acc： 0.8038277511961722 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 168, batch:0, train loss: 0.1925085335969925\n",
      "epoch: 168, batch:10, train loss: 0.5744757652282715\n",
      "epoch: 168, batch:20, train loss: 0.2916465699672699\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 169, batch:0, train loss: 0.19096319377422333\n",
      "epoch: 169, batch:10, train loss: 0.3560611605644226\n",
      "epoch: 169, batch:20, train loss: 0.29183703660964966\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 170, batch:0, train loss: 0.1909768432378769\n",
      "epoch: 170, batch:10, train loss: 0.34888744354248047\n",
      "epoch: 170, batch:20, train loss: 0.3003574311733246\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 171, batch:0, train loss: 0.19128403067588806\n",
      "epoch: 171, batch:10, train loss: 0.3381355106830597\n",
      "epoch: 171, batch:20, train loss: 0.3139151632785797\n",
      "开始测试\n",
      "train Acc： 0.8038277511961722 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 172, batch:0, train loss: 0.19233635067939758\n",
      "epoch: 172, batch:10, train loss: 0.5783354043960571\n",
      "epoch: 172, batch:20, train loss: 0.2915046215057373\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 173, batch:0, train loss: 0.19097158312797546\n",
      "epoch: 173, batch:10, train loss: 0.3573662340641022\n",
      "epoch: 173, batch:20, train loss: 0.2906251549720764\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 174, batch:0, train loss: 0.19096338748931885\n",
      "epoch: 174, batch:10, train loss: 0.3502569794654846\n",
      "epoch: 174, batch:20, train loss: 0.29999008774757385\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 175, batch:0, train loss: 0.19126468896865845\n",
      "epoch: 175, batch:10, train loss: 0.3384125828742981\n",
      "epoch: 175, batch:20, train loss: 0.3096994161605835\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 176, batch:0, train loss: 0.19197747111320496\n",
      "epoch: 176, batch:10, train loss: 0.3472867012023926\n",
      "epoch: 176, batch:20, train loss: 0.3152497410774231\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 177, batch:0, train loss: 0.19251474738121033\n",
      "epoch: 177, batch:10, train loss: 0.40075165033340454\n",
      "epoch: 177, batch:20, train loss: 0.32331526279449463\n",
      "开始测试\n",
      "train Acc： 0.7942583732057417 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 178, batch:0, train loss: 0.19287152588367462\n",
      "epoch: 178, batch:10, train loss: 0.6473597288131714\n",
      "epoch: 178, batch:20, train loss: 0.2880042791366577\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 179, batch:0, train loss: 0.1909651905298233\n",
      "epoch: 179, batch:10, train loss: 0.3592531979084015\n",
      "epoch: 179, batch:20, train loss: 0.41800710558891296\n",
      "开始测试\n",
      "train Acc： 0.8755980861244019 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 180, batch:0, train loss: 0.19095464050769806\n",
      "epoch: 180, batch:10, train loss: 0.377300500869751\n",
      "epoch: 180, batch:20, train loss: 0.29722124338150024\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 181, batch:0, train loss: 0.19112853705883026\n",
      "epoch: 181, batch:10, train loss: 0.34187471866607666\n",
      "epoch: 181, batch:20, train loss: 0.3076150119304657\n",
      "开始测试\n",
      "train Acc： 0.8564593301435407 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 182, batch:0, train loss: 0.191677063703537\n",
      "epoch: 182, batch:10, train loss: 0.33655884861946106\n",
      "epoch: 182, batch:20, train loss: 0.3118518590927124\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 183, batch:0, train loss: 0.19218146800994873\n",
      "epoch: 183, batch:10, train loss: 0.40574556589126587\n",
      "epoch: 183, batch:20, train loss: 0.316026896238327\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 184, batch:0, train loss: 0.1926220953464508\n",
      "epoch: 184, batch:10, train loss: 0.34573817253112793\n",
      "epoch: 184, batch:20, train loss: 0.3182705342769623\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 185, batch:0, train loss: 0.19285300374031067\n",
      "epoch: 185, batch:10, train loss: 0.32611414790153503\n",
      "epoch: 185, batch:20, train loss: 0.3216705918312073\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 186, batch:0, train loss: 0.19331711530685425\n",
      "epoch: 186, batch:10, train loss: 0.5793285965919495\n",
      "epoch: 186, batch:20, train loss: 0.3139308989048004\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 187, batch:0, train loss: 0.19232068955898285\n",
      "epoch: 187, batch:10, train loss: 0.3319741189479828\n",
      "epoch: 187, batch:20, train loss: 0.3128698468208313\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 188, batch:0, train loss: 0.192295104265213\n",
      "epoch: 188, batch:10, train loss: 0.33904343843460083\n",
      "epoch: 188, batch:20, train loss: 0.3152026832103729\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 189, batch:0, train loss: 0.1925085037946701\n",
      "epoch: 189, batch:10, train loss: 0.3280118405818939\n",
      "epoch: 189, batch:20, train loss: 0.3184281885623932\n",
      "开始测试\n",
      "train Acc： 0.8564593301435407 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 190, batch:0, train loss: 0.19288191199302673\n",
      "epoch: 190, batch:10, train loss: 0.46518102288246155\n",
      "epoch: 190, batch:20, train loss: 0.3146801292896271\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 191, batch:0, train loss: 0.1924668550491333\n",
      "epoch: 191, batch:10, train loss: 0.3298451602458954\n",
      "epoch: 191, batch:20, train loss: 0.3167286515235901\n",
      "开始测试\n",
      "train Acc： 0.8755980861244019 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 192, batch:0, train loss: 0.1926998794078827\n",
      "epoch: 192, batch:10, train loss: 0.3493272662162781\n",
      "epoch: 192, batch:20, train loss: 0.3190529942512512\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 193, batch:0, train loss: 0.19295910000801086\n",
      "epoch: 193, batch:10, train loss: 0.3257675766944885\n",
      "epoch: 193, batch:20, train loss: 0.320381760597229\n",
      "开始测试\n",
      "train Acc： 0.8708133971291866 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 194, batch:0, train loss: 0.19312337040901184\n",
      "epoch: 194, batch:10, train loss: 0.326816588640213\n",
      "epoch: 194, batch:20, train loss: 0.3209306299686432\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 195, batch:0, train loss: 0.19318124651908875\n",
      "epoch: 195, batch:10, train loss: 0.32638347148895264\n",
      "epoch: 195, batch:20, train loss: 0.3202430009841919\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 196, batch:0, train loss: 0.19308854639530182\n",
      "epoch: 196, batch:10, train loss: 0.32518649101257324\n",
      "epoch: 196, batch:20, train loss: 0.32174497842788696\n",
      "开始测试\n",
      "train Acc： 0.8755980861244019 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 197, batch:0, train loss: 0.1933036744594574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 197, batch:10, train loss: 0.3232566714286804\n",
      "epoch: 197, batch:20, train loss: 0.32441458106040955\n",
      "开始测试\n",
      "train Acc： 0.8133971291866029 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 198, batch:0, train loss: 0.1936722844839096\n",
      "epoch: 198, batch:10, train loss: 0.4538336396217346\n",
      "epoch: 198, batch:20, train loss: 0.323021799325943\n",
      "开始测试\n",
      "train Acc： 0.84688995215311 \n",
      "test Acc： 0.66 \n",
      "\n",
      "epoch: 199, batch:0, train loss: 0.19338515400886536\n",
      "epoch: 199, batch:10, train loss: 0.32599130272865295\n",
      "epoch: 199, batch:20, train loss: 0.31965371966362\n",
      "开始测试\n",
      "train Acc： 0.8660287081339713 \n",
      "test Acc： 0.66 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch:0, train loss: 2.440150499343872\n",
      "epoch: 0, batch:10, train loss: 18.014667510986328\n",
      "epoch: 0, batch:20, train loss: 1.2610212564468384\n",
      "开始测试\n",
      "train Acc： 0.36363636363636365 \n",
      "test Acc： 0.58 \n",
      "\n",
      "epoch: 1, batch:0, train loss: 2.3194193840026855\n",
      "epoch: 1, batch:10, train loss: 21.169109344482422\n",
      "epoch: 1, batch:20, train loss: 1.82694411277771\n",
      "开始测试\n",
      "train Acc： 0.6746411483253588 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 2, batch:0, train loss: 3.836623430252075\n",
      "epoch: 2, batch:10, train loss: 1.1058416366577148\n",
      "epoch: 2, batch:20, train loss: 0.23394370079040527\n",
      "开始测试\n",
      "train Acc： 0.6698564593301436 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 3, batch:0, train loss: 1.1347805261611938\n",
      "epoch: 3, batch:10, train loss: 2.6035683155059814\n",
      "epoch: 3, batch:20, train loss: 0.5534945130348206\n",
      "开始测试\n",
      "train Acc： 0.6889952153110048 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 4, batch:0, train loss: 0.5813700556755066\n",
      "epoch: 4, batch:10, train loss: 0.45397648215293884\n",
      "epoch: 4, batch:20, train loss: 0.7332730293273926\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 5, batch:0, train loss: 1.3266031742095947\n",
      "epoch: 5, batch:10, train loss: 1.1594396829605103\n",
      "epoch: 5, batch:20, train loss: 0.6697525382041931\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 6, batch:0, train loss: 0.4955061376094818\n",
      "epoch: 6, batch:10, train loss: 0.6975364685058594\n",
      "epoch: 6, batch:20, train loss: 0.4825041592121124\n",
      "开始测试\n",
      "train Acc： 0.6602870813397129 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 7, batch:0, train loss: 0.3724398612976074\n",
      "epoch: 7, batch:10, train loss: 0.7006499171257019\n",
      "epoch: 7, batch:20, train loss: 0.6593647003173828\n",
      "开始测试\n",
      "train Acc： 0.6507177033492823 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 8, batch:0, train loss: 0.851373553276062\n",
      "epoch: 8, batch:10, train loss: 0.7097105383872986\n",
      "epoch: 8, batch:20, train loss: 0.6293138265609741\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 9, batch:0, train loss: 0.5828239917755127\n",
      "epoch: 9, batch:10, train loss: 0.7225068807601929\n",
      "epoch: 9, batch:20, train loss: 0.5529325008392334\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 10, batch:0, train loss: 0.5662903189659119\n",
      "epoch: 10, batch:10, train loss: 0.7351686358451843\n",
      "epoch: 10, batch:20, train loss: 0.4815773367881775\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 11, batch:0, train loss: 0.39881762862205505\n",
      "epoch: 11, batch:10, train loss: 0.7363704442977905\n",
      "epoch: 11, batch:20, train loss: 0.5701614618301392\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 12, batch:0, train loss: 0.5589457154273987\n",
      "epoch: 12, batch:10, train loss: 0.7406923174858093\n",
      "epoch: 12, batch:20, train loss: 0.5247901082038879\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 13, batch:0, train loss: 0.549081563949585\n",
      "epoch: 13, batch:10, train loss: 0.7529377341270447\n",
      "epoch: 13, batch:20, train loss: 0.49960586428642273\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 14, batch:0, train loss: 0.48584216833114624\n",
      "epoch: 14, batch:10, train loss: 0.7032278776168823\n",
      "epoch: 14, batch:20, train loss: 0.5664999485015869\n",
      "开始测试\n",
      "train Acc： 0.6507177033492823 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 15, batch:0, train loss: 0.7880980372428894\n",
      "epoch: 15, batch:10, train loss: 0.7531720399856567\n",
      "epoch: 15, batch:20, train loss: 0.5534980297088623\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 16, batch:0, train loss: 0.5895857214927673\n",
      "epoch: 16, batch:10, train loss: 0.7665418386459351\n",
      "epoch: 16, batch:20, train loss: 0.5356385111808777\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 17, batch:0, train loss: 0.5774757266044617\n",
      "epoch: 17, batch:10, train loss: 0.7807031869888306\n",
      "epoch: 17, batch:20, train loss: 0.5212334990501404\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 18, batch:0, train loss: 0.5680481791496277\n",
      "epoch: 18, batch:10, train loss: 0.7920814752578735\n",
      "epoch: 18, batch:20, train loss: 0.5113394260406494\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 19, batch:0, train loss: 0.5617581605911255\n",
      "epoch: 19, batch:10, train loss: 0.7997792959213257\n",
      "epoch: 19, batch:20, train loss: 0.5053727030754089\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 20, batch:0, train loss: 0.5580422282218933\n",
      "epoch: 20, batch:10, train loss: 0.8043287992477417\n",
      "epoch: 20, batch:20, train loss: 0.5020734071731567\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 21, batch:0, train loss: 0.5560129880905151\n",
      "epoch: 21, batch:10, train loss: 0.8068180084228516\n",
      "epoch: 21, batch:20, train loss: 0.5002290606498718\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 22, batch:0, train loss: 0.55488520860672\n",
      "epoch: 22, batch:10, train loss: 0.8081588745117188\n",
      "epoch: 22, batch:20, train loss: 0.49922215938568115\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 23, batch:0, train loss: 0.5542718768119812\n",
      "epoch: 23, batch:10, train loss: 0.8088680505752563\n",
      "epoch: 23, batch:20, train loss: 0.4986473321914673\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 24, batch:0, train loss: 0.5539215803146362\n",
      "epoch: 24, batch:10, train loss: 0.8092573881149292\n",
      "epoch: 24, batch:20, train loss: 0.4983079135417938\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 25, batch:0, train loss: 0.5537148714065552\n",
      "epoch: 25, batch:10, train loss: 0.8094708323478699\n",
      "epoch: 25, batch:20, train loss: 0.49810394644737244\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 26, batch:0, train loss: 0.5535907745361328\n",
      "epoch: 26, batch:10, train loss: 0.8095861673355103\n",
      "epoch: 26, batch:20, train loss: 0.49796661734580994\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 27, batch:0, train loss: 0.5535070896148682\n",
      "epoch: 27, batch:10, train loss: 0.8096505403518677\n",
      "epoch: 27, batch:20, train loss: 0.49787652492523193\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 28, batch:0, train loss: 0.5534521341323853\n",
      "epoch: 28, batch:10, train loss: 0.8096903562545776\n",
      "epoch: 28, batch:20, train loss: 0.49781104922294617\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 29, batch:0, train loss: 0.5534122586250305\n",
      "epoch: 29, batch:10, train loss: 0.8097156286239624\n",
      "epoch: 29, batch:20, train loss: 0.4977617859840393\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 30, batch:0, train loss: 0.5533820390701294\n",
      "epoch: 30, batch:10, train loss: 0.8097324371337891\n",
      "epoch: 30, batch:20, train loss: 0.497723251581192\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 31, batch:0, train loss: 0.5533584356307983\n",
      "epoch: 31, batch:10, train loss: 0.8097443580627441\n",
      "epoch: 31, batch:20, train loss: 0.4976922273635864\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 32, batch:0, train loss: 0.5533394813537598\n",
      "epoch: 32, batch:10, train loss: 0.8097532987594604\n",
      "epoch: 32, batch:20, train loss: 0.49766677618026733\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 33, batch:0, train loss: 0.5533238649368286\n",
      "epoch: 33, batch:10, train loss: 0.809760570526123\n",
      "epoch: 33, batch:20, train loss: 0.4976382255554199\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 34, batch:0, train loss: 0.5533064603805542\n",
      "epoch: 34, batch:10, train loss: 0.8097651600837708\n",
      "epoch: 34, batch:20, train loss: 0.49762147665023804\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 35, batch:0, train loss: 0.5532961487770081\n",
      "epoch: 35, batch:10, train loss: 0.8097690343856812\n",
      "epoch: 35, batch:20, train loss: 0.49759572744369507\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 36, batch:0, train loss: 0.5532806515693665\n",
      "epoch: 36, batch:10, train loss: 0.8097688555717468\n",
      "epoch: 36, batch:20, train loss: 0.4975835978984833\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 37, batch:0, train loss: 0.55327308177948\n",
      "epoch: 37, batch:10, train loss: 0.8097707629203796\n",
      "epoch: 37, batch:20, train loss: 0.4975734353065491\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 38, batch:0, train loss: 0.5532668828964233\n",
      "epoch: 38, batch:10, train loss: 0.809773325920105\n",
      "epoch: 38, batch:20, train loss: 0.4975684583187103\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 39, batch:0, train loss: 0.553263783454895\n",
      "epoch: 39, batch:10, train loss: 0.8097729682922363\n",
      "epoch: 39, batch:20, train loss: 0.49755996465682983\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 40, batch:0, train loss: 0.5532585382461548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, batch:10, train loss: 0.8097774386405945\n",
      "epoch: 40, batch:20, train loss: 0.49755117297172546\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 41, batch:0, train loss: 0.5532530546188354\n",
      "epoch: 41, batch:10, train loss: 0.8097821474075317\n",
      "epoch: 41, batch:20, train loss: 0.4975382387638092\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 42, batch:0, train loss: 0.5532451868057251\n",
      "epoch: 42, batch:10, train loss: 0.8097856640815735\n",
      "epoch: 42, batch:20, train loss: 0.49753251671791077\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 43, batch:0, train loss: 0.5532416105270386\n",
      "epoch: 43, batch:10, train loss: 0.8097888231277466\n",
      "epoch: 43, batch:20, train loss: 0.4975699186325073\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 44, batch:0, train loss: 0.5532644987106323\n",
      "epoch: 44, batch:10, train loss: 0.8097880482673645\n",
      "epoch: 44, batch:20, train loss: 0.4975602924823761\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 45, batch:0, train loss: 0.5532581806182861\n",
      "epoch: 45, batch:10, train loss: 0.809799313545227\n",
      "epoch: 45, batch:20, train loss: 0.4975486993789673\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 46, batch:0, train loss: 0.5532510280609131\n",
      "epoch: 46, batch:10, train loss: 0.8098090887069702\n",
      "epoch: 46, batch:20, train loss: 0.49753978848457336\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 47, batch:0, train loss: 0.5532455444335938\n",
      "epoch: 47, batch:10, train loss: 0.8098165392875671\n",
      "epoch: 47, batch:20, train loss: 0.49753326177597046\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 48, batch:0, train loss: 0.5532414317131042\n",
      "epoch: 48, batch:10, train loss: 0.8098222017288208\n",
      "epoch: 48, batch:20, train loss: 0.49752840399742126\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 49, batch:0, train loss: 0.5532385110855103\n",
      "epoch: 49, batch:10, train loss: 0.809826672077179\n",
      "epoch: 49, batch:20, train loss: 0.49752479791641235\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 50, batch:0, train loss: 0.5532363057136536\n",
      "epoch: 50, batch:10, train loss: 0.8098305463790894\n",
      "epoch: 50, batch:20, train loss: 0.4975219964981079\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 51, batch:0, train loss: 0.5532344579696655\n",
      "epoch: 51, batch:10, train loss: 0.8098341226577759\n",
      "epoch: 51, batch:20, train loss: 0.4975196421146393\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 52, batch:0, train loss: 0.5532329678535461\n",
      "epoch: 52, batch:10, train loss: 0.8098373413085938\n",
      "epoch: 52, batch:20, train loss: 0.4975624680519104\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 53, batch:0, train loss: 0.5532591938972473\n",
      "epoch: 53, batch:10, train loss: 0.8098346590995789\n",
      "epoch: 53, batch:20, train loss: 0.4975236654281616\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 54, batch:0, train loss: 0.5532354116439819\n",
      "epoch: 54, batch:10, train loss: 0.8098393678665161\n",
      "epoch: 54, batch:20, train loss: 0.49751970171928406\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 55, batch:0, train loss: 0.5532328486442566\n",
      "epoch: 55, batch:10, train loss: 0.8098434209823608\n",
      "epoch: 55, batch:20, train loss: 0.4975167512893677\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 56, batch:0, train loss: 0.5532310009002686\n",
      "epoch: 56, batch:10, train loss: 0.8098467588424683\n",
      "epoch: 56, batch:20, train loss: 0.4975147247314453\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 57, batch:0, train loss: 0.553229808807373\n",
      "epoch: 57, batch:10, train loss: 0.8098499178886414\n",
      "epoch: 57, batch:20, train loss: 0.49751344323158264\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 58, batch:0, train loss: 0.5532289743423462\n",
      "epoch: 58, batch:10, train loss: 0.8098527789115906\n",
      "epoch: 58, batch:20, train loss: 0.4975126087665558\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 59, batch:0, train loss: 0.5532284379005432\n",
      "epoch: 59, batch:10, train loss: 0.8098552823066711\n",
      "epoch: 59, batch:20, train loss: 0.4975121319293976\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 60, batch:0, train loss: 0.5532281398773193\n",
      "epoch: 60, batch:10, train loss: 0.8098577260971069\n",
      "epoch: 60, batch:20, train loss: 0.49751198291778564\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 61, batch:0, train loss: 0.5532279014587402\n",
      "epoch: 61, batch:10, train loss: 0.8098602294921875\n",
      "epoch: 61, batch:20, train loss: 0.49751198291778564\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 62, batch:0, train loss: 0.553227961063385\n",
      "epoch: 62, batch:10, train loss: 0.8098626136779785\n",
      "epoch: 62, batch:20, train loss: 0.49751609563827515\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 63, batch:0, train loss: 0.5532293319702148\n",
      "epoch: 63, batch:10, train loss: 0.8098652958869934\n",
      "epoch: 63, batch:20, train loss: 0.4975121319293976\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 64, batch:0, train loss: 0.553227961063385\n",
      "epoch: 64, batch:10, train loss: 0.8098675012588501\n",
      "epoch: 64, batch:20, train loss: 0.49751269817352295\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 65, batch:0, train loss: 0.5532283782958984\n",
      "epoch: 65, batch:10, train loss: 0.8098695874214172\n",
      "epoch: 65, batch:20, train loss: 0.49751317501068115\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 66, batch:0, train loss: 0.553228497505188\n",
      "epoch: 66, batch:10, train loss: 0.8098717927932739\n",
      "epoch: 66, batch:20, train loss: 0.4975139796733856\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 67, batch:0, train loss: 0.5532289743423462\n",
      "epoch: 67, batch:10, train loss: 0.8098741769790649\n",
      "epoch: 67, batch:20, train loss: 0.4975147247314453\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 68, batch:0, train loss: 0.5532294511795044\n",
      "epoch: 68, batch:10, train loss: 0.8098762631416321\n",
      "epoch: 68, batch:20, train loss: 0.49751579761505127\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 69, batch:0, train loss: 0.5532299280166626\n",
      "epoch: 69, batch:10, train loss: 0.8098783493041992\n",
      "epoch: 69, batch:20, train loss: 0.4975167512893677\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 70, batch:0, train loss: 0.5532305240631104\n",
      "epoch: 70, batch:10, train loss: 0.8098805546760559\n",
      "epoch: 70, batch:20, train loss: 0.4975179135799408\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 71, batch:0, train loss: 0.5532312393188477\n",
      "epoch: 71, batch:10, train loss: 0.809882640838623\n",
      "epoch: 71, batch:20, train loss: 0.49751901626586914\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 72, batch:0, train loss: 0.5532318353652954\n",
      "epoch: 72, batch:10, train loss: 0.8098848462104797\n",
      "epoch: 72, batch:20, train loss: 0.49752017855644226\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 73, batch:0, train loss: 0.5532326102256775\n",
      "epoch: 73, batch:10, train loss: 0.8098869323730469\n",
      "epoch: 73, batch:20, train loss: 0.49752146005630493\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 74, batch:0, train loss: 0.5532333850860596\n",
      "epoch: 74, batch:10, train loss: 0.809889018535614\n",
      "epoch: 74, batch:20, train loss: 0.4975006878376007\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 75, batch:0, train loss: 0.5532208681106567\n",
      "epoch: 75, batch:10, train loss: 0.8098880648612976\n",
      "epoch: 75, batch:20, train loss: 0.49750593304634094\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 76, batch:0, train loss: 0.5532239675521851\n",
      "epoch: 76, batch:10, train loss: 0.8098864555358887\n",
      "epoch: 76, batch:20, train loss: 0.49751028418540955\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 77, batch:0, train loss: 0.5532265901565552\n",
      "epoch: 77, batch:10, train loss: 0.80988609790802\n",
      "epoch: 77, batch:20, train loss: 0.4975135028362274\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 78, batch:0, train loss: 0.553228497505188\n",
      "epoch: 78, batch:10, train loss: 0.809886634349823\n",
      "epoch: 78, batch:20, train loss: 0.49751603603363037\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 79, batch:0, train loss: 0.5532301664352417\n",
      "epoch: 79, batch:10, train loss: 0.8098875880241394\n",
      "epoch: 79, batch:20, train loss: 0.4976074993610382\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 80, batch:0, train loss: 0.5532873272895813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80, batch:10, train loss: 0.8098417520523071\n",
      "epoch: 80, batch:20, train loss: 0.49762699007987976\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 81, batch:0, train loss: 0.553297758102417\n",
      "epoch: 81, batch:10, train loss: 0.8098510503768921\n",
      "epoch: 81, batch:20, train loss: 0.4976031482219696\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 82, batch:0, train loss: 0.5532826781272888\n",
      "epoch: 82, batch:10, train loss: 0.8098791241645813\n",
      "epoch: 82, batch:20, train loss: 0.4975827634334564\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 83, batch:0, train loss: 0.553270161151886\n",
      "epoch: 83, batch:10, train loss: 0.8098992109298706\n",
      "epoch: 83, batch:20, train loss: 0.49757036566734314\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 84, batch:0, train loss: 0.5532625317573547\n",
      "epoch: 84, batch:10, train loss: 0.809911847114563\n",
      "epoch: 84, batch:20, train loss: 0.4975636899471283\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 85, batch:0, train loss: 0.5532585382461548\n",
      "epoch: 85, batch:10, train loss: 0.8099196553230286\n",
      "epoch: 85, batch:20, train loss: 0.4975605010986328\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 86, batch:0, train loss: 0.5532565712928772\n",
      "epoch: 86, batch:10, train loss: 0.8099247813224792\n",
      "epoch: 86, batch:20, train loss: 0.49755939841270447\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 87, batch:0, train loss: 0.5532559156417847\n",
      "epoch: 87, batch:10, train loss: 0.8099282383918762\n",
      "epoch: 87, batch:20, train loss: 0.49755948781967163\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 88, batch:0, train loss: 0.5532558560371399\n",
      "epoch: 88, batch:10, train loss: 0.8099309206008911\n",
      "epoch: 88, batch:20, train loss: 0.4975602924823761\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 89, batch:0, train loss: 0.5532563924789429\n",
      "epoch: 89, batch:10, train loss: 0.8099328875541687\n",
      "epoch: 89, batch:20, train loss: 0.49756139516830444\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 90, batch:0, train loss: 0.5532570481300354\n",
      "epoch: 90, batch:10, train loss: 0.8099347949028015\n",
      "epoch: 90, batch:20, train loss: 0.49756282567977905\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 91, batch:0, train loss: 0.5532578825950623\n",
      "epoch: 91, batch:10, train loss: 0.8099364042282104\n",
      "epoch: 91, batch:20, train loss: 0.497564435005188\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 92, batch:0, train loss: 0.5532588362693787\n",
      "epoch: 92, batch:10, train loss: 0.8099379539489746\n",
      "epoch: 92, batch:20, train loss: 0.4975661635398865\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 93, batch:0, train loss: 0.5532598495483398\n",
      "epoch: 93, batch:10, train loss: 0.8099393844604492\n",
      "epoch: 93, batch:20, train loss: 0.4975679814815521\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 94, batch:0, train loss: 0.5532609820365906\n",
      "epoch: 94, batch:10, train loss: 0.8099409937858582\n",
      "epoch: 94, batch:20, train loss: 0.49756982922554016\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 95, batch:0, train loss: 0.5532620549201965\n",
      "epoch: 95, batch:10, train loss: 0.8099424242973328\n",
      "epoch: 95, batch:20, train loss: 0.4975709319114685\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 96, batch:0, train loss: 0.5532628893852234\n",
      "epoch: 96, batch:10, train loss: 0.8099434971809387\n",
      "epoch: 96, batch:20, train loss: 0.49757325649261475\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 97, batch:0, train loss: 0.5532640814781189\n",
      "epoch: 97, batch:10, train loss: 0.8099448084831238\n",
      "epoch: 97, batch:20, train loss: 0.49757522344589233\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 98, batch:0, train loss: 0.553265392780304\n",
      "epoch: 98, batch:10, train loss: 0.8099462389945984\n",
      "epoch: 98, batch:20, train loss: 0.4975771903991699\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 99, batch:0, train loss: 0.5532665252685547\n",
      "epoch: 99, batch:10, train loss: 0.8099477887153625\n",
      "epoch: 99, batch:20, train loss: 0.4975793659687042\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 100, batch:0, train loss: 0.553267776966095\n",
      "epoch: 100, batch:10, train loss: 0.8099492192268372\n",
      "epoch: 100, batch:20, train loss: 0.497581422328949\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 101, batch:0, train loss: 0.5532690286636353\n",
      "epoch: 101, batch:10, train loss: 0.8099506497383118\n",
      "epoch: 101, batch:20, train loss: 0.4975835084915161\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 102, batch:0, train loss: 0.5532703399658203\n",
      "epoch: 102, batch:10, train loss: 0.8099520802497864\n",
      "epoch: 102, batch:20, train loss: 0.49758556485176086\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 103, batch:0, train loss: 0.553271472454071\n",
      "epoch: 103, batch:10, train loss: 0.8099536895751953\n",
      "epoch: 103, batch:20, train loss: 0.4975876212120056\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 104, batch:0, train loss: 0.5532727241516113\n",
      "epoch: 104, batch:10, train loss: 0.8099552989006042\n",
      "epoch: 104, batch:20, train loss: 0.4975897967815399\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 105, batch:0, train loss: 0.5532740950584412\n",
      "epoch: 105, batch:10, train loss: 0.8099567294120789\n",
      "epoch: 105, batch:20, train loss: 0.4975919723510742\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 106, batch:0, train loss: 0.5532754063606262\n",
      "epoch: 106, batch:10, train loss: 0.809958279132843\n",
      "epoch: 106, batch:20, train loss: 0.49759429693222046\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 107, batch:0, train loss: 0.5532767176628113\n",
      "epoch: 107, batch:10, train loss: 0.8099597692489624\n",
      "epoch: 107, batch:20, train loss: 0.49759647250175476\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 108, batch:0, train loss: 0.5532780885696411\n",
      "epoch: 108, batch:10, train loss: 0.809961199760437\n",
      "epoch: 108, batch:20, train loss: 0.49759891629219055\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 109, batch:0, train loss: 0.5532795190811157\n",
      "epoch: 109, batch:10, train loss: 0.8099626302719116\n",
      "epoch: 109, batch:20, train loss: 0.4976012408733368\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 110, batch:0, train loss: 0.5532810091972351\n",
      "epoch: 110, batch:10, train loss: 0.8099640607833862\n",
      "epoch: 110, batch:20, train loss: 0.49760356545448303\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 111, batch:0, train loss: 0.5532823801040649\n",
      "epoch: 111, batch:10, train loss: 0.8099654912948608\n",
      "epoch: 111, batch:20, train loss: 0.4976058602333069\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 112, batch:0, train loss: 0.5532838106155396\n",
      "epoch: 112, batch:10, train loss: 0.809967041015625\n",
      "epoch: 112, batch:20, train loss: 0.49760833382606506\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 113, batch:0, train loss: 0.5532852411270142\n",
      "epoch: 113, batch:10, train loss: 0.8099684715270996\n",
      "epoch: 113, batch:20, train loss: 0.49761077761650085\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 114, batch:0, train loss: 0.5532866716384888\n",
      "epoch: 114, batch:10, train loss: 0.8099699020385742\n",
      "epoch: 114, batch:20, train loss: 0.49765545129776\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 115, batch:0, train loss: 0.5533140301704407\n",
      "epoch: 115, batch:10, train loss: 0.8099683523178101\n",
      "epoch: 115, batch:20, train loss: 0.49765193462371826\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 116, batch:0, train loss: 0.5533115267753601\n",
      "epoch: 116, batch:10, train loss: 0.8099777102470398\n",
      "epoch: 116, batch:20, train loss: 0.49764665961265564\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 117, batch:0, train loss: 0.5533083081245422\n",
      "epoch: 117, batch:10, train loss: 0.809985339641571\n",
      "epoch: 117, batch:20, train loss: 0.4976438879966736\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 118, batch:0, train loss: 0.5533065795898438\n",
      "epoch: 118, batch:10, train loss: 0.8099902868270874\n",
      "epoch: 118, batch:20, train loss: 0.49768516421318054\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 119, batch:0, train loss: 0.5533323287963867\n",
      "epoch: 119, batch:10, train loss: 0.8099733591079712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 119, batch:20, train loss: 0.49768176674842834\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 120, batch:0, train loss: 0.5533294677734375\n",
      "epoch: 120, batch:10, train loss: 0.8099855184555054\n",
      "epoch: 120, batch:20, train loss: 0.49767202138900757\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 121, batch:0, train loss: 0.5533234477043152\n",
      "epoch: 121, batch:10, train loss: 0.8099964261054993\n",
      "epoch: 121, batch:20, train loss: 0.4976659417152405\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 122, batch:0, train loss: 0.5533197522163391\n",
      "epoch: 122, batch:10, train loss: 0.8100034594535828\n",
      "epoch: 122, batch:20, train loss: 0.497663289308548\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 123, batch:0, train loss: 0.5533180832862854\n",
      "epoch: 123, batch:10, train loss: 0.8100077509880066\n",
      "epoch: 123, batch:20, train loss: 0.49766260385513306\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 124, batch:0, train loss: 0.553317666053772\n",
      "epoch: 124, batch:10, train loss: 0.810010552406311\n",
      "epoch: 124, batch:20, train loss: 0.497663289308548\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 125, batch:0, train loss: 0.5533180832862854\n",
      "epoch: 125, batch:10, train loss: 0.8100122213363647\n",
      "epoch: 125, batch:20, train loss: 0.49766460061073303\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 126, batch:0, train loss: 0.5533188581466675\n",
      "epoch: 126, batch:10, train loss: 0.8100136518478394\n",
      "epoch: 126, batch:20, train loss: 0.4976380169391632\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 127, batch:0, train loss: 0.5533028841018677\n",
      "epoch: 127, batch:10, train loss: 0.8100104331970215\n",
      "epoch: 127, batch:20, train loss: 0.49764537811279297\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 128, batch:0, train loss: 0.5533073544502258\n",
      "epoch: 128, batch:10, train loss: 0.8100064992904663\n",
      "epoch: 128, batch:20, train loss: 0.4976513683795929\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 129, batch:0, train loss: 0.5533111095428467\n",
      "epoch: 129, batch:10, train loss: 0.8100044131278992\n",
      "epoch: 129, batch:20, train loss: 0.49765607714653015\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 130, batch:0, train loss: 0.5533139109611511\n",
      "epoch: 130, batch:10, train loss: 0.8100038766860962\n",
      "epoch: 130, batch:20, train loss: 0.4976598918437958\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 131, batch:0, train loss: 0.5533162355422974\n",
      "epoch: 131, batch:10, train loss: 0.8100040555000305\n",
      "epoch: 131, batch:20, train loss: 0.4976630210876465\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 132, batch:0, train loss: 0.5533181428909302\n",
      "epoch: 132, batch:10, train loss: 0.8100045323371887\n",
      "epoch: 132, batch:20, train loss: 0.4976660907268524\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 133, batch:0, train loss: 0.5533199310302734\n",
      "epoch: 133, batch:10, train loss: 0.8100053668022156\n",
      "epoch: 133, batch:20, train loss: 0.49766889214515686\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 134, batch:0, train loss: 0.5533217191696167\n",
      "epoch: 134, batch:10, train loss: 0.8100064396858215\n",
      "epoch: 134, batch:20, train loss: 0.4976421594619751\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 135, batch:0, train loss: 0.5533056259155273\n",
      "epoch: 135, batch:10, train loss: 0.810002326965332\n",
      "epoch: 135, batch:20, train loss: 0.49764981865882874\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 136, batch:0, train loss: 0.5533102750778198\n",
      "epoch: 136, batch:10, train loss: 0.8099983334541321\n",
      "epoch: 136, batch:20, train loss: 0.4976561963558197\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 137, batch:0, train loss: 0.5533140897750854\n",
      "epoch: 137, batch:10, train loss: 0.8099962472915649\n",
      "epoch: 137, batch:20, train loss: 0.4976610541343689\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 138, batch:0, train loss: 0.5533171892166138\n",
      "epoch: 138, batch:10, train loss: 0.8099955320358276\n",
      "epoch: 138, batch:20, train loss: 0.4976648688316345\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 139, batch:0, train loss: 0.5533193945884705\n",
      "epoch: 139, batch:10, train loss: 0.8099955320358276\n",
      "epoch: 139, batch:20, train loss: 0.49766820669174194\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 140, batch:0, train loss: 0.5533214211463928\n",
      "epoch: 140, batch:10, train loss: 0.8099962472915649\n",
      "epoch: 140, batch:20, train loss: 0.4976712763309479\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 141, batch:0, train loss: 0.5533232688903809\n",
      "epoch: 141, batch:10, train loss: 0.8099969625473022\n",
      "epoch: 141, batch:20, train loss: 0.4977145195007324\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 142, batch:0, train loss: 0.5533498525619507\n",
      "epoch: 142, batch:10, train loss: 0.8099945783615112\n",
      "epoch: 142, batch:20, train loss: 0.49771276116371155\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 143, batch:0, train loss: 0.5533483624458313\n",
      "epoch: 143, batch:10, train loss: 0.8100038766860962\n",
      "epoch: 143, batch:20, train loss: 0.4977084696292877\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 144, batch:0, train loss: 0.5533456206321716\n",
      "epoch: 144, batch:10, train loss: 0.8100115656852722\n",
      "epoch: 144, batch:20, train loss: 0.4977065324783325\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 145, batch:0, train loss: 0.5533444881439209\n",
      "epoch: 145, batch:10, train loss: 0.8100168108940125\n",
      "epoch: 145, batch:20, train loss: 0.4977066218852997\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 146, batch:0, train loss: 0.5533444881439209\n",
      "epoch: 146, batch:10, train loss: 0.8100202679634094\n",
      "epoch: 146, batch:20, train loss: 0.4977077841758728\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 147, batch:0, train loss: 0.5533452033996582\n",
      "epoch: 147, batch:10, train loss: 0.8100227117538452\n",
      "epoch: 147, batch:20, train loss: 0.4977097511291504\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 148, batch:0, train loss: 0.5533463954925537\n",
      "epoch: 148, batch:10, train loss: 0.810024619102478\n",
      "epoch: 148, batch:20, train loss: 0.4977121353149414\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 149, batch:0, train loss: 0.5533478260040283\n",
      "epoch: 149, batch:10, train loss: 0.8100262880325317\n",
      "epoch: 149, batch:20, train loss: 0.49771466851234436\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 150, batch:0, train loss: 0.5533493757247925\n",
      "epoch: 150, batch:10, train loss: 0.8100277185440063\n",
      "epoch: 150, batch:20, train loss: 0.49768516421318054\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 151, batch:0, train loss: 0.5533315539360046\n",
      "epoch: 151, batch:10, train loss: 0.810023307800293\n",
      "epoch: 151, batch:20, train loss: 0.4976935386657715\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 152, batch:0, train loss: 0.5533367395401001\n",
      "epoch: 152, batch:10, train loss: 0.8100188970565796\n",
      "epoch: 152, batch:20, train loss: 0.49770036339759827\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 153, batch:0, train loss: 0.5533409714698792\n",
      "epoch: 153, batch:10, train loss: 0.8100164532661438\n",
      "epoch: 153, batch:20, train loss: 0.4977056682109833\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 154, batch:0, train loss: 0.5533441305160522\n",
      "epoch: 154, batch:10, train loss: 0.8100156784057617\n",
      "epoch: 154, batch:20, train loss: 0.49770990014076233\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 155, batch:0, train loss: 0.5533466935157776\n",
      "epoch: 155, batch:10, train loss: 0.8100156784057617\n",
      "epoch: 155, batch:20, train loss: 0.497713565826416\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 156, batch:0, train loss: 0.5533488988876343\n",
      "epoch: 156, batch:10, train loss: 0.8100164532661438\n",
      "epoch: 156, batch:20, train loss: 0.49771684408187866\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 157, batch:0, train loss: 0.5533508658409119\n",
      "epoch: 157, batch:10, train loss: 0.8100172281265259\n",
      "epoch: 157, batch:20, train loss: 0.49771997332572937\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 158, batch:0, train loss: 0.5533527135848999\n",
      "epoch: 158, batch:10, train loss: 0.8100181818008423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 158, batch:20, train loss: 0.49772393703460693\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 159, batch:0, train loss: 0.55335533618927\n",
      "epoch: 159, batch:10, train loss: 0.8100181818008423\n",
      "epoch: 159, batch:20, train loss: 0.4977271854877472\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 160, batch:0, train loss: 0.5533570647239685\n",
      "epoch: 160, batch:10, train loss: 0.8100196719169617\n",
      "epoch: 160, batch:20, train loss: 0.49772971868515015\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 161, batch:0, train loss: 0.5533587336540222\n",
      "epoch: 161, batch:10, train loss: 0.8100212216377258\n",
      "epoch: 161, batch:20, train loss: 0.49773237109184265\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 162, batch:0, train loss: 0.5533602833747864\n",
      "epoch: 162, batch:10, train loss: 0.8100227117538452\n",
      "epoch: 162, batch:20, train loss: 0.49773523211479187\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 163, batch:0, train loss: 0.5533620119094849\n",
      "epoch: 163, batch:10, train loss: 0.810024082660675\n",
      "epoch: 163, batch:20, train loss: 0.4977380335330963\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 164, batch:0, train loss: 0.5533636808395386\n",
      "epoch: 164, batch:10, train loss: 0.8100253939628601\n",
      "epoch: 164, batch:20, train loss: 0.4977410137653351\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 165, batch:0, train loss: 0.5533655285835266\n",
      "epoch: 165, batch:10, train loss: 0.8100267648696899\n",
      "epoch: 165, batch:20, train loss: 0.49785158038139343\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 166, batch:0, train loss: 0.5534331798553467\n",
      "epoch: 166, batch:10, train loss: 0.8100116848945618\n",
      "epoch: 166, batch:20, train loss: 0.4978295564651489\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 167, batch:0, train loss: 0.5534189939498901\n",
      "epoch: 167, batch:10, train loss: 0.8100305795669556\n",
      "epoch: 167, batch:20, train loss: 0.49780845642089844\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 168, batch:0, train loss: 0.5534060597419739\n",
      "epoch: 168, batch:10, train loss: 0.8100454211235046\n",
      "epoch: 168, batch:20, train loss: 0.4977962076663971\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 169, batch:0, train loss: 0.5533985495567322\n",
      "epoch: 169, batch:10, train loss: 0.8100539445877075\n",
      "epoch: 169, batch:20, train loss: 0.4977901875972748\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 170, batch:0, train loss: 0.5533949732780457\n",
      "epoch: 170, batch:10, train loss: 0.8100584149360657\n",
      "epoch: 170, batch:20, train loss: 0.49801144003868103\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 171, batch:0, train loss: 0.5535331964492798\n",
      "epoch: 171, batch:10, train loss: 0.8099397420883179\n",
      "epoch: 171, batch:20, train loss: 0.49797508120536804\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 172, batch:0, train loss: 0.5535073280334473\n",
      "epoch: 172, batch:10, train loss: 0.8099953532218933\n",
      "epoch: 172, batch:20, train loss: 0.49791350960731506\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 173, batch:0, train loss: 0.5534693002700806\n",
      "epoch: 173, batch:10, train loss: 0.8100436925888062\n",
      "epoch: 173, batch:20, train loss: 0.4978735148906708\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 174, batch:0, train loss: 0.5534449815750122\n",
      "epoch: 174, batch:10, train loss: 0.8100720643997192\n",
      "epoch: 174, batch:20, train loss: 0.4978514313697815\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 175, batch:0, train loss: 0.5534316897392273\n",
      "epoch: 175, batch:10, train loss: 0.8100868463516235\n",
      "epoch: 175, batch:20, train loss: 0.49784013628959656\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 176, batch:0, train loss: 0.5534248948097229\n",
      "epoch: 176, batch:10, train loss: 0.8100941777229309\n",
      "epoch: 176, batch:20, train loss: 0.4978350102901459\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 177, batch:0, train loss: 0.5534218549728394\n",
      "epoch: 177, batch:10, train loss: 0.8100975155830383\n",
      "epoch: 177, batch:20, train loss: 0.49783310294151306\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 178, batch:0, train loss: 0.5534206628799438\n",
      "epoch: 178, batch:10, train loss: 0.8100990056991577\n",
      "epoch: 178, batch:20, train loss: 0.4978329837322235\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 179, batch:0, train loss: 0.5534205436706543\n",
      "epoch: 179, batch:10, train loss: 0.8100996017456055\n",
      "epoch: 179, batch:20, train loss: 0.4980856478214264\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 180, batch:0, train loss: 0.55357825756073\n",
      "epoch: 180, batch:10, train loss: 0.8099699020385742\n",
      "epoch: 180, batch:20, train loss: 0.4981466829776764\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 181, batch:0, train loss: 0.553613007068634\n",
      "epoch: 181, batch:10, train loss: 0.8099833726882935\n",
      "epoch: 181, batch:20, train loss: 0.49807009100914\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 182, batch:0, train loss: 0.5535640120506287\n",
      "epoch: 182, batch:10, train loss: 0.810064435005188\n",
      "epoch: 182, batch:20, train loss: 0.49799907207489014\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 183, batch:0, train loss: 0.5535203814506531\n",
      "epoch: 183, batch:10, train loss: 0.8101199269294739\n",
      "epoch: 183, batch:20, train loss: 0.49795660376548767\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 184, batch:0, train loss: 0.5534946322441101\n",
      "epoch: 184, batch:10, train loss: 0.8101499676704407\n",
      "epoch: 184, batch:20, train loss: 0.49793392419815063\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 185, batch:0, train loss: 0.5534809827804565\n",
      "epoch: 185, batch:10, train loss: 0.8101649284362793\n",
      "epoch: 185, batch:20, train loss: 0.4979226887226105\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 186, batch:0, train loss: 0.5534742474555969\n",
      "epoch: 186, batch:10, train loss: 0.8101717233657837\n",
      "epoch: 186, batch:20, train loss: 0.49791765213012695\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 187, batch:0, train loss: 0.5534711480140686\n",
      "epoch: 187, batch:10, train loss: 0.8101744651794434\n",
      "epoch: 187, batch:20, train loss: 0.49791574478149414\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 188, batch:0, train loss: 0.5534700751304626\n",
      "epoch: 188, batch:10, train loss: 0.8101752996444702\n",
      "epoch: 188, batch:20, train loss: 0.4979034960269928\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 189, batch:0, train loss: 0.5534628033638\n",
      "epoch: 189, batch:10, train loss: 0.8101733922958374\n",
      "epoch: 189, batch:20, train loss: 0.4979066848754883\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 190, batch:0, train loss: 0.5534647703170776\n",
      "epoch: 190, batch:10, train loss: 0.8101705312728882\n",
      "epoch: 190, batch:20, train loss: 0.4979097545146942\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 191, batch:0, train loss: 0.5534666180610657\n",
      "epoch: 191, batch:10, train loss: 0.8101687431335449\n",
      "epoch: 191, batch:20, train loss: 0.49791255593299866\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 192, batch:0, train loss: 0.5534682869911194\n",
      "epoch: 192, batch:10, train loss: 0.8101674914360046\n",
      "epoch: 192, batch:20, train loss: 0.4979149401187897\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 193, batch:0, train loss: 0.5534697771072388\n",
      "epoch: 193, batch:10, train loss: 0.8101667165756226\n",
      "epoch: 193, batch:20, train loss: 0.4979172945022583\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 194, batch:0, train loss: 0.5534712672233582\n",
      "epoch: 194, batch:10, train loss: 0.8101663589477539\n",
      "epoch: 194, batch:20, train loss: 0.49791961908340454\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 195, batch:0, train loss: 0.5534726977348328\n",
      "epoch: 195, batch:10, train loss: 0.81016606092453\n",
      "epoch: 195, batch:20, train loss: 0.4979220926761627\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 196, batch:0, train loss: 0.5534741878509521\n",
      "epoch: 196, batch:10, train loss: 0.81016606092453\n",
      "epoch: 196, batch:20, train loss: 0.49792444705963135\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 197, batch:0, train loss: 0.5534757375717163\n",
      "epoch: 197, batch:10, train loss: 0.81016606092453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 197, batch:20, train loss: 0.4979269206523895\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 198, batch:0, train loss: 0.5534771680831909\n",
      "epoch: 198, batch:10, train loss: 0.8101661801338196\n",
      "epoch: 198, batch:20, train loss: 0.4979293644428253\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n",
      "epoch: 199, batch:0, train loss: 0.5534787774085999\n",
      "epoch: 199, batch:10, train loss: 0.8101664781570435\n",
      "epoch: 199, batch:20, train loss: 0.49793195724487305\n",
      "开始测试\n",
      "train Acc： 0.6555023923444976 \n",
      "test Acc： 0.34 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "\n",
    "for j in range(num_epochs):\n",
    "\n",
    "    for i in range(batch_number):\n",
    "        \n",
    "        # 如果不是该epoch里面最后一组数据\n",
    "        if i != (batch_number - 1):\n",
    "#             print(y[i * batch_size: (i+1) * batch_size, :].shape)\n",
    "            batch_y = y[i * batch_size: (i+1) * batch_size, :]\n",
    "            batch_x = x[i * batch_size: (i+1) * batch_size, :]\n",
    "        else:\n",
    "#             print(y[i * batch_size: , :].shape)\n",
    "            batch_y = y[i * batch_size: , :]\n",
    "            batch_x = x[i * batch_size: , :]\n",
    "        \n",
    "        # 前向传播\n",
    "        out = net(batch_x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, batch_y)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch: {j}, batch:{i}, train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "    \n",
    "\n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result_train = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "        result_test = (np.argmax(net(test_x_orig).detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "        \n",
    "\n",
    "        print(\"train Acc：\", np.mean(result_train),'\\ntest Acc：', np.mean(result_test), '\\n')\n",
    "    \n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result_train))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务8: \n",
    "\n",
    "使用pytorch内置的工具,构建一个数据读取器(dataloader)来读取数据 \\\n",
    "**(代码写在下方)**\n",
    "\n",
    "参考资料:\n",
    "1. 两文读懂PyTorch中Dataset与DataLoader（一）打造自己的数据集\n",
    "https://zhuanlan.zhihu.com/p/105507334\n",
    "2. 两文读懂PyTorch中Dataset与DataLoader（二）理解DataLoader源码\n",
    "https://zhuanlan.zhihu.com/p/105578087\n",
    "\n",
    "tips: \n",
    "ndarray的数据长度用.shape来提取 \\\n",
    "\n",
    "\n",
    "首先需要根据参考资料1,自定义Dataset类型 \\\n",
    "再根据参考资料2,将自定义Dataset类型创建出的对象,传到torch.utils.data.DataLoader \\\n",
    "\n",
    "参考资料2里的sampler,不需要使用.该任务仅仅需要做DataLoader \\\n",
    "\n",
    "参考资料1里的数据是从文件夹里面读取图片,而我们提供的数据,是已经转化为ndarray, \\\n",
    "因此在做自定义dataset不需要再像参考资料那样从文件夹中读取图片和标签放到ndarray \\\n",
    "主要需要考虑如何从ndarray数组里面取出单个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 64, 64, 3])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([9, 64, 64, 3])\n",
      "torch.Size([9, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Cat_Dataset(data.Dataset):\n",
    "    def __init__(self, img_ndarray, label_ndarray):\n",
    "        self.img_ndarray = img_ndarray\n",
    "        self.label_ndarray = label_ndarray\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.img_ndarray[index-1, :]\n",
    "        label = self.label_ndarray[index-1, :]\n",
    "        \n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_ndarray.shape[0]\n",
    "    \n",
    "cat_dataset = Cat_Dataset(x, y)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(cat_dataset, batch_size = 10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader迭代产生训练数据提供给模型\n",
    "for i in range(1):\n",
    "    for index,(img,label) in enumerate(dataloader):\n",
    "        print(img.shape)\n",
    "        print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 0, batch:10, train loss: 2.073611259460449\n",
      "epoch: 0, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 1, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 1, batch:10, train loss: 2.073611259460449\n",
      "epoch: 1, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 2, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 2, batch:10, train loss: 2.073611259460449\n",
      "epoch: 2, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 3, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 3, batch:10, train loss: 2.073611259460449\n",
      "epoch: 3, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 4, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 4, batch:10, train loss: 2.073611259460449\n",
      "epoch: 4, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 5, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 5, batch:10, train loss: 2.073611259460449\n",
      "epoch: 5, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 6, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 6, batch:10, train loss: 2.073611259460449\n",
      "epoch: 6, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 7, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 7, batch:10, train loss: 2.073611259460449\n",
      "epoch: 7, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 8, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 8, batch:10, train loss: 2.073611259460449\n",
      "epoch: 8, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 9, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 9, batch:10, train loss: 2.073611259460449\n",
      "epoch: 9, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 10, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 10, batch:10, train loss: 2.073611259460449\n",
      "epoch: 10, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 11, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 11, batch:10, train loss: 2.073611259460449\n",
      "epoch: 11, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 12, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 12, batch:10, train loss: 2.073611259460449\n",
      "epoch: 12, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 13, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 13, batch:10, train loss: 2.073611259460449\n",
      "epoch: 13, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 14, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 14, batch:10, train loss: 2.073611259460449\n",
      "epoch: 14, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 15, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 15, batch:10, train loss: 2.073611259460449\n",
      "epoch: 15, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 16, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 16, batch:10, train loss: 2.073611259460449\n",
      "epoch: 16, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 17, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 17, batch:10, train loss: 2.073611259460449\n",
      "epoch: 17, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 18, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 18, batch:10, train loss: 2.073611259460449\n",
      "epoch: 18, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 19, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 19, batch:10, train loss: 2.073611259460449\n",
      "epoch: 19, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 20, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 20, batch:10, train loss: 2.073611259460449\n",
      "epoch: 20, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 21, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 21, batch:10, train loss: 2.073611259460449\n",
      "epoch: 21, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 22, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 22, batch:10, train loss: 2.073611259460449\n",
      "epoch: 22, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 23, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 23, batch:10, train loss: 2.073611259460449\n",
      "epoch: 23, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 24, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 24, batch:10, train loss: 2.073611259460449\n",
      "epoch: 24, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 25, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 25, batch:10, train loss: 2.073611259460449\n",
      "epoch: 25, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 26, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 26, batch:10, train loss: 2.073611259460449\n",
      "epoch: 26, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 27, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 27, batch:10, train loss: 2.073611259460449\n",
      "epoch: 27, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 28, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 28, batch:10, train loss: 2.073611259460449\n",
      "epoch: 28, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 29, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 29, batch:10, train loss: 2.073611259460449\n",
      "epoch: 29, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 30, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 30, batch:10, train loss: 2.073611259460449\n",
      "epoch: 30, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 31, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 31, batch:10, train loss: 2.073611259460449\n",
      "epoch: 31, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 32, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 32, batch:10, train loss: 2.073611259460449\n",
      "epoch: 32, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 33, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 33, batch:10, train loss: 2.073611259460449\n",
      "epoch: 33, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 34, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 34, batch:10, train loss: 2.073611259460449\n",
      "epoch: 34, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 35, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 35, batch:10, train loss: 2.073611259460449\n",
      "epoch: 35, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 36, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 36, batch:10, train loss: 2.073611259460449\n",
      "epoch: 36, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 37, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 37, batch:10, train loss: 2.073611259460449\n",
      "epoch: 37, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 38, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 38, batch:10, train loss: 2.073611259460449\n",
      "epoch: 38, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 39, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 39, batch:10, train loss: 2.073611259460449\n",
      "epoch: 39, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 40, batch:0, train loss: 2.1870334148406982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, batch:10, train loss: 2.073611259460449\n",
      "epoch: 40, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 41, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 41, batch:10, train loss: 2.073611259460449\n",
      "epoch: 41, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 42, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 42, batch:10, train loss: 2.073611259460449\n",
      "epoch: 42, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 43, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 43, batch:10, train loss: 2.073611259460449\n",
      "epoch: 43, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 44, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 44, batch:10, train loss: 2.073611259460449\n",
      "epoch: 44, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 45, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 45, batch:10, train loss: 2.073611259460449\n",
      "epoch: 45, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 46, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 46, batch:10, train loss: 2.073611259460449\n",
      "epoch: 46, batch:20, train loss: 4.308969020843506\n",
      "开始测试\n",
      "train Acc： 0.3397129186602871 \n",
      "test Acc： 0.68 \n",
      "\n",
      "epoch: 47, batch:0, train loss: 2.1870334148406982\n",
      "epoch: 47, batch:10, train loss: 2.073611259460449\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34452/3430073474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# 更新参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# 清空梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                    maximize=group['maximize'])\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf1\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "\n",
    "for j in range(num_epochs):\n",
    "\n",
    "# 原来不用dataloader时\n",
    "\n",
    "#     for i in range(batch_number):\n",
    "        \n",
    "#         # 如果不是该epoch里面最后一组数据\n",
    "#         if i != (batch_number - 1):\n",
    "# #             print(y[i * batch_size: (i+1) * batch_size, :].shape)\n",
    "#             batch_y = y[i * batch_size: (i+1) * batch_size, :]\n",
    "#             batch_x = x[i * batch_size: (i+1) * batch_size, :]\n",
    "#         else:\n",
    "# #             print(y[i * batch_size: , :].shape)\n",
    "#             batch_y = y[i * batch_size: , :]\n",
    "#             batch_x = x[i * batch_size: , :]\n",
    "        \n",
    "# 用torch的 dataloader 替换后\n",
    "    for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        \n",
    "        # 前向传播\n",
    "        out = net(batch_x)\n",
    "\n",
    "        # 计算loss\n",
    "        ls = loss(out, batch_y)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch: {j}, batch:{i}, train loss:\", ls.item())\n",
    "\n",
    "        # 反向传播\n",
    "        ls.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "    \n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result_train = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "        result_test = (np.argmax(net(test_x_orig).detach().numpy(),axis=1) == np.argmax(y_test_true.numpy(),axis=1))\n",
    "        \n",
    "\n",
    "        print(\"train Acc：\", np.mean(result_train),'\\ntest Acc：', np.mean(result_test), '\\n')\n",
    "    \n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result_train))\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
