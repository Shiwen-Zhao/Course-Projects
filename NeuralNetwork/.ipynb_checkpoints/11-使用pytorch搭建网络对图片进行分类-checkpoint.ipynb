{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy的ndarray和Tensor的区别\n",
    "\n",
    "参考资料：\n",
    "PyTorch中torch.tensor与torch.Tensor的区别详解\n",
    "https://www.jb51.net/article/186764.htm\n",
    "\n",
    "**注：pytorch里的Tensor与tensorflow无任何关系，仅仅只是pytorch里面的一个数据类型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "创建一个Tensor并设置requires_grad=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_zeros: \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]], requires_grad=True)\n",
      "x_ones: \n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "x_randn: \n",
      " tensor([[-1.7271,  0.8246],\n",
      "        [-0.6548,  0.1346]], requires_grad=True)\n",
      "\n",
      "x.grad_fn:  None\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "# 分别创建全0、全1、随机的tensor\n",
    "x_zeros = torch.zeros(2, 2, requires_grad=True)\n",
    "x_ones = torch.ones(2, 2, requires_grad=True)\n",
    "x_randn = torch.randn(2, 2, requires_grad=True) # 缺失情况下默认 requires_grad = False\n",
    "\n",
    "print(\"x_zeros: \\n\", x_zeros)\n",
    "print(\"x_ones: \\n\", x_ones)\n",
    "print(\"x_randn: \\n\", x_randn)\n",
    "print()\n",
    "print(\"x.grad_fn: \",x_ones.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy的ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_zeros_np: \n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "x_ones_np_np: \n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "x_rand_np_np: \n",
      " [[0.27921345 0.5446601 ]\n",
      " [0.81431251 0.05515949]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分别创建全0、全1、随机的ndarray数组\n",
    "\n",
    "x_zeros_np = np.zeros((2, 2))\n",
    "x_ones_np = np.ones((2, 2))\n",
    "x_rand_np = np.random.randn(2, 2)\n",
    "\n",
    "print(\"x_zeros_np: \\n\", x_zeros_np)\n",
    "print(\"x_ones_np_np: \\n\", x_ones_np)\n",
    "print(\"x_rand_np_np: \\n\", x_rand_np)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy array 和 Tensor 相互转换\n",
    "\n",
    "参考资料：\n",
    "torch里面的Tensor、as_tensor、tensor以及from_numpy究竟有何区别? \\\n",
    "https://www.136.la/jingpin/show-208975.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Tensor 转 numpy array\n",
    "# RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "# a = x_zeros.numpy()\n",
    "\n",
    "a = x_zeros.detach().numpy()\n",
    "\n",
    "a_tensor = torch.from_numpy(a)\n",
    "a_tensor = torch.tensor(a)\n",
    "a_tensor = torch.as_tensor(a)\n",
    "a_tensor = torch.Tensor(a)\n",
    "\n",
    "print(a_tensor)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "y的ndarray.grad_fn:  <AddBackward0 object at 0x000002625BF38748>\n"
     ]
    }
   ],
   "source": [
    "y = x_ones + 2\n",
    "print(y)\n",
    "print(\"y的ndarray.grad_fn: \",y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有grad_fn, 而y是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn。\n",
    "\n",
    "像x这种直接创建的称为叶子节点，叶子节点对应的grad_fn是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x_ones.is_leaf, y.is_leaf) # True False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来点复杂度运算操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "\n",
    "out_mean = z.mean()\n",
    "z_sum = z.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 要求查Tensor.sum()的函数使用方法去是实现以下要求\n",
    "\n",
    "### 对 z.sum(None) 中的None进行填空\n",
    "\n",
    "### (查询函数需要传入的参数)\n",
    "### 参考: https://blog.csdn.net/hahameier/article/details/103742831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor(108., grad_fn=<SumBackward0>)\n",
      "\n",
      "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
      "torch.Size([2])\n",
      "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 第1个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_sum_dim1 = z.sum(None)\n",
    "# 第0个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_sum_dim0 = z.sum(None)\n",
    "\n",
    "print(z)\n",
    "print(out_mean)\n",
    "print(z_sum)\n",
    "\n",
    "print()\n",
    "print(z_sum_dim1)\n",
    "print(z_sum_dim1.shape)\n",
    "print(z_sum_dim0)\n",
    "print(z_sum_dim0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期望输出\n",
    "\n",
    "```python\n",
    "tensor([[27., 27.],\n",
    "        [27., 27.]], grad_fn=<MulBackward0>)\n",
    "    \n",
    "tensor(27., grad_fn=<MeanBackward0>)\n",
    "tensor(108., grad_fn=<SumBackward0>)\n",
    "\n",
    "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
    "torch.Size([2])\n",
    "tensor([54., 54.], grad_fn=<SumBackward1>)\n",
    "torch.Size([2])\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "z_re: \n",
      " tensor([[[27.]],\n",
      "\n",
      "        [[27.]],\n",
      "\n",
      "        [[27.]],\n",
      "\n",
      "        [[27.]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "z_re_sum_dim0: \n",
      " tensor([[108.]], grad_fn=<SumBackward1>)\n",
      "z_re_sum_dim1: \n",
      " tensor([[27.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [27.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z_re = z.reshape(4,1,1)\n",
    "# （1，1）\n",
    "# （4，1）\n",
    "print()\n",
    "print(\"z_re: \\n\",z_re)\n",
    "\n",
    "# 第0个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_re_sum_dim0 = z_re.sum(None)\n",
    "print(\"z_re_sum_dim0: \\n\",z_re_sum_dim0)\n",
    "\n",
    "# 第1个维度里的所有元素相加 （代码填空，仅需填入一个参数替换掉None）\n",
    "z_re_sum_dim1 = z_re.sum(None)\n",
    "print(\"z_re_sum_dim1: \\n\",z_re_sum_dim1)\n",
    "\n",
    "\n",
    "# 所有的第二个维度的元素相加\n",
    "# 第二个维度的结构: [[27.]]\n",
    "# 第二个维度数组中包含的所有元素: [27.]\n",
    "\n",
    "# 所有的第二个维度的元素相加结果\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "# [[27.]] -> [27.]\n",
    "\n",
    "# 最后变成：\n",
    "# [[27.],\n",
    "#  [27.],\n",
    "#  [27.],\n",
    "#  [27.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期望输出\n",
    "\n",
    "```python\n",
    "z_re: \n",
    " tensor([[[27.]],\n",
    "\n",
    "        [[27.]],\n",
    "\n",
    "        [[27.]],\n",
    "\n",
    "        [[27.]]], grad_fn=<ReshapeAliasBackward0>)\n",
    "z_re_sum_dim0: \n",
    " tensor([[108.]], grad_fn=<SumBackward1>)\n",
    "z_re_sum_dim1: \n",
    " tensor([[27.],\n",
    "        [27.],\n",
    "        [27.],\n",
    "        [27.]], grad_fn=<SumBackward1>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过.requires_grad_()来用in-place的方式改变requires_grad属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000002627C2FAF08>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "因为out是一个标量，所以调用backward()时不需要指定求导变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward() # 等价于 b.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看out关于a的梯度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9810,  2.5706],\n",
      "        [-1.1622,  2.0829]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x_ones.sum()\n",
    "out2.backward()\n",
    "print(x_ones)\n",
    "\n",
    "# out3 = x_ones.sum()\n",
    "x_ones.grad.data.zero_()\n",
    "# out3.backward()\n",
    "print(x_ones.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图 Computer Graph(CG) 是用来描述运算的有向无环图\n",
    "\n",
    "计算图（Computer Graph）是用来描述运算的有向无环图\n",
    "CG包括两个主要元素：结点（Node）、边（Edge）\n",
    "\n",
    "Node表示数据\n",
    "Edge表示运算\n",
    "如果想使用非leaf结点的梯度。则需要在反向传播之前调用.retain_grad() (不用记)\n",
    "\n",
    "**CG作用：方便梯度求取及反向传播**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例\n",
    "#### y = (x+w)*w，则当w=1，x=2时，y对w的grad是多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.])\n",
      "<AddBackward0 object at 0x000002627C338D48>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# b = torch.tensor([6.], requires_grad=True)\n",
    "w = torch.tensor([5.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "y = torch.mul(a, w)\n",
    "\n",
    "y = x + (w * w)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二层神经网络的从零开始实现\n",
    "\n",
    "下面，我们一起来动手实现二层神经网络。首先导入实现所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from func.dnn_app_utils_v2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获取和读取数据\n",
    "\n",
    "这里继续使用上一次实验使用的数据集,但本次使用构建神经网络的框架pytorch来进行构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "# Example of a picture\n",
    "index = 7\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\n",
    "\n",
    "# 标准化\n",
    "\n",
    "train_x_orig = train_x_orig/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义模型参数\n",
    "\n",
    "我们提供的数据集中图像形状为 $64 \\times 64 \\times 3$，类别数为2。本节中我们依然使用长度为 $64 \\times 64 \\times 3 = 12288$ 的向量表示每一张图像。因此，输入个数为$12288$，输出类别个数为2。实验中，我们设超参数隐藏单元个数为256。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens, num_hiddens2 = 12288, 2, 256, 128\n",
    "\n",
    "def params_init_func():\n",
    "    W1_np = np.random.normal(0, 0.01, (num_inputs, num_hiddens))\n",
    "    W2_np = np.random.normal(0, 0.01, (num_hiddens, num_outputs))\n",
    "\n",
    "    W1 = torch.tensor(W1_np, dtype=torch.float, requires_grad = True)\n",
    "    b1 = torch.zeros(num_hiddens, dtype=torch.float, requires_grad = True)\n",
    "\n",
    "    W2 = torch.tensor(W2_np, dtype=torch.float, requires_grad = True)\n",
    "    b2 = torch.zeros(num_outputs, dtype=torch.float, requires_grad = True)\n",
    "\n",
    "    params = [W1, b1, W2, b2]\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义激活函数(该步骤非必须)\n",
    "\n",
    "这里我们使用基础的`max`函数来实现ReLU，而非直接调用`relu`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义模型\n",
    "\n",
    "我们通过`view`函数将每张原始图像改成长度为`num_inputs`的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_inputs, num_outputs, num_hiddens,num_hiddens2 = 12288, 2, 256, 128\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, params_init_func, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        params = params_init_func()\n",
    "        \n",
    "        for param in params:\n",
    "            param.requires_grad_(requires_grad=True)\n",
    "        \n",
    "#         # 确保都是fp32\n",
    "#         nn.Parameter(torch.Tensor)\n",
    "        \n",
    "        self.W1 = nn.Parameter(params[0].float())\n",
    "        self.b1 = nn.Parameter(params[1].float())\n",
    "\n",
    "        self.W2 = nn.Parameter(params[2].float())\n",
    "        self.b2 = nn.Parameter(params[3].float())\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view((-1, num_inputs))\n",
    "        \n",
    "        H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "        out = torch.matmul(H, self.W2) + self.b2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义损失函数\n",
    "\n",
    "为了得到更好的数值稳定性，我们直接使用PyTorch提供的包括softmax运算和交叉熵损失计算的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练模型\n",
    "\n",
    "构建模型,定义optimizer,传递模型需要优化的参数给optimizer,然后开始训练,我们在这里设超参数迭代周期数为500，学习率为0.01。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 0.6890744566917419\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 1 train loss: 0.6718174815177917\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 2 train loss: 0.6627060770988464\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 3 train loss: 0.6576961278915405\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 4 train loss: 0.6548213958740234\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 5 train loss: 0.6530501842498779\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 6 train loss: 0.6518672704696655\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 7 train loss: 0.6509720087051392\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 8 train loss: 0.6502195596694946\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 9 train loss: 0.6495435237884521\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 10 train loss: 0.6489176750183105\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 11 train loss: 0.6483206748962402\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 12 train loss: 0.647745668888092\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 13 train loss: 0.6471775770187378\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 14 train loss: 0.6466094851493835\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 15 train loss: 0.6460389494895935\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 16 train loss: 0.6454674601554871\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 17 train loss: 0.6448970437049866\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 18 train loss: 0.6443307399749756\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 19 train loss: 0.6437655091285706\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 20 train loss: 0.6431982517242432\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 21 train loss: 0.6426321268081665\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 22 train loss: 0.6420623660087585\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 23 train loss: 0.641484797000885\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 24 train loss: 0.6408979296684265\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 25 train loss: 0.6403197050094604\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 26 train loss: 0.6397410035133362\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 27 train loss: 0.6391553282737732\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 28 train loss: 0.6385663747787476\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 29 train loss: 0.6379783153533936\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 30 train loss: 0.6373897194862366\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 31 train loss: 0.6367998123168945\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 32 train loss: 0.6362066864967346\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 33 train loss: 0.6356046199798584\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 34 train loss: 0.6349997520446777\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 35 train loss: 0.63438481092453\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 36 train loss: 0.6337667107582092\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 37 train loss: 0.6331413388252258\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 38 train loss: 0.6325124502182007\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 39 train loss: 0.631876528263092\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 40 train loss: 0.6312288641929626\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 41 train loss: 0.6305721402168274\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 42 train loss: 0.6299190521240234\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 43 train loss: 0.629265308380127\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 44 train loss: 0.6286056637763977\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 45 train loss: 0.6279394626617432\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 46 train loss: 0.6272693872451782\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 47 train loss: 0.6265926361083984\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 48 train loss: 0.6259109377861023\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 49 train loss: 0.6252248883247375\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 50 train loss: 0.6245285272598267\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 51 train loss: 0.6238259673118591\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 52 train loss: 0.6231176853179932\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 53 train loss: 0.6224026679992676\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 54 train loss: 0.6216816306114197\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 55 train loss: 0.6209567785263062\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 56 train loss: 0.6202248930931091\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 57 train loss: 0.6194847822189331\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 58 train loss: 0.6187390685081482\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 59 train loss: 0.6179874539375305\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 60 train loss: 0.6172278523445129\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 61 train loss: 0.6164675354957581\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 62 train loss: 0.6157011985778809\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 63 train loss: 0.6149265766143799\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 64 train loss: 0.6141455769538879\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 65 train loss: 0.6133589148521423\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 66 train loss: 0.6125649809837341\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 67 train loss: 0.6117643117904663\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 68 train loss: 0.610956609249115\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 69 train loss: 0.610140860080719\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 70 train loss: 0.6093159317970276\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 71 train loss: 0.608482837677002\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 72 train loss: 0.607641339302063\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 73 train loss: 0.6067908406257629\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 74 train loss: 0.6059370636940002\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 75 train loss: 0.6050758957862854\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 76 train loss: 0.6042055487632751\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 77 train loss: 0.6033250689506531\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 78 train loss: 0.6024408340454102\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 79 train loss: 0.6015530824661255\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 80 train loss: 0.6006583571434021\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 81 train loss: 0.5997571349143982\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 82 train loss: 0.5988476872444153\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 83 train loss: 0.5979369878768921\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 84 train loss: 0.5970139503479004\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 85 train loss: 0.5960882306098938\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 86 train loss: 0.5951533913612366\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 87 train loss: 0.5942125916481018\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 88 train loss: 0.5932667851448059\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 89 train loss: 0.5923126339912415\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 90 train loss: 0.5913536548614502\n",
      "开始测试\n",
      "Acc： 0.6555023923444976 \n",
      "\n",
      "epoch: 91 train loss: 0.590388834476471\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 92 train loss: 0.5894191861152649\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 93 train loss: 0.588441014289856\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 94 train loss: 0.5874560475349426\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 95 train loss: 0.586469292640686\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 96 train loss: 0.5854708552360535\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 97 train loss: 0.5844693183898926\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 98 train loss: 0.58346027135849\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 99 train loss: 0.5824472308158875\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 100 train loss: 0.5814319252967834\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 101 train loss: 0.5804055333137512\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 102 train loss: 0.5793784856796265\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 103 train loss: 0.5783452391624451\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 104 train loss: 0.5773041844367981\n",
      "开始测试\n",
      "Acc： 0.6650717703349283 \n",
      "\n",
      "epoch: 105 train loss: 0.5762603878974915\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 106 train loss: 0.5752134919166565\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 107 train loss: 0.574155867099762\n",
      "开始测试\n",
      "Acc： 0.6698564593301436 \n",
      "\n",
      "epoch: 108 train loss: 0.5730942487716675\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 109 train loss: 0.5720254778862\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 110 train loss: 0.570951521396637\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 111 train loss: 0.5698738694190979\n",
      "开始测试\n",
      "Acc： 0.6746411483253588 \n",
      "\n",
      "epoch: 112 train loss: 0.5687891244888306\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 113 train loss: 0.5677036643028259\n",
      "开始测试\n",
      "Acc： 0.6794258373205742 \n",
      "\n",
      "epoch: 114 train loss: 0.5666073560714722\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 115 train loss: 0.5655065178871155\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 116 train loss: 0.5643983483314514\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 117 train loss: 0.5632836222648621\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 118 train loss: 0.5621653199195862\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 119 train loss: 0.5610471367835999\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 120 train loss: 0.5599095821380615\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 121 train loss: 0.5587825179100037\n",
      "开始测试\n",
      "Acc： 0.69377990430622 \n",
      "\n",
      "epoch: 122 train loss: 0.5576415061950684\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 123 train loss: 0.5565061569213867\n",
      "开始测试\n",
      "Acc： 0.6889952153110048 \n",
      "\n",
      "epoch: 124 train loss: 0.5553498864173889\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 125 train loss: 0.5541995167732239\n",
      "开始测试\n",
      "Acc： 0.6985645933014354 \n",
      "\n",
      "epoch: 126 train loss: 0.553045392036438\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 127 train loss: 0.551889181137085\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 128 train loss: 0.5507315993309021\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 129 train loss: 0.5495606660842896\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 130 train loss: 0.5483892560005188\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 131 train loss: 0.5472132563591003\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 132 train loss: 0.5460374355316162\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 133 train loss: 0.5448517203330994\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 134 train loss: 0.5436644554138184\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 135 train loss: 0.5424756407737732\n",
      "开始测试\n",
      "Acc： 0.7033492822966507 \n",
      "\n",
      "epoch: 136 train loss: 0.5412705540657043\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 137 train loss: 0.5400719046592712\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 138 train loss: 0.538878321647644\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 139 train loss: 0.5376772880554199\n",
      "开始测试\n",
      "Acc： 0.7129186602870813 \n",
      "\n",
      "epoch: 140 train loss: 0.5364667773246765\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 141 train loss: 0.535260021686554\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 142 train loss: 0.5340444445610046\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 143 train loss: 0.5328198075294495\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 144 train loss: 0.5315931439399719\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 145 train loss: 0.5303657650947571\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 146 train loss: 0.5291262269020081\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 147 train loss: 0.5278961062431335\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 148 train loss: 0.5266680121421814\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 149 train loss: 0.5254312753677368\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 150 train loss: 0.524183452129364\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 151 train loss: 0.5229448080062866\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 152 train loss: 0.5216911435127258\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 153 train loss: 0.520434558391571\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 154 train loss: 0.519176185131073\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 155 train loss: 0.5179151296615601\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 156 train loss: 0.5166462659835815\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 157 train loss: 0.5153791308403015\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 158 train loss: 0.5141064524650574\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 159 train loss: 0.5128416419029236\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 160 train loss: 0.5115500092506409\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 161 train loss: 0.5102674961090088\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 162 train loss: 0.5089967846870422\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 163 train loss: 0.507714033126831\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 164 train loss: 0.5064091086387634\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 165 train loss: 0.5051291584968567\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 166 train loss: 0.5038442611694336\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 167 train loss: 0.5025661587715149\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 168 train loss: 0.5012434720993042\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 169 train loss: 0.4999423325061798\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 170 train loss: 0.4986376464366913\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 171 train loss: 0.4973611831665039\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 172 train loss: 0.49607381224632263\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 173 train loss: 0.4948030710220337\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 174 train loss: 0.493461936712265\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 175 train loss: 0.49216514825820923\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 176 train loss: 0.49091410636901855\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 177 train loss: 0.48963463306427\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 178 train loss: 0.48836690187454224\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 179 train loss: 0.48712530732154846\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 180 train loss: 0.4858352839946747\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 181 train loss: 0.48457297682762146\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 182 train loss: 0.4832861125469208\n",
      "开始测试\n",
      "Acc： 0.7894736842105263 \n",
      "\n",
      "epoch: 183 train loss: 0.4820447266101837\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 184 train loss: 0.48076820373535156\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 185 train loss: 0.4795878529548645\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 186 train loss: 0.4783436954021454\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 187 train loss: 0.4771778881549835\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 188 train loss: 0.47606152296066284\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 189 train loss: 0.4748981297016144\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 190 train loss: 0.4738474190235138\n",
      "开始测试\n",
      "Acc： 0.8038277511961722 \n",
      "\n",
      "epoch: 191 train loss: 0.47281017899513245\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 192 train loss: 0.4718470275402069\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 193 train loss: 0.47087791562080383\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 194 train loss: 0.469991534948349\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 195 train loss: 0.46935713291168213\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 196 train loss: 0.46900978684425354\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 197 train loss: 0.46868035197257996\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 198 train loss: 0.4687173068523407\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 199 train loss: 0.46867242455482483\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 200 train loss: 0.46867498755455017\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 201 train loss: 0.469084769487381\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 202 train loss: 0.46918168663978577\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 203 train loss: 0.46960917115211487\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 204 train loss: 0.4704899787902832\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 205 train loss: 0.4713347852230072\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 206 train loss: 0.47228455543518066\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 207 train loss: 0.47261130809783936\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 208 train loss: 0.47480306029319763\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 209 train loss: 0.47516053915023804\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 210 train loss: 0.4768187701702118\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 211 train loss: 0.47600841522216797\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 212 train loss: 0.47722071409225464\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 213 train loss: 0.4756631851196289\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 214 train loss: 0.4770282804965973\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 215 train loss: 0.47531601786613464\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 216 train loss: 0.47679221630096436\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 217 train loss: 0.4747697114944458\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 218 train loss: 0.47633805871009827\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 219 train loss: 0.4735192060470581\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 220 train loss: 0.47585171461105347\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 221 train loss: 0.4736705720424652\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 222 train loss: 0.4757731258869171\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 223 train loss: 0.4732172191143036\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 224 train loss: 0.47587183117866516\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 225 train loss: 0.4733680784702301\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 226 train loss: 0.4756629168987274\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 227 train loss: 0.4707576334476471\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 228 train loss: 0.4742354154586792\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 229 train loss: 0.47116416692733765\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 230 train loss: 0.4736403822898865\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 231 train loss: 0.4689035713672638\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 232 train loss: 0.472187340259552\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 233 train loss: 0.46942538022994995\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 234 train loss: 0.4725194573402405\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 235 train loss: 0.46727752685546875\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 236 train loss: 0.4723890721797943\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 237 train loss: 0.469946026802063\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 238 train loss: 0.4739428758621216\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 239 train loss: 0.4679486155509949\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 240 train loss: 0.4717312455177307\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 241 train loss: 0.46735069155693054\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 242 train loss: 0.470353364944458\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 243 train loss: 0.46411943435668945\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 244 train loss: 0.4677289128303528\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 245 train loss: 0.462973415851593\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 246 train loss: 0.4671054184436798\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 247 train loss: 0.46267688274383545\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 248 train loss: 0.4673113226890564\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 249 train loss: 0.46157753467559814\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 250 train loss: 0.46533700823783875\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 251 train loss: 0.46011883020401\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 252 train loss: 0.4654628038406372\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 253 train loss: 0.46017947793006897\n",
      "开始测试\n",
      "Acc： 0.7177033492822966 \n",
      "\n",
      "epoch: 254 train loss: 0.46455681324005127\n",
      "开始测试\n",
      "Acc： 0.8325358851674641 \n",
      "\n",
      "epoch: 255 train loss: 0.45776239037513733\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 256 train loss: 0.46242669224739075\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 257 train loss: 0.45715606212615967\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 258 train loss: 0.4610362946987152\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 259 train loss: 0.45307406783103943\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 260 train loss: 0.45839565992355347\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 261 train loss: 0.45455402135849\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 262 train loss: 0.4594104588031769\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 263 train loss: 0.4509309232234955\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 264 train loss: 0.4568331837654114\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 265 train loss: 0.4533980190753937\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 266 train loss: 0.45879703760147095\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 267 train loss: 0.4492046535015106\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 268 train loss: 0.4553055167198181\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 269 train loss: 0.4515073597431183\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 270 train loss: 0.4562234878540039\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 271 train loss: 0.44693514704704285\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 272 train loss: 0.4527035355567932\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 273 train loss: 0.4485613703727722\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 274 train loss: 0.45301496982574463\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 275 train loss: 0.44431033730506897\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 276 train loss: 0.45001187920570374\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 277 train loss: 0.44546493887901306\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 278 train loss: 0.449809193611145\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 279 train loss: 0.4407767057418823\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 280 train loss: 0.44697970151901245\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 281 train loss: 0.44224220514297485\n",
      "开始测试\n",
      "Acc： 0.722488038277512 \n",
      "\n",
      "epoch: 282 train loss: 0.4477675259113312\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 283 train loss: 0.4398825168609619\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 284 train loss: 0.44575950503349304\n",
      "开始测试\n",
      "Acc： 0.8421052631578947 \n",
      "\n",
      "epoch: 285 train loss: 0.4401117265224457\n",
      "开始测试\n",
      "Acc： 0.7272727272727273 \n",
      "\n",
      "epoch: 286 train loss: 0.4446369409561157\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 287 train loss: 0.4360382854938507\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 288 train loss: 0.44191351532936096\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 289 train loss: 0.4364217221736908\n",
      "开始测试\n",
      "Acc： 0.7320574162679426 \n",
      "\n",
      "epoch: 290 train loss: 0.44106972217559814\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 291 train loss: 0.4353610575199127\n",
      "开始测试\n",
      "Acc： 0.7368421052631579 \n",
      "\n",
      "epoch: 292 train loss: 0.43984255194664\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 293 train loss: 0.4325772821903229\n",
      "开始测试\n",
      "Acc： 0.7416267942583732 \n",
      "\n",
      "epoch: 294 train loss: 0.4361012578010559\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 295 train loss: 0.4290507137775421\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 296 train loss: 0.43401390314102173\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 297 train loss: 0.42832356691360474\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 298 train loss: 0.43310317397117615\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 299 train loss: 0.4275384545326233\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 300 train loss: 0.4329099953174591\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 301 train loss: 0.42645686864852905\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 302 train loss: 0.4316088557243347\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 303 train loss: 0.42573049664497375\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 304 train loss: 0.43017128109931946\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 305 train loss: 0.42273974418640137\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 306 train loss: 0.42777884006500244\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 307 train loss: 0.42139217257499695\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 308 train loss: 0.4263680577278137\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 309 train loss: 0.42147475481033325\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 310 train loss: 0.42609304189682007\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 311 train loss: 0.4189335107803345\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 312 train loss: 0.42423754930496216\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 313 train loss: 0.4189789295196533\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 314 train loss: 0.4233201742172241\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 315 train loss: 0.417572557926178\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 316 train loss: 0.42250657081604004\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 317 train loss: 0.4166417121887207\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 318 train loss: 0.4207289218902588\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 319 train loss: 0.41419726610183716\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 320 train loss: 0.41873952746391296\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 321 train loss: 0.4132195711135864\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 322 train loss: 0.4170689284801483\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 323 train loss: 0.4106806814670563\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 324 train loss: 0.4160327911376953\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 325 train loss: 0.4114423096179962\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 326 train loss: 0.41546308994293213\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 327 train loss: 0.40921008586883545\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 328 train loss: 0.4145117998123169\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 329 train loss: 0.4095376431941986\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 330 train loss: 0.4133364260196686\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 331 train loss: 0.40582263469696045\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 332 train loss: 0.4106414020061493\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 333 train loss: 0.4048187732696533\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 334 train loss: 0.40912190079689026\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 335 train loss: 0.403242826461792\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 336 train loss: 0.4080962538719177\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 337 train loss: 0.4033883810043335\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 338 train loss: 0.4075902998447418\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 339 train loss: 0.40075111389160156\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 340 train loss: 0.40599894523620605\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 341 train loss: 0.4018334448337555\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 342 train loss: 0.4056393504142761\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 343 train loss: 0.3990127742290497\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 344 train loss: 0.4036957323551178\n",
      "开始测试\n",
      "Acc： 0.8564593301435407 \n",
      "\n",
      "epoch: 345 train loss: 0.39805668592453003\n",
      "开始测试\n",
      "Acc： 0.7464114832535885 \n",
      "\n",
      "epoch: 346 train loss: 0.4020490050315857\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 347 train loss: 0.39509230852127075\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 348 train loss: 0.3991582691669464\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 349 train loss: 0.394610732793808\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 350 train loss: 0.39946839213371277\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 351 train loss: 0.39386609196662903\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 352 train loss: 0.3985578417778015\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 353 train loss: 0.3938283622264862\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 354 train loss: 0.3975268304347992\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 355 train loss: 0.39023953676223755\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 356 train loss: 0.3943420946598053\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 357 train loss: 0.38899216055870056\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 358 train loss: 0.39328885078430176\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 359 train loss: 0.3883771300315857\n",
      "开始测试\n",
      "Acc： 0.7511961722488039 \n",
      "\n",
      "epoch: 360 train loss: 0.3916999399662018\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 361 train loss: 0.3861148953437805\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 362 train loss: 0.39075416326522827\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 363 train loss: 0.38549259305000305\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 364 train loss: 0.3899933993816376\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 365 train loss: 0.3865622580051422\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 366 train loss: 0.39007315039634705\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 367 train loss: 0.38419389724731445\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 368 train loss: 0.3874785602092743\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 369 train loss: 0.38242778182029724\n",
      "开始测试\n",
      "Acc： 0.7559808612440191 \n",
      "\n",
      "epoch: 370 train loss: 0.38556578755378723\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 371 train loss: 0.379034161567688\n",
      "开始测试\n",
      "Acc： 0.7607655502392344 \n",
      "\n",
      "epoch: 372 train loss: 0.3823373019695282\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 373 train loss: 0.3785148859024048\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 374 train loss: 0.3817036747932434\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 375 train loss: 0.37619537115097046\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 376 train loss: 0.380481094121933\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 377 train loss: 0.37596943974494934\n",
      "开始测试\n",
      "Acc： 0.7703349282296651 \n",
      "\n",
      "epoch: 378 train loss: 0.3802848756313324\n",
      "开始测试\n",
      "Acc： 0.8660287081339713 \n",
      "\n",
      "epoch: 379 train loss: 0.37655287981033325\n",
      "开始测试\n",
      "Acc： 0.7655502392344498 \n",
      "\n",
      "epoch: 380 train loss: 0.3813420832157135\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 381 train loss: 0.3745625913143158\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 382 train loss: 0.3776235282421112\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 383 train loss: 0.37237587571144104\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 384 train loss: 0.37662845849990845\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 385 train loss: 0.37151676416397095\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 386 train loss: 0.3750121295452118\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 387 train loss: 0.3692759573459625\n",
      "开始测试\n",
      "Acc： 0.7751196172248804 \n",
      "\n",
      "epoch: 388 train loss: 0.3742407560348511\n",
      "开始测试\n",
      "Acc： 0.8708133971291866 \n",
      "\n",
      "epoch: 389 train loss: 0.3703548014163971\n",
      "开始测试\n",
      "Acc： 0.7799043062200957 \n",
      "\n",
      "epoch: 390 train loss: 0.3733541965484619\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 391 train loss: 0.36834782361984253\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 392 train loss: 0.37224215269088745\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 393 train loss: 0.365490585565567\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 394 train loss: 0.368804007768631\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 395 train loss: 0.3633526563644409\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 396 train loss: 0.36776071786880493\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 397 train loss: 0.3631928861141205\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 398 train loss: 0.3666840195655823\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 399 train loss: 0.3611372411251068\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 400 train loss: 0.36565330624580383\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 401 train loss: 0.36065104603767395\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 402 train loss: 0.3632294833660126\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 403 train loss: 0.3586357533931732\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 404 train loss: 0.36193975806236267\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 405 train loss: 0.35589519143104553\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 406 train loss: 0.358488529920578\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 407 train loss: 0.3527520000934601\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 408 train loss: 0.3568500280380249\n",
      "开始测试\n",
      "Acc： 0.8803827751196173 \n",
      "\n",
      "epoch: 409 train loss: 0.35235098004341125\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 410 train loss: 0.35575348138809204\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 411 train loss: 0.35181373357772827\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 412 train loss: 0.356607049703598\n",
      "开始测试\n",
      "Acc： 0.8755980861244019 \n",
      "\n",
      "epoch: 413 train loss: 0.35225462913513184\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 414 train loss: 0.3550665080547333\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 415 train loss: 0.3508172631263733\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 416 train loss: 0.3547476530075073\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 417 train loss: 0.34851768612861633\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 418 train loss: 0.3511713147163391\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 419 train loss: 0.3456593155860901\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 420 train loss: 0.3488450050354004\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 421 train loss: 0.3435121476650238\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 422 train loss: 0.34628134965896606\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 423 train loss: 0.3421081006526947\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 424 train loss: 0.34623226523399353\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 425 train loss: 0.3435354232788086\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 426 train loss: 0.34704849123954773\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 427 train loss: 0.3433087468147278\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 428 train loss: 0.347247451543808\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 429 train loss: 0.34365129470825195\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 430 train loss: 0.34650635719299316\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 431 train loss: 0.3404497504234314\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 432 train loss: 0.3429201543331146\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 433 train loss: 0.3364982008934021\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 434 train loss: 0.3387630879878998\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 435 train loss: 0.33422741293907166\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 436 train loss: 0.33760982751846313\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 437 train loss: 0.3327285945415497\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 438 train loss: 0.3342145085334778\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 439 train loss: 0.33036482334136963\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 440 train loss: 0.3339768052101135\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 441 train loss: 0.3306795358657837\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 442 train loss: 0.33250993490219116\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 443 train loss: 0.33083751797676086\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 444 train loss: 0.33430546522140503\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 445 train loss: 0.33168864250183105\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 446 train loss: 0.33315905928611755\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 447 train loss: 0.3286207318305969\n",
      "开始测试\n",
      "Acc： 0.784688995215311 \n",
      "\n",
      "epoch: 448 train loss: 0.33161401748657227\n",
      "开始测试\n",
      "Acc： 0.8851674641148325 \n",
      "\n",
      "epoch: 449 train loss: 0.3276597261428833\n",
      "开始测试\n",
      "Acc： 0.7942583732057417 \n",
      "\n",
      "epoch: 450 train loss: 0.3273344337940216\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 451 train loss: 0.32430994510650635\n",
      "开始测试\n",
      "Acc： 0.7990430622009569 \n",
      "\n",
      "epoch: 452 train loss: 0.32707878947257996\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 453 train loss: 0.3224583566188812\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 454 train loss: 0.32274898886680603\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 455 train loss: 0.3196679949760437\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 456 train loss: 0.3217064440250397\n",
      "开始测试\n",
      "Acc： 0.8899521531100478 \n",
      "\n",
      "epoch: 457 train loss: 0.3179917335510254\n",
      "开始测试\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 458 train loss: 0.31767889857292175\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 459 train loss: 0.31526979804039\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 460 train loss: 0.3178176283836365\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 461 train loss: 0.3151700794696808\n",
      "开始测试\n",
      "Acc： 0.8277511961722488 \n",
      "\n",
      "epoch: 462 train loss: 0.316291868686676\n",
      "开始测试\n",
      "Acc： 0.8947368421052632 \n",
      "\n",
      "epoch: 463 train loss: 0.3160882890224457\n",
      "开始测试\n",
      "Acc： 0.8133971291866029 \n",
      "\n",
      "epoch: 464 train loss: 0.31828343868255615\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 465 train loss: 0.31519246101379395\n",
      "开始测试\n",
      "Acc： 0.8181818181818182 \n",
      "\n",
      "epoch: 466 train loss: 0.3161484897136688\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 467 train loss: 0.31336313486099243\n",
      "开始测试\n",
      "Acc： 0.8229665071770335 \n",
      "\n",
      "epoch: 468 train loss: 0.3156677782535553\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 469 train loss: 0.31160295009613037\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 470 train loss: 0.31147268414497375\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 471 train loss: 0.3076978027820587\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 472 train loss: 0.30927732586860657\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 473 train loss: 0.30469444394111633\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 474 train loss: 0.3050708472728729\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 475 train loss: 0.30433833599090576\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 476 train loss: 0.30706045031547546\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 477 train loss: 0.30523812770843506\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 478 train loss: 0.3069020211696625\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 479 train loss: 0.30789440870285034\n",
      "开始测试\n",
      "Acc： 0.8373205741626795 \n",
      "\n",
      "epoch: 480 train loss: 0.3093292713165283\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 481 train loss: 0.3053872585296631\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 482 train loss: 0.3053738474845886\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 483 train loss: 0.3012750744819641\n",
      "开始测试\n",
      "Acc： 0.84688995215311 \n",
      "\n",
      "epoch: 484 train loss: 0.3019815683364868\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 485 train loss: 0.29794570803642273\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 486 train loss: 0.29854750633239746\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 487 train loss: 0.2961830496788025\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 488 train loss: 0.2979269325733185\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 489 train loss: 0.2963061034679413\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 490 train loss: 0.29695582389831543\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 491 train loss: 0.29703694581985474\n",
      "开始测试\n",
      "Acc： 0.8516746411483254 \n",
      "\n",
      "epoch: 492 train loss: 0.2989288866519928\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 493 train loss: 0.29756516218185425\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 494 train loss: 0.29794543981552124\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 495 train loss: 0.29498475790023804\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 496 train loss: 0.29562634229660034\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 497 train loss: 0.2919616103172302\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n",
      "epoch: 498 train loss: 0.2918926775455475\n",
      "开始测试\n",
      "Acc： 0.8995215311004785 \n",
      "\n",
      "epoch: 499 train loss: 0.28747159242630005\n",
      "开始测试\n",
      "Acc： 0.861244019138756 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xVVbrw8d+THlpCCYEUCL1KkUiRqqJiBRV7d7yIZcYyTb0z886dub6vM7arY2G4trGLBUVEER2pSgm9Y+gphNAChBSSPO8fe6OHkIQg2Tk55zzfz+d8cs7ea+/zrEDOc/Zae60lqooxxpjQFebvAIwxxviXJQJjjAlxlgiMMSbEWSIwxpgQZ4nAGGNCnCUCY4wJcZYITNATkeEistHfcRjTUFkiMJ4SkW0iMtqfMajqPFXt5s8YjhGRUSKSVU/vdZ6IbBCRIyLyrYi0r6FsCxGZKiKFIrJdRG7w2RclIh+6/5YqIqPqI35TfywRmIAnIuH+jgFAHA3ib0pEWgEfA38EWgAZwPs1HPICUAokAjcCL4lIL5/984GbgF2eBGz8qkH8pzWhR0TCRORhEdksIntFZIqItPDZ/4GI7BKRAhGZ6/uhJCKvi8hLIjJDRAqBc9xvq78RkVXuMe+LSIxb/rhv4TWVdff/TkRyRSRHRO50vwV3rqYes0XkMRFZABwBOorI7SKyXkQOicgWEbnLLdsY+AJIEpHD7iPpZL+Ln+lKYK2qfqCqxcCfgb4i0r2KOjQGrgL+qKqHVXU+MA24GUBVS1X1f9zt5acZl2mALBEYf/kVMA4YCSQB+3G+lR7zBdAFaA0sA96udPwNwGNAU5xvqwDXAGOADkAf4LYa3r/KsiIyBngIGA10duM7mZuBCW4s24HdwKVAM+B24BkROVNVC4GLgBxVbeI+cmrxu/iRiLQTkQM1PI416fQCVh47zn3vze72yroC5aq6yWfbymrKmiAU4e8ATMi6C7hPVbMAROTPwA4RuVlVy1T11WMF3X37RSROVQvczZ+q6gL3ebGIADznfrAiIp8B/Wp4/+rKXgO8pqpr3X3/hdMkUpPXj5V3fe7zfI6IfAUMx0loVanxd+FbUFV3APEniQegCZBfaVsBTrKqqmxBLcuaIGRXBMZf2gNTj32TBdbjNDskiki4iDzuNpUcBLa5x7TyOX5nFef0bb8+gvMBV53qyiZVOndV71PZcWVE5CIRWSgi+9y6XczxsVdW7e+iFu9dncM4VyS+mgGHTrOsCUKWCIy/7AQuUtV4n0eMqmbjNPuMxWmeiQPS3GPE53ivps3NBVJ8XqfW4pgfYxGRaOAj4EkgUVXjgRn8FHtVcdf0uziO2zR0uIbHjW7RtUBfn+MaA53c7ZVtAiJEpIvPtr7VlDVByBKBqQ+RIhLj84gAJgGPHbulUUQSRGSsW74pUALsBRoB/7ceY50C3C4iPUSkEfCnUzw+CojGaZYpE5GLgAt89ucBLUUkzmdbTb+L46jqDp/+haoex/pSpgK9ReQqtyP8T8AqVd1QxTkLce4w+ouINBaRoTiJ+M1jZUQk2qdDPcr9d5TK5zKByRKBqQ8zgCKfx5+BZ3HuTPlKRA4BC4FBbvk3cDpds4F17r56oapfAM8B3wKZwPfurpJaHn8Ip/N3Ck6n7w049Ty2fwPwLrDFbQpKoubfxc+tRz7OnUCPuXEMAq47tl9EHhWRL3wOuQeIxenofhe4u1K/x0acf7tkYKb7vNpxCSawiC1MY0z1RKQHsAaIrtxxa0ywsCsCYyoRkSvEGU3bHPgb8JklARPMLBEYc6K7cNr4N+PcvXO3f8MxxlvWNGSMMSHOrgiMMSbEBdzI4latWmlaWpq/wzDGmICydOnSPaqaUNW+gEsEaWlpZGRk+DsMY4wJKCKyvbp91jRkjDEhzhKBMcaEOEsExhgT4iwRGGNMiLNEYIwxIc7TRCAiY0Rko4hkisjDVexvLs6C2atEZLGI9PYyHmOMMSfyLBGIs6D4CzhL8/UErheRnpWKPQqsUNU+wC04szAaY4ypR16OIxgIZKrqFgAReQ9njvN1PmV6Av8PnOl5RSRNRBJVNc/DuIwxxn+O7IPtC2DXGtCKUzu23WDofF6dh+RlIkjm+CX8sjhxjvWVwJXAfBEZiDO/eQrO4h0/EpEJOIuD065dO6/iNcaYuld0ALZ/B9vmwdZ5kLeGnxaqO8W1fYY9EHCJoKoaVp7h7nHgWRFZAawGlgMnTPerqpOByQDp6ek2S54xpuEqLoAdC2HrXOfDP3cVoBARA6kD4Zz/hA7DIelMiIjyd7SAt4kgi+PXe00BcnwLqOpB4HYAd9m7re7DGGMCQ8kh2LEIts11vvHnrnCafMKjIGUgjHoY0oZDSjpERPs72ip5mQiWAF1EpAPOkoPX4Szb9yMRiQeOqGopcCcw100OxhjTMJUWOt/4t82DbfMhexloOYRFQspZMPw3zjf+lLMgMtbf0daKZ4lAVctE5D6c9U3DgVdVda2ITHT3TwJ6AG+ISDlOJ/IvvIrHGBOCDuyE+c9A9tK6OV9FGeRvhIqjEBYByQOcdvu04ZA6CKIa1c371DNPZx9V1Rk4C5f7bpvk8/x7oIuXMRhjQlBBNsx7Cpa94bzuMNz5xn66RKDL+ZA2DFIHQ3ST0z9nAxBw01AbY0y1DubC/Kdh6etOO33/m2H4ryE+9aSHhjJLBMaYhqOi3Lm98lina8UJNxFWr/wo/DDLOab/jU5bffP23sUaRCwRGGP8p6ICdq/76R777Qug+ICzLy711Dtb+1ztJIAWHeo+1iBmicAYU39UYfd6946bebBtARTtc/Y17wA9LoMOI5w2+GZJ/o01hFgiMMbUjYoKWPkO7Nty4j5VZ/u2+XBkj7Mtrh10u8i54yZtmLXj+5ElAmPM6TuyD6beBT98BRLu3F1TWZM20Hm0cwdP2nBrv29ALBEYY07PziXwwW1QuBsufhLOurPqRGAaLEsExgSTigrI/Nr5UK5KsyR34FPjk59n9zrYmwlJ/av+9q4KC1+CWX90znvHTEg+8/TrYOqdJQJjgkFFBaz/FGb/DfLX11w2LNIZEZs2zGmmSR3kTIiWv8G5c+fY1AnHOnHBac8/1qSTNgximsGn98L6z6DbJTDuBYht7m0djWcsERgTyCoqYMN0mP047F4LrbrCVa84s1xWpgp7f3A/7Oc7Uy/Me9KZHC26KRzZ65SLS4WuY5wP/lbdnOkZts2FjTNgxdtOmYhYZ5qFCx6DIfdaU1CAs0RggteBndC0LYQH4X/zo0XO4Kk5f4e81dCyM1z5MvS+EsLCqz+ueXunwxag+OBPk6cd2Qvthjgf/s3Tjj8mZQAMmnD8Pf95a6D/LdCu8hIjJhAF4V+IMUDeOpg0zGkCufo1iEvxd0Snp6wEspY43+S3znOel5c4995f8U/oPf7UE15MM+h6gfOojbAwaNPbeZigYonABKeFL0J4pPMNdtJwuPJ/octob9/zx3vl3Q/quFSnPT3lrOrnoS859NMiJvu3VV2maL9zvrJiQKBtHxj4H87Aq07nBecVj6lX9j/IBJ/D+bBqijPfzOB7Ycot8PZ4Z/KxUY/U3QenqvPhvW3+Tx2sB7OdfbHNnSUKfVemSnNHzB4t/Ombfc7yn+ayb9ERJOzE94mMhfQ7nI7a9mdDbHzdxG+MyxKBCT5LX3OaTQbdDa06w51fwxe/czpGdy5yOlObJv68cx/Y8dOH+LZ5UOAuy92olXtXzTDnA79VF2fJwmNr1W6bB98+xo+rtYZFQHI6DHvQXcRkYMDOZW8Cn6gG1hLA6enpmpGR4e8wTENVVgLP9Ia2feGmD4/ft+IdmP6Qc4fMqIeh4yj3W3gNd7wUZLvf+N1lCA9sd7bHtnA/9Ic7H+QJ3U9+58yRfbDje+cKod3gk9/Lb0wdEpGlqppe1T67IjDBZc1HzmCqwXefuK/fDdC2H3x4B3z+kLOtaZLPN/nhTjPMj/fSz/tp3pyYeKfM4Hucn617Op2np6JRC+h+yenVzxgPWCIwwUPV6SRO6A6dzq26TGJPuOd7Z8Ts1rnOh/3mf8Oq948vFx3ntMefdaeTIBJ7n/oHvzEBwhKBCR7b5sOu1XDZszU304g4bfitusBZv3ASSP5GJymUlUDaUGjTp+b78Y0JIpYITPBY+JLTdt/n2lM7TgRad3cexoQgu9Y1wWHvZmcKhPQ7Tn1VK2NCnCUCExwWT3ZuyTzrTn9HYkzAsURgAl9xASx/y5lnp1lbf0djTMCxRGAC37I3oPRw1beMGmNOytNEICJjRGSjiGSKyMNV7I8Tkc9EZKWIrBWR272MxwSh8jJYNBnane0soGKMOWWeJQIRCQdeAC4CegLXi0jPSsXuBdapal9gFPCUiER5FZMJQnOfgIIdMOQef0diTMDy8opgIJCpqltUtRR4DxhbqYwCTUVEgCbAPqDMw5hMMJn7BMx5HPpe76ySZYz5WbxMBMnATp/XWe42X88DPYAcYDVwv6pWVD6RiEwQkQwRycjPz/cqXhNI5j0N//5vZ8zA2Bds1K8xp8HLv56qhnZWnuHuQmAFkAT0A54XkWYnHKQ6WVXTVTU9ISGh7iM1gWXBs/DNf8EZV8O4l2wEsDGnyctEkAWk+rxOwfnm7+t24GN1ZAJbARveaar33fMw60/Q+yoYN8mSgDF1wMtEsAToIiId3A7g64BplcrsAM4DEJFEoBuwxcOYTCD7/kX46j+h5zi4YrKtzGVMHfHsL0lVy0TkPmAmEA68qqprRWSiu38S8FfgdRFZjdOU9HtV3eNVTCaALX8bZj4CPS6Hq162JGBMHfL0r0lVZwAzKm2b5PM8B6jlytkmZB3Y4awwljYcxr/qrEVsjKkzdquFadhU4bP7nZ9jX7AkYIwH7PraNGzL33IWjrn4SWje3t/RGBOU7IrANFwHc2Dmf0L7YZD+C39HY0zQskRgGiZVmP4glJfC5c/ZgDFjPGR/XaZhWjUFNn0J5/0RWnbydzTGBDVLBKbhOZTn3CWUMhAGTfR3NMYEPUsEpmFRhc8fgqNF7hxCNnLYGK9ZIjANy9qPYcN0OOdRSOjq72iMCQmWCEzDsXUefHIvJA+AIff5OxpjQoYlAtMwbJsP71zjjBW4/n2bQsKYemSJwPjf9u/g7WsgLhVu/Qya2FTjxtQnSwTGv3YshLfGQ1yymwRa+zsiY0KOJQLjPzsWwVtXQbO2ThJomujviIwJSdYQa7x1eDeUHDpx+/6tMOU2aJIIt06Hpm3qPTRjjMMSgfFGRTnMftxZYP6EFUpdzTvAbdOdKwJjjN9YIjB171AefPQL2DYP+l4PHc85sYyEQadzoXHL+o/PGHMcSwSmbm2d5ySB4oPOyOD+N/k7ImPMSVgiMHWjogLmPw3fPgYtOsHNUyGxl7+jMsbUgiUCc/qK9sNH/wGZs6D3VXDZsxDd1N9RGWNqyRKBOT1FB+CNcZC3Fi55yllARsTfURljToElAvPzFRfAm1c4SeC6t6Hrhf6OyBjzM9iAMvPzFB+EN6+EXavh2jctCRgTwOyKwJy64oPOiODcFXDNG9DtIn9HZIw5DXZFYE5NySF4ezzkLIOrX4ful/g7ImPMafI0EYjIGBHZKCKZIvJwFft/KyIr3McaESkXkRZexmROQ8lhePtqyMqA8a9Cj8v8HZExpg54lghEJBx4AbgI6AlcLyI9fcuo6hOq2k9V+wGPAHNUdZ9XMZmfqawUMl6FFwbBzsUw/hXoOdbfURlj6oiXfQQDgUxV3QIgIu8BY4F11ZS/HnjXw3jMqSo/CivehrlPQcEOSDkLrpgEHYb7OzJjTB3yMhEkAzt9XmcBg6oqKCKNgDFAlesTisgEYAJAu3bt6jZKc6Lyo7DyPZj7dziww1k68tJnoPN5NkbAmCDkZSKo6hOjmmkouQxYUF2zkKpOBiYDpKenV3cOUxeK9jtjA3KWQ9t+cPGT0OUCSwDGBDEvO4uzgFSf1ylATjVlr8PjZqGd+47w9y83UFZe4eXbBDbfUcJXvQITZjvjAywJGBPUvEwES4AuItJBRKJwPuynVS4kInHASOBTD2Nhfe5BXpy9mc9X53r5NoGr6MBPo4SvfQvOGG8JwJgQ4VkiUNUynDb/mcB6YIqqrhWRiSIy0afoFcBXqlroVSwAo3sk0qV1E16avRlVa106zrEBYjZK2JiQ5Ok4AlWdoapdVbWTqj7mbpukqpN8yryuqtd5GQdAWJgwcWQnNuw6xLcbd3v9doGj5JDPKOF/2ShhY0JQSI0svrxfEsnxsbz47WZ/h9IwlByCt2yUsDGhLqQSQWR4GBNGdCRj+34Wbw3xcWs5y52O4awlNkrYmBAXUokA4Jr0VFo2juKl2Zn+DsU/clfCO9fB5FGwbzNc/ZqNEjYmxIXc7KOxUeHcPjSNJ7/axLqcg/RMaubvkOrHrtUw+3HYMB1i4uDcP8DAuyAmROpvjKlWyF0RANw8JI0m0RFMmhMCfQV5a+H9m2HSMGdh+VGPwgOrYcRvLQkYY4AQvCIAiIuN5MbB7fjfuVv49QVdad+ysb9Dqnu71ztXAOs+gaimMPL3MPgeiI33d2TGmAYmJK8IAH4xtAMR4WH8c+4Wf4dSt/I3wod3wItDIPNrGP4beGAVnPOoJQFjTJVC54pgzw+wccaPL1sDz6buYtWyAg4360STmCD4VexaDas/hMhGMOwBGPJLaNzS31EZYxq4IPj0q6W8NTDrT8dtugi4KByY55eI6l5kIzj7lzD0fmjcyt/RGGMCRK0SgYhcAfxbVQvc1/HAKFX9xMvg6lT3y+DRE+e8+/UHK5m7KZ/pvxxGYrMYPwRWh8KjIDzS31EYYwKM1GbeHRFZ4a4i5rttuar29yyyaqSnp2tGRkadnW997kEu+8d8ylUZ0K455/VIZHSP1nRu3QSxSdeMMUFCRJaqanqV+2qZCFapap9K21ar6hl1FGOt1XUiANi46xAzVufyzYY81mQfBKB9y0ac1z2R0T1bMzCtBRHhIduvbowJAnWRCF4FDuCsQazAL4HmqnpbHcZZK14kAl+5BUV8s34336zPY8HmvZSWVRAXG8k53RIY3TORkV0TaBpjzS/GmMBSF4mgMfBHYLS76SvgMa+njq6K14nA15HSMuZu2sOsdXn8e0Me+48cJTJcGNyxJRf0asOFvRJp3TTA+xWMMSHhtBNBQ1KficBXeYWydPt+vl6fx6x1eWzdU4gInNW+BWN6t2FM7zYkxcfWe1zGGFMbdXFFMAu4WlUPuK+bA++par2vYOKvROBLVdmUd5gv1uTyxepdbMw7BEC/1HguPqMNl/VNom2cJQVjTMNRF4nghDuEguWuobqwJf8wX6zZxZdrdrE6uwARGJjWgnH9k7m4d1viGlmfgjHGv+oiESwFrlDVHe7r9sBUVT2zTiOthYaYCHxt21PIpyty+HRFNlv2FBIZLozq1ppx/ZIZ3bM10RHh/g7RGBOC6iIRjAEmA3PcTSOACao6s86irKWGngiOUVXWZB/kkxXZfLYyh92HSohvFMm4fslck54aOtNfG2MahDrpLBaRVsBgQIDvVXVP3YVYe4GSCHyVVygLMvcwJWMnX63No7S8gjOS47jmrFQu75tEXKw1HRljvFVXiaA50AX48X5JVZ1bJxGegkBMBL72F5by6Yps3s/IYn3uQaIjwri0TxK3DGlP31SbHdQY4426aBq6E7gfSAFW4FwZfK+q59ZloLUR6IngmGNNR+8t2cHU5dkcKS2nb0ocNw9J49I+bYmJtL4EY0zdqYtEsBo4C1ioqv1EpDvwX6p6bd2GenLBkgh8HSo+ysfLsnnj+21szi+keaNIrklP5eYh7Ulp3sjf4RljgkBNiaC2E+gUq2qxe7JoVd0AdKvFG48RkY0ikikiD1dTZpSIrBCRtSIyp6oywa5pTCS3np3G1w+N5J07BzGoQ0tenr+VkU/M5lfvLmdNdoG/QzTGBLHarkeQ5U49/QkwS0T2AyfO6exDRMJx5iY6H8gClojINFVd51MmHngRGKOqO0Sk9c+pRLAQEc7u3IqzO7ci50ARry3YyruLdzJtZQ5DO7dkwohOjOjSymZFNcbUqVOeYkJERgJxwJeqWlpDuSHAn4+NPhaRRwBU9f/5lLkHSFLVP9T2/YOxaagmB4uP8s6iHby2YCt5B0vo3qYpE0d24tI+bW1GVGNMrf3spiERyRCRZ90mnhgAVZ2jqtNqSgKuZGCnz+ssd5uvrkBzEZktIktF5JaTnDPkNIuJZOLITsz73bk8Mb4PFao88P4KRj89hw8ydnK0vMLfIRpjAtzJvlIOBqYCo4A5IjJDRO4Xka61OHdV7ReVLz8igAHAJcCFwB+rOreITHCTUkZ+fn4t3jr4REWEcXV6Kl/eP4J/3jyAxtER/PbDVZzz5GzeXbyD0jJLCMaYn6fGRKCqZao6W1UfVtVBwC+AQ8B/i8gyEXmxhsOzgFSf1ymc2K+QhdPEVOgOUJsL9K0ijsmqmq6q6QkJCbWoVvAKCxMu7NWG6b8cxiu3ptOycRSPfLyaUU98y5sLt1tCMMacstrePnq1qn5Qads1QLaqLqjmmAhgE3AekA0sAW5Q1bU+ZXoAz+NcDUQBi4HrVHVNdbGEWh/Byagqc3/Yw7Nfb2LZjgOkNI/lgdFduaJ/MuFh1qlsjHHUxe2jj1Sx7eHqkgA4VxPAfcBMYD0wRVXXishEEZnollkPfAmswkkCL9eUBMyJRISRXRP46O6z+dcdA4lvFMlvPljJhf8zly/X5BJo600YY+pfjVcEInIRcDFwDfC+z65mQE9VHehteCeyK4KaVVQoX67dxVNfbWRzfiFnJMfx2wu7MdxuOzUmpJ3OFUEOkAEUA0t9HtNwmnNMAxMWJlx8RltmPjCCJ8b3YV9hKbe8upibX1nM2hwbmGaMOVFt+wgiVfWo+7w5kKqqq7wOrip2RXBqSsrKeXvhDp779w8UFB3lyv4p/ObCrraCmjEhpi76CGaJSDMRaQGsBF4TkafrLELjmeiIcO4Y1oE5vz2HCcM78tnKHEY9MZsnZ27kUPFRf4dnjGkAapsI4lT1IHAl8JqqDgBGexeWqWtxsZE8cnEPvvn1SC7s1Ybnv81k1BOzeWfRDsorrEPZmFBW20QQISJtcTqNp3sYj/FYaotGPHd9fz69dyidEprw6NTVXP78fJZs2+fv0IwxflLbRPAXnNtAN6vqEhHpCPzgXVjGa31T43n/rsH84/r+7C8s5epJ3/Ord5eTW1Dk79CMMfXslCed8zfrLK57RaXlvDRnM/+cs5kwEe4Z1Yn/GNHRFscxJoicdmexiKSIyFQR2S0ieSLykYik1G2Yxl9io8J56PyufP3QSEZ1S+CpWZu44Jm5fLtht79DM8bUg9o2Db2GM3YgCWcG0c/cbSaIpLZoxEs3DeCdOwcRGS7c/voS7nozg+wD1lxkTDCrbSJIUNXX3EnoylT1dSC0Z38LYmd3bsUX94/gd2O6MWdTPqOfmsOkOZttymtjglRtE8EeEblJRMLdx03AXi8DM/4VFRHGPaM68/VDIxnWpRWPf7GBi5+dx8It9s9uTLCpbSK4A+fW0V1ALjAeuN2roEzDkdK8Ef97Szqv3JpO0dFyrpu8kN9+sJL9hSdbl8gYEyhqmwj+Ctyqqgmq2honMfzZs6hMg3Nej0RmPTiSu0d14uPl2Yx+eg5Tl2fZ7KbGBIHaJoI+qrr/2AtV3Qf09yYk01DFRoXz+zHdmf7LYaS2aMSD76/kllcXs31vob9DM8achtomgjB3sjkA3DmHIrwJyTR0Pdo246O7z+YvY3uxfMcBLnhmLi/OzrTOZGMCVG0TwVPAdyLyVxH5C/Ad8HfvwjINXXiYcMuQNL5+aCTndGvN37/cyNjnF7Am26a6NibQ1CoRqOobwFVAHpAPXKmqb3oZmAkMbeJimHTzACbddCb5h0sY+8IC/vblBoqPlvs7NGNMLdW6eUdV1wHrPIzFBLAxvdsypGMrHpuxjpdmb2bmml08flUfBnZo4e/QjDEnUdumIWNOKq5RJH8f35e3fjGI0vIKrvnn9/zxkzUcLinzd2jGmBpYIjB1bliXVnz14AjuGNqBtxZt58Jn5rIgc4+/wzLGVMMSgfFEo6gI/nRZTz6cOIToiDBufHkR/zl1tV0dGNMAWSIwnhrQvgUz7h/OncM68M7iHVz4zFy+s6sDYxoUSwTGczGR4fzh0p58cNcQoiLCuOHlRfzhk9UU2tWBMQ2CJQJTb9LTWjDjV87VwduLdjDm2bkssknsjPE7TxOBiIwRkY0ikikiD1exf5SIFIjICvfxJy/jMf4XG+VcHUy5awhhIlz3vwv57+nrbNyBMX7kWSIQkXDgBeAioCdwvYj0rKLoPFXt5z7+4lU8pmE5y706uGlQe16ev5VL/zGfVVkH/B2WMSHJyyuCgUCmqm5R1VLgPWCsh+9nAkzj6Aj+Oq43b9wxkMPFZVzx4nc8PWsTpWU2Z5Ex9cnLRJAM7PR5neVuq2yIiKwUkS9EpFdVJxKRCSKSISIZ+fn5XsRq/GhE1wRmPjiCsf2SeO6bH7jixQVs2HXQ32EZEzK8TARSxbbKk9cvA9qral/gH8AnVZ1IVSerarqqpick2AqZwSguNpKnr+nHP28eQN7BYi77x3xe+DaTMpvR1BjPeZkIsoBUn9cpQI5vAVU9qKqH3eczgEgRaeVhTKaBu7BXG2Y+MIILerbhiZkbGT/pezJ3H/Z3WMYENS8TwRKgi4h0EJEo4Dpgmm8BEWkjIuI+H+jGY/cThriWTaJ54cYz+cf1/dm2t5BLnpvHy/O2UFFhq6EZ4wXPEoGqlgH3ATOB9cAUVV0rIhNFZKJbbDywRkRWAs8B16mtfWhcl/VN4qsHRzC8Syv++/P1XDvZrg6M8YIE2uduenq6ZmRk+DsMU49UlY+WZfPX6esoKi3n3nM6M3FUR6Ijwv0dmjEBQ0SWqmp6VftsZLFp8ESE8QNS+PqhkYzp3YZnvt7EJc/NZ8m2ff4OzZigYFcEJuB8u3E3f5i6huwDRdwwqB2/H9OduNjIkx5XVl7BrHV5vLN4B3sPl0jnyg8AABOLSURBVNIkOoLG0eE0jo5wn0fQonEUSfExJMXFkhQfS5u4GCLD7fuSCXw1XRFYIjAB6UhpGc/M2sQr87fSvFEU5/dMZHiXBIZ2bkl8o6jjyu4vLOW9JTt58/tt5BQUkxwfS/c2TTlcUsaR0nIKS8o4XFJGYUkZhaXHT3UhAolNY2gbH0Ni0xhaN4smsVkMrZs6P5PiY+iU0AT3ngdjGixLBCZorc4q4IVvM1mQuYdDJWWIQJ+UeEZ0aUW/1Hhmrctj6vJsSsoqGNKxJbcNTWN0j0TCw6r+4C4qLSe3oIicA8XkHCgi+0AROQeKyC0oZvehYvIOllBQdPS4Y1Kax3JJn7Zc1ieJXknNLCmYBskSgQl6ZeUVrMw6wNxNe5j3Qz4rdh6gQiE6Iowrz0zm1rPT6N6mWZ28V/HRcnYfLCHvUDGbdx/my7W7mP/DHsoqlLSWjbikT1suOSOJHm2bWlIwDYYlAhNyCoqOsirrAL2T4mjeOOrkB5ym/YWlfLVuF9NX5fLd5r2UVyhdE5tw5ZkpjOuXTJu4GM9jMKYmlgiMqUd7D5fwxZpdfLI8m4zt+xGBoZ1aceWZyVzYqw2NoyP8HaIJQZYIjPGTbXsKmbo8m4+XZ7FzXxGNosLp3y6etu5dSUlxMbR1f6a2aERMpI2NMN6wRGCMn6kqGdv3M3V5NhtyD5JzwOl89p01o1lMBLcP7cAdQzsQ1+jkt8MacyosERjTAB0tryDvYDG5BcVk7y9ixupcvlqXR5PoCG4Z0p5fDOtAyybR/g7TBAlLBMYEiPW5B3n+20xmrM4lJiKcmwa34z+Gd6R1M+tsNqfHEoExASZz9yFe+HYzn67IJkyEIZ1ackHPREb3TKRtXKy/wzMByBKBMQFq255C3l28g6/W5bF1TyEAfVPiuKBXGy7omUiXxKZ+jtAECksExgQ4VWVz/mFmrs3jq3V5rNx5AIAz28Vz+9AOjOndxuZEMjWyRGBMkNlVUMz0VTm8uXA72/ceIbFZNDcPbs/1A9tZB7OpkiUCY4JUeYUye+NuXv9uG/N+2ENURBhj+yZx/aB29E+NtykuzI9qSgQ2xNGYABYeJpzXI5HzeiTyQ94hXv9uGx8vy+aDpVmktWzEuP7JjOuXTFqrxv4O1TRgdkVgTJA5WHyUL90pLr7fshdV6N8univ6J3NZn6R6mXvJNDzWNGRMiMotKGLaihxnRPOuQ0SFh3F+r0SuTU9lWOdWhFUzHbcJPpYIjDGszz3IBxlZfLw8iwNHjpIcH8vV6SlcnZ5KcryNTQh2lgiMMT8qKStn1ro83l+yk/mZewAY0rEl53ZvzahuremU0Ng6mYOQJQJjTJV27jvCB0uzmLE6l8zdhwFnxbVR3RIY1bU1Z3duSaMou6ckGFgiMMac1M59R5izKZ/ZG/P5bvMejpSWEx0RxkW923DNWakM7tDS+hQCmCUCY8wpKSkrJ2PbfufuoxXZHCouo12LRlyTnsL4Aam24loA8lsiEJExwLNAOPCyqj5eTbmzgIXAtar6YU3ntERgTP0qPlrOl2t28f6SnXy/ZS9hAqO6tWZc/2RG92htTUcBwi8DykQkHHgBOB/IApaIyDRVXVdFub8BM72KxRjz88VEhjsD0/ons31vIVMydvLR0mz+vWE3sZHhjO6ZyGV92jKyWwLREbbCWiDyMpUPBDJVdQuAiLwHjAXWVSr3S+Aj4CwPYzHG1IH2LRvz2wu78+vzu7Fk2z6mrcxhxupcPluZQ9OYCMb0asNVA1IY1KGF3XkUQLxMBMnATp/XWcAg3wIikgxcAZxLDYlARCYAEwDatWtX54EaY05NWJgwqGNLBnVsyZ8v78V3m/cybUUOX67ZxQdLs+jSugk3DmrHFWemEBdry242dF4mgqq+DlTukPgf4PeqWl7TtwdVnQxMBqePoM4iNMactsjwMEZ2TWBk1wSKj/bms5U5vLVoB3/+bB1/+3Ijl/dN4qbB7TkjJc7foZpqeJkIsoBUn9cpQE6lMunAe24SaAVcLCJlqvqJh3EZYzwSExnO1empXJ2eyuqsAt5etJ1PVmTzfsZOerZtxsVntGFM77Z0bt3E36EaH57dNSQiEcAm4DwgG1gC3KCqa6sp/zow3e4aMia4FBQdZeqyLKatzGHZDmdBnS6tm3BRbycp9Gjb1PoT6oFf7hpS1TIRuQ/nbqBw4FVVXSsiE939k7x6b2NMwxEXG8ltQztw29AO5BYUMXPNLr5cu4vnv83kuX9nkhwfS3pac85s5zy6t21qq63VMxtQZozxiz2HS/hqbR7zfshn2Y795B0sASA2MpwzUuI4K605V52ZQscEa0aqCzay2BjToKkqOQXFLNu+n2U79rNsxwHWZhdQVqGM6JrArUPaM6pba8JtioufzRKBMSbg5B8q4b3FO3hr0XbyDpaQ2iKWmwe355r0VOIb2eI6p8oSgTEmYB0tr2DWujxe/24bi7fuIzoijMvcW1L7psRZR3MtWSIwxgSF9bkHeeP77Xy6IpsjpeX0SmrGjYPaM7ZfEo2jbc6jmlgiMMYElUPFR/lkRQ5vL9zOhl2HaBIdwbj+Sdx2dgcbo1ANSwTGmKCkqizbcYC3F21n+qpcysoruPLMFO4/rwupLRr5O7wGxRKBMSbo7T1cwkuzN/PGwu2oKjcMbMe953amdVNbOwEsERhjQkhuQRHPfZPJlIydRIYLtw/twF0jOob8nUaWCIwxIWfbnkKe+XoT01bmEBUexvk9E7nyzGSGd0kIyZHLlgiMMSFr465DvLNoO9NW5rD/yFFaNYnisr5JXNk/hd7JzULm9lNLBMaYkFdaVsGcTfl8vCyLb9bvprS8gtQWsXRs1YTk5rGkNI8lpXkjkuNjad+yEa2aRPs75Drll0nnjDGmIYmKcJqHzu+ZSMGRo3y+Opd5P+STfaCI1dkF7CssPa78TYPb8fBFPWgSAuMT7IrAGGOAwpIycg4UkbW/iDmb8vnX99tIiovlb1f1YViXVv4O77TVdEUQej0mxhhThcbREXRJbMo53Vvz58t78eHEIURHhnHTK4t45ONVHCo+6u8QPWOJwBhjqjCgfQtm/Go4d43oyPtLdnLhM3OZsynf32F5whKBMcZUIyYynEcu7sFHd59No+gIbn11Mdf883veXrSd/ZX6FAKZ9REYY0wtFB8t57UF2/hw6U425xcSESaM7JrA5f2SGN0jscFPeme3jxpjTB1RVdblHmTaihymrcwht6CY2MhwxvZL4t5zOjfYOY4sERhjjAcqKpSM7fuZujyLj5ZlU1GhXJ2ewj2jGl5CsERgjDEe21VQzEuzM3l38U4UZfyAVO47tzPJ8bH+Dg2wRGCMMfUmt6CIF7/dzPtLnIQwrl8yY/slM7hjCyL8OMeRJQJjjKlnOQeKeHF2JlOXZVNYWk6LxlFc2KsNl/Zpy6AO9Z8ULBEYY4yfFB8tZ/bGfD5fncs36/M4UlpOy8ZRnNu9NWmtGtM2Loa2cbEkxcfQJi6G6IhwT+KwuYaMMcZPYiLDGdO7DWN6t3GTwm6mr8rl6/V57D9y4mjlxGbR3DWiE7eenUZ4WP3MjOrpFYGIjAGeBcKBl1X18Ur7xwJ/BSqAMuABVZ1f0zntisAYEyyKSsvJKSgi90Dxjz8Xb9vLgsy9DGjfnL9d1afO1mD2S9OQiIQDm4DzgSxgCXC9qq7zKdMEKFRVFZE+wBRV7V7TeS0RGGOCmaoydXk2f5m+jiOl5dx/XhcmjOh42ovp+GvSuYFApqpuUdVS4D1grG8BVT2sP2WixkBgdVgYY0wdExGuPDOFWQ+OZHSP1jwxcyPjXljA2pwCz97Ty0SQDOz0eZ3lbjuOiFwhIhuAz4E7qjqRiEwQkQwRycjPD85Jn4wxxldC02hevHEAk246k7yDJYx9fgEvz9viyXt5mQiq6uU44Ru/qk51m4PG4fQXnHiQ6mRVTVfV9ISEhDoO0xhjGq4xvdvy9UMjGNsvmQ6tGnvyHl7eNZQFpPq8TgFyqiusqnNFpJOItFLVPR7GZYwxASW+URRPXdPXs/N7eUWwBOgiIh1EJAq4DpjmW0BEOou7crSInAlEAXs9jMkYY0wlnl0RqGqZiNwHzMS5ffRVVV0rIhPd/ZOAq4BbROQoUARcq4E2ws0YYwKcjSw2xpgQYGsWG2OMqZYlAmOMCXGWCIwxJsRZIjDGmBBnicAYY0JcwN01JCL5wPafeXgrIFQHq4Vq3a3eocXqXb32qlrl1AwBlwhOh4hkVHf7VLAL1bpbvUOL1fvnsaYhY4wJcZYIjDEmxIVaIpjs7wD8KFTrbvUOLVbvnyGk+giMMcacKNSuCIwxxlRiicAYY0JcyCQCERkjIhtFJFNEHvZ3PF4RkVdFZLeIrPHZ1kJEZonID+7P5v6M0Qsikioi34rIehFZKyL3u9uDuu4iEiMii0VkpVvv/3K3B3W9jxGRcBFZLiLT3ddBX28R2SYiq0VkhYhkuNtOq94hkQhEJBx4AbgI6AlcLyI9/RuVZ14HxlTa9jDwjap2Ab5xXwebMuDXqtoDGAzc6/4bB3vdS4BzVbUv0A8YIyKDCf56H3M/sN7ndajU+xxV7eczduC06h0SiQAYCGSq6hZVLQXeA8b6OSZPqOpcYF+lzWOBf7nP/4WzPnRQUdVcVV3mPj+E8+GQTJDXXR2H3ZeR7kMJ8noDiEgKcAnwss/moK93NU6r3qGSCJKBnT6vs9xtoSJRVXPB+cAEWvs5Hk+JSBrQH1hECNTdbR5ZAewGZqlqSNQb+B/gd0CFz7ZQqLcCX4nIUhGZ4G47rXp7uXh9QyJVbLP7ZoOQiDQBPgIeUNWD7pLYQU1Vy4F+IhIPTBWR3v6OyWsicimwW1WXisgof8dTz4aqao6ItAZmiciG0z1hqFwRZAGpPq9TgBw/xeIPeSLSFsD9udvP8XhCRCJxksDbqvqxuzkk6g6gqgeA2Th9RMFe76HA5SKyDaep91wReYvgrzeqmuP+3A1MxWn6Pq16h0oiWAJ0EZEOIhIFXAdM83NM9WkacKv7/FbgUz/G4glxvvq/AqxX1ad9dgV13UUkwb0SQERigdHABoK83qr6iKqmqGoazt/zv1X1JoK83iLSWESaHnsOXACs4TTrHTIji0XkYpw2xXDgVVV9zM8heUJE3gVG4UxLmwf8H+ATYArQDtgBXK2qlTuUA5qIDAPmAav5qc34UZx+gqCtu4j0wekcDMf5YjdFVf8iIi0J4nr7cpuGfqOqlwZ7vUWkI85VADhN+++o6mOnW++QSQTGGGOqFipNQ8YYY6phicAYY0KcJQJjjAlxlgiMMSbEWSIwxpgQZ4nANBgi8p37M01Ebqjjcz9a1Xt5RUTGicifPDr3oycvdcrnPENEXq/r85rAYLePmgbH977wUzgm3J1qobr9h1W1SV3EV8t4vgMuV9U9p3meE+rlVV1E5GvgDlXdUdfnNg2bXRGYBkNEjs2i+Tgw3J1v/UF3UrUnRGSJiKwSkbvc8qPcNQjewRlIhoh84k7GtfbYhFwi8jgQ657vbd/3EscTIrLGneP9Wp9zzxaRD0Vkg4i87Y5eRkQeF5F1bixPVlGPrkDJsSQgIq+LyCQRmScim9x5co5NFlerevmcu6q63CTOmgQrROSf7rTriMhhEXlMnLUKFopIorv9are+K0Vkrs/pP8MZpWtCjarawx4N4gEcdn+OAqb7bJ8A/MF9Hg1kAB3ccoVAB5+yLdyfsThD71v6nruK97oKmIUzMjcRZ1RmW/fcBTjzUoUB3wPDgBbARn66mo6voh63A0/5vH4d+NI9Txecua9iTqVeVcXuPu+B8wEe6b5+EbjFfa7AZe7zv/u812oguXL8OPP3fObv/wf2qP9HqMw+agLbBUAfERnvvo7D+UAtBRar6lafsr8SkSvc56luub01nHsY8K46zS95IjIHOAs46J47C0CcaZ7TgIVAMfCyiHwOTK/inG2B/ErbpqhqBfCDiGwBup9ivapzHjAAWOJesMTy04RjpT7xLQXOd58vAF4XkSnAxz+dit1AUi3e0wQZSwQmEAjwS1WdedxGpy+hsNLr0cAQVT0iIrNxvnmf7NzVKfF5Xg5EqGqZiAzE+QC+DrgPOLfScUU4H+q+KnfGKbWs10kI8C9VfaSKfUdV9dj7luP+vavqRBEZhLOoywoR6aeqe3F+V0W1fF8TRKyPwDREh4CmPq9nAneLM800ItLVnXmxsjhgv5sEuuMsWXnM0WPHVzIXuNZtr08ARgCLqwtMnPUO4lR1BvAAzvKQla0HOlfadrWIhIlIJ6AjTvNSbetVmW9dvgHGizM3/bG1a9vXdLCIdFLVRar6J2APP03R3hWnOc2EGLsiMA3RKqBMRFbitK8/i9Mss8ztsM2n6qX4vgQmisgqnA/ahT77JgOrRGSZqt7os30qMARYifMt/XequstNJFVpCnwqIjE438YfrKLMXOApERGfb+QbgTk4/RATVbVYRF6uZb0qO64uIvIHnBWrwoCjwL3A9hqOf0JEurjxf+PWHeAc4PNavL8JMnb7qDEeEJFncTpev3bvz5+uqh/6OaxqiUg0TqIapqpl/o7H1C9rGjLGG/8XaOTvIE5BO+BhSwKhya4IjDEmxNkVgTHGhDhLBMYYE+IsERhjTIizRGCMMSHOEoExxoS4/w+EdcApg/01KAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_ont_hot_tensor(class_num):\n",
    "    return torch.eye(2,2)\n",
    "\n",
    "# [[1, 0, 1, 0, .....]]\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "# 初始化模型\n",
    "net = Net(params_init_func, num_inputs)\n",
    "net.train()\n",
    "\n",
    "# 定义optimizer\n",
    "lr = 0.01\n",
    "optim = SGD(net.parameters(),lr=lr)\n",
    "\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "# 定义epoch数量\n",
    "num_epochs = 500\n",
    "\n",
    "# 首先清空一次梯度\n",
    "optim.zero_grad()\n",
    "\n",
    "# num_outputs = 2, 已经在上方定义\n",
    "onehot_classes_tensor =  make_ont_hot_tensor(num_outputs)\n",
    "    \n",
    "# [1, 0]\n",
    "# [0, 1]\n",
    "\n",
    "y=torch.tensor(np.eye(2)[train_y.reshape(-1)]).float()\n",
    "x = torch.tensor(train_x_orig).float()\n",
    "\n",
    "# print(x)\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    j = 0\n",
    "    \n",
    "    # 前向传播\n",
    "    out = net(x)\n",
    "    \n",
    "    # 计算loss\n",
    "    ls = loss(out, y)\n",
    "    print(f\"epoch: {i} train loss:\", ls.item())\n",
    "    \n",
    "    # 反向传播\n",
    "    ls.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optim.step()\n",
    "    \n",
    "    # 清空梯度\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    \n",
    "    # 测试部分\n",
    "    with torch.no_grad():\n",
    "        print(\"开始测试\")\n",
    "#         print(np.argmax(out.detach().numpy(), axis=1))\n",
    "\n",
    "        result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "#         print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "#         print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "        print(\"Acc：\", np.mean(result),'\\n')\n",
    "    \n",
    "    # record the cost every 10 training epoch\n",
    "    if i % 10 == 0:\n",
    "        costs.append(ls.item())\n",
    "        accs.append(np.mean(result))\n",
    "\n",
    "# plot the cost\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.plot(np.squeeze(accs))\n",
    "plt.ylabel('cost/acc')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(lr))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "模型对所有训练数据进行预测、分类的结果：\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "所有训练输入数据真实分类标签：\n",
      " [0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      "result: [ True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True False\n",
      "  True  True False  True  True  True  True  True  True False  True  True\n",
      "  True False  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True False  True  True  True  True False  True  True  True  True False\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True False  True  True  True  True  True\n",
      "  True  True False False  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True False False  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True]\n",
      "\n",
      "Acc： 0.861244019138756\n"
     ]
    }
   ],
   "source": [
    "# print(np.argmax(net(x).detach().numpy(),axis=1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(np.argmax(net(x).detach().numpy(), axis=1))\n",
    "\n",
    "    result = (np.argmax(net(x).detach().numpy(),axis=1) == np.argmax(y.numpy(),axis=1))\n",
    "    print(\"\\n模型对所有训练数据进行预测、分类的结果：\\n\", np.argmax(net(x).detach().numpy(),axis=1))\n",
    "    print(\"\\n所有训练输入数据真实分类标签：\\n\", np.argmax(y.numpy(),axis=1))\n",
    "    print(\"result:\",result)\n",
    "    print(\"\\nAcc：\", np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集说明\n",
    "本项目提供了一个数据集(data.h5,用load_data()读取),其中包含：一组标记为cat(1)或non-cat(0)的图像训练集(209张图片,209个标签),一组标记为cat(1)或non-cat(0)的测试图像测试集(50张图片,50个标签). 每个图像形状都是（64, 64, 3），其中3代表3个通道（RGB）\n",
    "\n",
    "## 任务要求\n",
    "\n",
    "### 必选项：\n",
    "\n",
    "**任务0.** 完整阅读一遍本notebook, 填写里面所需的代码\n",
    "\n",
    "**任务1.** 尝试用本实验项目提供的测试集数据(变量名为test_x_orig, test_y)对上面训练好的模型进行测试\n",
    "\n",
    "**任务2.** 按照表格要求调整各项参数，然后进行实验，填写表格\n",
    "\n",
    "**任务3.** 尝试更换优化器，将随机梯度下降(SGD)换成Adam,然后将lr设置成0.0001, 使用训练集对模型进行训练和测试\n",
    "\n",
    "**任务4.** 尝试在神经网络的初始化(init)阶段用torch.nn.linear()定义self.linear，并且用self.linear替换所有的torch.matmul(X, self.W1) + self.b1 \\\n",
    "(使用SGD, lr = 0.001, 使用本项目提供的原版np.random.normal(),不需要变更里面的参数)\n",
    "\n",
    "**任务5.** 搭建一个新模型, 要求如下:\\\n",
    "5-1. 参考上面示例模型搭建步骤，尝试搭建一个4层的模型(4个linear层，其中包括输出层) \\\n",
    "5-2. 每一层的神经元个数为128/256/512/2 (最后为2,因为是二分类) \\\n",
    "5.3. 每一层linear的bias设置为True\n",
    "5-4. 前三层linear,每层输出接一个relu,即一层神经网络为: (linear->relu), 然后整个网络: (linear->relu) * 3 \\-> linear \\\n",
    "5-5. 在模型初始化时，使用xavier参数初始化方法对权重矩阵$W_i$进行初始化，偏置项$b_i$初始化置0 **(下标i，表示第i层)** \\\n",
    "5-6. 优化器使用SGD, lr = 0.001 \\\n",
    "5-7. 设置训练迭代epoch为200 \\\n",
    "5-8. 上述实验结束后,使用Adam作为优化器,lr不变,再进行一次实验,观察结果\n",
    "\n",
    "---\n",
    "\n",
    "### 加分项：\n",
    "\n",
    "**任务7/8/9为一个系列**\n",
    "\n",
    "**任务6.** 在图像数据输入神经网络训练之前，使用torchvision库对图像数据进行增强\n",
    "\n",
    "**任务7.** 将训练集中的209个数据分批(batch)输入训练,要求: 一个batch的大小为10(batch_size = 10), 并且回答为何需要将数据分批训练\n",
    "\n",
    "**任务8.** 使用torch内置的工具,构建一个数据读取器(dataloader)来读取数据\n",
    "\n",
    "**任务9.** 使用**任务8**中的DataLoader替换任务7中自定义的batch数据提取器 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务1:\n",
    "尝试用本实验项目提供的测试集数据对上面训练好的模型进行测试 \\\n",
    "(测试集的图像数据及标签的变量名分别为test_x_orig和test_y)\n",
    "\n",
    "**(将代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = net(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务2: \n",
    "按照表格要求调整各项参数，然后用训练集进行实验，填写表格 \\\n",
    "**(实验完成后需要提交表格)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务3: \n",
    "尝试更换优化器，将SGD换成Adam,然后使用将lr设置成0.0001,然后使用训练集对模型进行训练和测试 \\\n",
    "**(将优化器定义在下方,并在下方再次编写模型训练的代码,然后训练,保留训练过程中输出的结果)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "\n",
    "# 获取数据\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "# 构建Adam优化器\n",
    "lr = None\n",
    "optim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "# 创建模型对象\n",
    "net = None\n",
    "net.train()\n",
    "\n",
    "# 创建损失函数对象\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# epoch数量\n",
    "num_epoch = None\n",
    "\n",
    "# 开始进行训练\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        # 前向传播\n",
    "        pred = net(None)\n",
    "        \n",
    "        # 计算loss\n",
    "        ls = loss(None, None)\n",
    "        \n",
    "        # 反向传播\n",
    "        None\n",
    "        \n",
    "        # 更新参数\n",
    "        None\n",
    "        \n",
    "        # 清空梯度\n",
    "        \n",
    "        # 打印\n",
    "        print(f'epoch {i}, train loss = {ls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = net(None)\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    Acc = None\n",
    "    print(\"Acc: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务4:  \n",
    "尝试在神经网络的初始化(init)阶段定义self.linear，并用self.linear替换所有的torch.matmul(X, self.W1) + self.b1 \\\n",
    "(使用SGD, lr = 0.001, 使用本项目提供的原版np.random.normal(),不需要变更里面的参数)\n",
    "\n",
    "#### 1. torch.nn.Linear创建出的对象是一个线性变换层函数： \n",
    "$$Y = WX+b$$\n",
    "\n",
    "```python\n",
    "对象创建方法： linear = torch.nn.Linear(in_features, out_features, bias=True)\n",
    "对象调用方法： 输出tensor = linear(输入tensor)\n",
    "```\n",
    "\n",
    "#### 参数bias=False时，变成：\n",
    "$$Y = WX$$\n",
    "\n",
    "#### 2. 实现Y = WX+b, 本项目初始提供的linear层构建/计算方案：\n",
    "\n",
    "init阶段：\n",
    "```python\n",
    "self.W1 = nn.Parameter(torch.tensor((num_inputs, num_hiddens), dtype=torch.float, requires_grad = True))\n",
    "self.b1 = nn.Parameter(torch.zeros(num_hiddens, dtype=torch.float, requires_grad = True))\n",
    "```     \n",
    "forward阶段：\n",
    "```python\n",
    "Y = torch.matmul(X, self.W1) + self.b1\n",
    "```\n",
    "\n",
    "#### 3. 更加方便的linear层构建/使用方式：\n",
    "\n",
    "init阶段：\n",
    "```python\n",
    "self.linear = torch.nn.Linear(num_inputs, num_hiddens, bias=True)\n",
    "```\n",
    "forward阶段：\n",
    "```python\n",
    "Y = self.linear(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**(将重新构建的模型代码写在下方,并且用训练集进行训练和测试,保留训练过程中输出的结果)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "# 128/256/512/2\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params_init_func, num_inputs):\n",
    "    def __init__(self, num_inputs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 替换这个部分\n",
    "#         params = params_init_func()\n",
    "        \n",
    "#         for param in params:\n",
    "#             param.requires_grad_(requires_grad=True)\n",
    "        \n",
    "#         self.W1 = nn.Parameter(params[0].float())\n",
    "#         self.b1 = nn.Parameter(params[1].float())\n",
    "\n",
    "#         self.W2 = nn.Parameter(params[2].float())\n",
    "#         self.b2 = nn.Parameter(params[3].float())\n",
    "\n",
    "        # 代码填空\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear((num_inputs, 128))\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view((-1, num_inputs))\n",
    "        \n",
    "#         H = self.linear1(X)\n",
    "#       等同于\n",
    "#       H = torch.matmul(X, self.W1) + self.b1\n",
    "        \n",
    "        \n",
    "        # 替换这个部分\n",
    "#         H = relu(torch.matmul(X, self.W1) + self.b1)\n",
    "#         out = torch.matmul(H, self.W2) + self.b2\n",
    "\n",
    "        # 代码填空\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "# for i in range(??):\n",
    "    \n",
    "#     # 前向传播\n",
    "    \n",
    "#     # 计算loss\n",
    "    \n",
    "#     # 反向传播\n",
    "    \n",
    "#     # 更新参数\n",
    "    \n",
    "#     # 清空梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必选项任务5: \n",
    "搭建一个新模型, 要求如下:\\\n",
    "5-1. 参考上面示例模型搭建步骤，尝试搭建一个4层的模型(4个linear层，其中包括输出层) \\\n",
    "5-2. 每一层的神经元个数为128/256/512/2 (最后为2,因为是二分类) \\\n",
    "5.3. 每一层linear的bias设置为True\n",
    "5-4. 前三层linear,每层输出接一个relu,即一层神经网络为: (linear->relu), 然后整个网络: (linear->relu) * 3 \\-> linear \\\n",
    "5-5. 在模型初始化时，使用xavier参数初始化方法对权重矩阵$W_i$进行初始化，偏置项$b_i$初始化置0 **(下标i，表示第i层)** \\\n",
    "5-6. 优化器使用SGD, lr = 0.001 \\\n",
    "5-7. 设置训练迭代epoch为200 \\\n",
    "5-8. 上述实验结束后,使用Adam作为优化器,lr不变,再进行一次实验,观察结果\n",
    "\n",
    "#### 与xavier方法相关的参考资料（若仅仅需要使用xavier，一般只用看前两个）：\n",
    "（1）神经网络各种初始化方法（normal，uniform，xavier）的numpy实现以及表现对比\n",
    "https://blog.csdn.net/kane7csdn/article/details/108896031\n",
    "\n",
    "（2）使用torch随机初始化参数 （包括xavier）\n",
    "https://blog.csdn.net/weixin_36893273/article/details/123641399\n",
    "\n",
    "（3）初始化及分布 （对各种初始化参数方法的原理介绍）\n",
    "https://blog.csdn.net/LWD19981223/article/details/124348675\n",
    "\n",
    "---\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建模型和各种工具\n",
    "\n",
    "\n",
    "# 参数初始化\n",
    "# # W1\n",
    "# input_dim= 12288 # = 64 x 64 x 3\n",
    "# output_dim= 128\n",
    "\n",
    "# # W2\n",
    "# input_dim= 128 #\n",
    "# output_dim= 256\n",
    "\n",
    "# # W3\n",
    "# input_dim= 256 #\n",
    "# output_dim= 512\n",
    "\n",
    "# # W4\n",
    "# input_dim= 512 #\n",
    "# output_dim= 2\n",
    "\n",
    "# # in: 12288 out: 128\n",
    "# # 128/256/512/2 \n",
    "    \n",
    "# std = np.sqrt(2. / (input_dim + output_dim))\n",
    "# self.W1 = np.random.normal(loc=0., scale=std, size=[out_dim, in_dim])\n",
    "# self.W2 = np.random.normal(loc=0., scale=std, size=[out_dim, in_dim])\n",
    "\n",
    "# 128/256/512/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务6: \n",
    "torchvision是pytorch的一个图形库, 主要用来处理图像数据,对图像数据进行数据增强\n",
    "\n",
    "要求:\n",
    "1. 在图像数据输入神经网络训练之前，使用python torchvision库对图像数据进行增强 \n",
    "2. 然后将增强后的图像数据输入到**本项目提供的2层神经网络中**进行训练和测试 \n",
    "\n",
    "参考资料: \\\n",
    "torch学习 (三十三)：Torch之图像增广 \\\n",
    "https://blog.csdn.net/weixin_44575152/article/details/118056405\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 图像增强\n",
    "import torchvision\n",
    "\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "\n",
    "# def test5(img):\n",
    "#     shape_aug = torchvision.transforms.RandomResizedCrop(200, scale=(0.1, 1), ratio=(0.5, 2))\n",
    "#     color_aug = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "#     aug = torchvision.transforms.Compose(\n",
    "#         [torchvision.transforms.RandomHorizontalFlip(),\n",
    "#         shape_aug,\n",
    "#         color_aug,]\n",
    "#     )\n",
    "    \n",
    "#     apply(img, aug)\n",
    "\n",
    "# test5(train_x_orig[7])\n",
    "\n",
    "# 中间可能会碰到的问题:\n",
    "# np数组格式\n",
    "# (64, 64, 3)\n",
    "\n",
    "# 用torchvision增强需要转换成以下格式吗?\n",
    "# 大家要查资料,做实验去尝试\n",
    "# (1, 64, 64, 3)\n",
    "# (1, 3, 64, 64)\n",
    "# (3, 64, 64)\n",
    "    \n",
    "# Example of a picture\n",
    "index = 7\n",
    "\n",
    "print(train_x_orig[index].shape)\n",
    "plt.imshow(train_x_orig[index])\n",
    "\n",
    "# plt.imshow()\n",
    "\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务7: \n",
    "我们在之前的实验中,均是一次性将209个图片输入到将训练集中的209个数据分批(batch)输入到**本项目提供的2层神经网络中**训练 \n",
    "\n",
    "要求: 一个batch的大小为10(batch_size = 10), 并且查资料/思考之后回答: 为何需要将数据分批训练 \n",
    "\n",
    "tips:\n",
    "1. 需要关注如何切分ndarray \n",
    "2. batch_size为10,将会分为21批(batch)的数据,最后一个batch的size如何通过程序计算(要求不能用if语句**直接指定**最后一个batchsize为9) \\\n",
    "(用取余操作来算出最后一个batch的size) \n",
    "3. 需要在迭代epoch的for循环中,再增加一层for循环来遍历各个batch的数据\n",
    "\n",
    "**(代码写在下方)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "\n",
    "print(train_x_orig.shape)\n",
    "# 209\n",
    "\n",
    "# x.shape -> (209, 64, 64, 3) tuple\n",
    "# batch_size = 10\n",
    "# num_of_batch = int(x.shape[0]/10) # 20\n",
    "\n",
    "# if x.shape[0] % 10 != 0: num_of_batch = num_of_batch + 1\n",
    "\n",
    "# for i in range(num_epochs):\n",
    "#     j = 0\n",
    "    \n",
    "#     for j in range(num_of_batch):\n",
    "#         x_batch_j = x[?,:] # 得到 x_batch_j -> (10, 64, 64, 3)? 或者该epoch的最后一组数据,它的shape (9, 64, 64, 3)\n",
    "#     # 前向传播\n",
    "#         out = net(x_batch_j)\n",
    "    \n",
    "#         ...\n",
    "\n",
    "\n",
    "# 如何拿出每一批10个数据去做训练?\n",
    "# (10, 64, 64, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: 为何要将数据分批训练:\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务8: \n",
    "\n",
    "使用pytorch内置的工具,构建一个数据读取器(dataloader)来读取数据 \\\n",
    "**(代码写在下方)**\n",
    "\n",
    "参考资料:\n",
    "1. 两文读懂PyTorch中Dataset与DataLoader（一）打造自己的数据集\n",
    "https://zhuanlan.zhihu.com/p/105507334\n",
    "2. 两文读懂PyTorch中Dataset与DataLoader（二）理解DataLoader源码\n",
    "https://zhuanlan.zhihu.com/p/105578087\n",
    "\n",
    "tips: \n",
    "ndarray的数据长度用.shape来提取 \\\n",
    "\n",
    "\n",
    "首先需要根据参考资料1,自定义Dataset类型 \\\n",
    "再根据参考资料2,将自定义Dataset类型创建出的对象,传到torch.utils.data.DataLoader \\\n",
    "\n",
    "参考资料2里的sampler,不需要使用.该任务仅仅需要做DataLoader \\\n",
    "\n",
    "参考资料1里的数据是从文件夹里面读取图片,而我们提供的数据,是已经转化为ndarray, \\\n",
    "因此在做自定义dataset不需要再像参考资料那样从文件夹中读取图片和标签放到ndarray \\\n",
    "主要需要考虑如何从ndarray数组里面取出单个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 209)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_77152/4262995201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 自定义Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mcat_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#         self.file_path = './data/faces/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "print(train_y.shape)\n",
    "\n",
    "# 自定义Dataset\n",
    "class Cat_dataset(data.Dataset):\n",
    "    def __init__(self, img_array, label_array):\n",
    "#         self.file_path = './data/faces/'\n",
    "#         f=open(\"final_train_tag_dict.txt\",\"r\")\n",
    "#         self.label_dict=eval(f.read())\n",
    "#         f.close()\n",
    "        self.img_array = img_array\n",
    "        self.label_array = label_array\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "#         label = list(self.label_dict.values())[index-1]\n",
    "#         img_id = list(self.label_dict.keys())[index-1]\n",
    "#         img_path = self.file_path+str(img_id)+\".jpg\"\n",
    "#         img = np.array(Image.open(img_path))\n",
    "        label = # 怎么从 self.label_array里面拿一个出来呢? (1, 209) -> 如何拿(1, 1)\n",
    "        img = # 同上\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(None)\n",
    "\n",
    "cat_dataset = Cat_dataset(train_x_orig, train_y)\n",
    "    \n",
    "# 构建DataLoader对象\n",
    "dataloader = torch.utils.data.DataLoader(cat_dataset,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加分项任务9: \n",
    "使用任务8中的DataLoader替换任务7中自定义的batch数据提取器 \n",
    "\n",
    "要求: \\\n",
    "batch_size = 10, shuffle = True (打乱数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
